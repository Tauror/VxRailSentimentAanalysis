,user,time,comment
0,JJ69,"Aug 23, 2018 2:59 AM","Hi all,Im deploying 4x VXRails HCI nodes for a VDI solution for Devs. Because of security requirements we will be leveraging NSX both for segregation/isolation and A/V etc. There will be two TOR switches. This will be our 1st NSX implementation.Question: Will this work with just 2 NICs per physical node? The VXRails come with 2x 10gb nics and this is as per the Dell/VMware RA for EUC. However the need for NSX with my pea sized brain, does not compute. I have the normal requirements for Management, vMotion, vSAN etc. However, to keep effective separation between Horizon server and desktop pools, I also planned separate DLR with separate Edges for server and desktop pools each VXLAN trunked to the TOR switch (using the switch as a VTEP gateway). I cant see how I could do this all over 2x NICs.Any help much appreciated.JJ"
1,Sreec,"Aug 23, 2018 3:35 AM","Will this work with just 2 NICs per physical node? The VXRails come with 2x 10gb nics and this is as per the Dell/VMware RA for EUC. However the need for NSX with my pea sized brain, does not compute. I have the normal requirements for Management, vMotion, vSAN etc. However, to keep effective separation between Horizon server and desktop pools, I also planned separate DLR with separate Edges for server and desktop pools each VXLAN trunked to the TOR switch (using the switch as a VTEP gateway).You can certainly leverage default NICs (2) for N-S and E-W traffic ,so Transit VLAN,VXLAN and all other traffic will exit via same set of interfaces which can be a potential bottleneck scenario based on the real use case also troubleshooting will be slightly difficult whenever situation demands .For DFW there are no changes happening at DVS level , ideally I prefer keeping VSAN/Management Traffic dedicated to default DVS/NICs and have 2-4 additional PCI slots for NSX traffic, this is one of the design I have implemented recently considering the use case. For VDI - micro segmentation is a good candidate and like you mentioned we can leverage DLR/Edges for routing scenarios between management subnets and desktop pools also NSX LB can be used we there are multiple Connection Servers and UAG instances .For hardware VTEP ensure that switch is supported and please note that we cannot use DLR in that case There is a good doc which covers few design scenarios as well- &gt;VMware NSX for vSphere End-User Computing Design Guide 1.2 Can check this doc for quick reference even though it covers very basic design scenarios for VXRAIL-NSX design. Security for Hyper-Converged Solutions: Dell EMC VxRAIL Appliances - VMware vSAN Readymades with VMware NSX-V "
2,JJ69,"Aug 23, 2018 5:03 AM","Thanks for that. I would prefer as you say to have separate NICs for NSX traffic. The plan was to place a NSX LB in-line of the UAGs and I have been trying to align with the EUC Design Guide where possible.Now for the stupid question: Why does the use of a hardware VTEP prevent the use of a DLR?Kind regards,JJ"
,user,time,comment
0,martin1975,"May 28, 2018 1:09 AM","i would like to know what are the software that will be bundled with vxrail , for eg..I understand it has its own management software, then the question is will I need additional vsan or nsx over and above what is provided..."
1,smitmartijn,"May 28, 2018 11:41 AM","Hi,There are a bunch of options when it comes to licensing a VxRail - you should contact your DellEMC representative to talk options.If you have an existing DellEMC relation, you might have access to see the licensing guide: https://support.emc.com/docu78571"
,user,time,comment
0,Dabrain99,"Jul 19, 2019 5:22 AM","Hi,i wanted to know if its possible for my customer to resell his VxRail including the VMware Vsan License.He is a Dell OEM Partner and Dell says its ok to resell the system including the VMware.The customer buys in Europe and will sell to China, India and Turkey.I guess he must also become a VMware Partner? Right?Please let me know if you have any valid informations."
1,daphnissov,"Jul 19, 2019 6:39 AM",I don't know if this is the best venue for such a question. I'd probably recommend he talk to his partner business manager to ask what the official process is and if there are any restrictions in those countries.
,user,time,comment
0,sandroalvesbrasil,"Jul 13, 2019 11:27 PM","Hi friends,I am evaluating some possibilities and would like the help of you.I'm in doubt if I'd better add servers (no operating system) and use my enterprise partner licenses to use vSAN.Or if it is better to purchase the Dell VxRail product.My opinion is that VxRail is a white elephant, full of things that in fact the final purpose is vSan only.Has anyone done this review yet?If I choose vSan, what care do I need to have to choose my servers?One factor that made me curious is that VxRail has some specific network requirements, vmNICs amount to guarantee throughput. They do not dedicate vmNICs to vMotion, Mgmt, and so on.They use all trunk cables, using VLANs.But I ask, as much as I use many vmNICs VMware does not increase the capacity. VMware does not work with Microsoft using the NIC Team.Can someone explain to me how this works?Thank you."
1,depping,"Jul 15, 2019 3:05 AM","VxRail offers a single package for everything: server+storage+support+vmware licenses. On top of that it adds some additional life cycle management, like updating of firmware etc. I have many customers who use VxRail. But I also have many customers who simply pick a VSAN Ready Node from the HCL (vmwa.re/vsanhcl) and go from there. Either of the two options work. If you are comfortable with updating firmware etc yourself through the available interfaces Dell provices you could go with Ready Nodes.When it comes to NIC teaming, you can use LACP if desired, or you can use any of the other options that vSphere provides. I typically don't see this being an issue to be honest for customers. With 10GbE and 25GbE, bandwidth is usually not the biggest problem."
2,srodenburg,"Jul 15, 2019 3:12 AM","Hello Sandro,To answer your questions:Q:My opinion is that VxRail is a white elephant, full of things that in fact the final purpose is vSan only.A:VxRail is a highly automated platform for setting up and expanding vSAN Clusters. It's basically ""power it on, enter data like hostnames and ip-adresses"" and the Automation Layer does the rest.VxRail also offers several other DELL-EMC products that are helpfull in certain use-cases. But the main driver is total automation, easy of management (updates incl. firmware, drivers etc. truly are just a couple of mouse clicks and it actually works).Q:If I choose vSan, what care do I need to have to choose my servers?A:If you purchase VxRail, everything like ensuring compatibility is done for you. If you go the DIY route, then select so called vSAN Ready Nodes from your preferred hardware vendor. This ensures that you don't run into compatibility issues during setup or later on during daily operations.Q:One factor that made me curious is that VxRail has some specific network requirements, vmNICs amount to guarantee throughput. They do not dedicate vmNICs to vMotion, Mgmt, and so on.A:They do actually. In order to make the extreme automation of VxRail possible, the networking is mostly pre-defined. So the networks for vSAN, Mngt, vMotion are all kind of hard coded. One wins total easy of use but one loses the flexibility to ""do whatever you want"".This you gain back when going with vSAN Ready Nodes although you must always adhere to the vSAN Networking Design Guide and it's supported Topologies. vSAN uses the network at it's core and it better be damn well designed. vSAN is no toy."
,user,time,comment
0,qrt1977,"May 27, 2019 11:39 PM","DELL/EMC Support ..... the joke saga continues !!!!And I thought support for Laptops and Desktops was a joke @ Dell3 months of planning to upgrade our VXRAIL.Pre-check done 1 month agoToday planned to start upgrade @ 5AM in the morning.Get out of bed @ 3.30AMArrived @ 4.30AM at the officeReceived mail from support to start a webex and start upgrade.OOPS !!!! something went wrong &hellip;. Inconsistent disks &hellip;.. how did pre-check missed this ?Support guy tries contacting VXRAIL support3.30 HOURS LATER (and counting) !!!! NOTHING happened, the unfortunate support guy cant reach VXRAIL SUPPORT and they arent answering his call.What If we had an outage on our vxrails ??? If a inside support guy isnt answered by his own support &hellip;. When will I get an answer ?What a JOKE &hellip;. Are we really paying for this ?Please do not call me today to say sorry, Im not in the mood!Also &hellip;. Thank you for your patience to Kyle Mao : this guy needs a raise !! 5 stars for you my friend"
1,depping,"May 27, 2019 11:44 PM",I am not sure Dell/EMC representatives are reading the threads on this forum to be honest. I would recommend escalating this within Dell/EMC. I will point one of my VxRail contacts to this threads to see if they can help getting you the support you deserve as a customer.
2,qrt1977,"May 28, 2019 12:02 AM","Hi,Thank you for your help depping15 minutes ago I finally received support.unfortunately there is a problem with 3 VM's and I need to put them offline and clone them, and then delete the original one and start the clone.This is a Exchange server, File and Print server and a webserver;Again ... this is not possible in business hours, and I also would like to do something else in my free-time.I'm really pissed today and thinking about leaving the VXRAIL like it is for the next 3 years (it actually runs fine as it is) without upgrading or whatever and make a switch to NUTANIX.Sorry guys .... but I never got the requested support from DELL/EMC when I needed it.First Dell desktop/laptop support and now even for EMC products.73's Kurt"
3,depping,"May 28, 2019 12:23 AM",I understand your problem. I have reached out to two folks I know within Dell/EMC to at least make them aware at a higher level what is happening. My apologies for the situation.
4,sk84,"May 28, 2019 3:54 AM","We have had similar experiences with VxRail support. Among other things also with upgrades. In the meantime, we have built up enough know-how to be able to carry out upgrades ourselves and to solve most of the problems that arise. In most cases a look into the logs is enough to identify the error or in the worst case a RASR reset is performed.I just received an email last week telling me that they want to plan another upgrade with us due to the vSAN guest-data inconsistency bug last year... However, we already installed the update in October/November last year and did the vSAN Pre-Checks documented in SolVe ourselves. I find it a bad joke that in case of a critical bug (worst case is data loss!) it takes them more than 6 months to inform us about the upgrade. And that they don't seem to see that we are already on a VxRail software version that contains the bugfix.I have to say that we also have had positive experiences with the support in the past. But as a tip: Use the chat at a time when the USA is working. The VxRail team there is still the most competent and can at least sometimes help promptly. If you call or chat to a time where the support is redirected to India or Asia, you are usually really lost.We've already talked about all these experiences with our Key Account Manager at Dell/EMC and he has confirmed that there are many complaints about support. He says that they have also recognized the problem on the management level and are working on improving the support. But this takes a lot of time and effort.Disclaimer: I don't have many insights in the Dell/EMC or VxRail organization structures, but that are our own experiences."
,user,time,comment
0,sandroalvesbrasil,"Apr 19, 2019 5:29 PM","Hello,in fact we have no problem, but a doubt.Shows in the free space of the datastore named as vxRail a free space of 27TB.In fact, that's the sum of all my HDD drives.The vSAN calculator says usable Storage is 9.4TB.We already use 6.49TB.- Storage Free 21.43TB / Used: 6.49TB / Capacity: 27.92TBConfiguration of the 3 nodes of the vSAN cluster:1820 GB x 5 (HDD)372 GB x 2 (SSD)Raw Storage: 1820 x 15 = 27TBUsable Storage: 9.4TB.Doubt:1 - I know we can not use the 27.92TB, but if we create new VMs the space will be used.- Why VMware shows RAW space (27.92TB) instead of just showing usable space (9.4TB)?- Can we block the use of space above the usable vSAN (9.4TB)?- Will any alarms be generated when near the usable space (9.4TB)?- If we go beyond the usable space? Will this cause what kind of problem for my environment?2 - Can you explain to me how vSAN does this 9.4TB calculation?Thank you."
1,TheBobkin,"Apr 20, 2019 3:41 PM","Hello sandroalvesbrasil,""- Why VMware shows RAW space (27.92TB) instead of just showing usable space (9.4TB)?""Because multiple different Storage Policies can be applied to different Objects e.g. some data could be stored as FTT=0 and some data as FTT=1 (and obviously far more possible variations in larger and All-Flash clusters).""- Can we block the use of space above the usable vSAN (9.4TB)?""Unfortunately I am not presently up to speed on what ended up being implemented in 6.7 U2 with regard to 'guard-rails' but there are/were plans to start limiting things such as VM creation past certain thresholds. Either way though, from a administrator perspective there is no real control over ""blocking"" space.""- Will any alarms be generated when near the usable space (9.4TB)?""Yes, you will start getting warnings in vSAN Health once any individual disk passes 80% full and it starts rebalancing this data to other disks where possible:VMware Knowledge Base ""- If we go beyond the usable space? Will this cause what kind of problem for my environment?""Like any storage system, the VMs need to be able to write to disk - if you run out of space on disks where these components reside then of course the VM will not be functional until space is freed up/rebalanced and it can write to disk. Obviously the solution to this is to size your environment accordingly and not run out of space.""2 - Can you explain to me how vSAN does this 9.4TB calculation?""It is very likely factoring in VMwares recommendation of allowing 25-30% slack space for temporary increases in used space (e.g. changing Storage Policy structure and of course snapshot-data during back-ups).Bob"
,user,time,comment
0,compwizpro,"Apr 18, 2019 9:10 AM","Hello,I apologize if this question was asked earlier in advance.We have a newly configured VxRail all flash appliance running ESXi 6.7 build 13004448. I have deployed a new server 2016 guest from a template with hardware version v14 and latest vmware tools installed. The VM as well as the template have the default vsan storage policy applied with object space reservation set to 0 - Thin.When I look in the windows guest under optimize drives, I see all the drives registering as Hard Disk Drive and not Thin Provisioned. I have this same configuration running on other non-vxrail storage minus the storage policy and the guest detects disk as thin provisioned without issue.Is there a setting I'm missing or a gotcha with running this configuration on vsan? I followed the article VMware Knowledge Base regarding OVA/OVF deployment in the web console but it didn't find any policies configured for thick and I am not deploying from OVF / OVA.Thanks for the help!"
1,compwizpro,"Apr 22, 2019 12:05 PM","After some search and other help, I was able to figure this out and it turned out to be quite simple. By default, unmap is not enabled on vSAN. I had to enable it on for the cluster running the command ""esxcfg-advcfg -s 1 /VSAN/GuestUnmap"" on one of the esx hosts. After that and rebooting one of the VMs, it recognized the virtual disks as thin."
,user,time,comment
0,sandroalvesbrasil,"Mar 23, 2019 2:54 PM","Hi,VxRail delivers a ready environment with vSAN with multiple networks that you have defined during startup.Can I make changes to the networks, how to create new ones or change existing ones?Thank you."
1,sk84,"Mar 24, 2019 3:35 AM","You can add more VM networks and edit or delete these additional networks/portgroups. But it is not supported to change or delete the initially created management networks yourself. You have to contact the Dell EMC Support for this. This includes the vSAN, vMotion, Marvin and VMware Server network."
,user,time,comment
0,sandroalvesbrasil,"Mar 23, 2019 2:47 PM","Hello,I have a vxRail Dell EMC solution with 3 servers with vSAN.Who manages nodes is vSAN.So, I wonder what the function of vxRail Manager in this environment?Thank you."
1,TheBobkin,"Mar 23, 2019 5:48 PM","Hello sandroalvesbrasil,It is essentially just a management appliance but one that is strongly tied to the VxRail cluster it manages.VxRail Manager is used for cluster-wide initial configuration, management (e.g. shutting down cluster) and change-management (e.g. upgrading driver+firmware+ESXi). While it is possible to make changes to a vSAN cluster outside of VxRail Manager (e.g. add storage or nodes) this is not advised (and Dell-EMC state unsupported) as VxRail Manager will not be aware of the changes.Bob"
2,sandroalvesbrasil,"Mar 23, 2019 5:55 PM","I understand the concept and I believed that answer.However, since the main layer is VMware (vSAN) and we have the possibility to make changes it is difficult to explain to the end user what he can and can not do in VMware (vSAN).I understand that vxRail manages the health of the cluster, but it has no visibility to what we do in other configurations, such as VMs and so on. They should basically use cluster-state responses.And it's also used to orchestrate the creation of the cluster to do node maintenance, as you yourself said.The question is how far can we make changes to vSAN, without creating problems with vxRail Manager.For example, could enabling DRS, being an option directly to the cluster, be a big problem?Thank you."
3,TheBobkin,"Mar 23, 2019 6:17 PM","Hello sandroalvesbrasil,Only changes such as adding/removing nodes/hardware components etc. may have an impact with regard to VxRail Manager - changes that are normal vSphere operations such as enabling/disabling/editing HA/DRS shouldn't have any impact. Think of it as having an inventory-management role and it makes more sense.Bob"
,user,time,comment
0,sandroalvesbrasil,"Mar 23, 2019 2:56 PM","Hi,VxRail delivers a ready environment with vSAN with HA.Is it possible to enable DRS also to automatically migrate VMs when a host is very stressed?Thank you."
1,TheBobkin,"Mar 23, 2019 4:22 PM","Hello sandroalvesbrasil,DRS works on vSAN-enabled nodes just like non-vSAN nodes so yes you can enable this to migrate compute under resource-stress - vSAN however does not support Storage-DRS.Bob"
,user,time,comment
0,rickolson2018,"Dec 6, 2018 11:17 AM","Hello, greetings and salutations.I'm nearing 2 months into my new role, where I've inherited an 8-node VxRail all-flash vSAN cluster (ESXi 6.5, separate vCenter, 10G fiber networking to Cisco Nexus switches). At the moment it is not in full production, but that will change in 3 weeks (whether we're ready or not). I've got plenty of VMware experience, and I understand HCI, but this is my first hands on with vSAN so it's been a pretty fun few weeks for me.I'm working through a few odd write-latency issues that seem to be reported by vCenter and the vSAN Health Service, but don't appear to be exhibiting any negative impact from the ""users standpoint"" while we're doing our tests. Now I realize that each case is different - I've actually got an open case with Dell (who has roped in a VMware engineer) to assist in analyzing the data. But I'm curious what users of a vSAN All-Flash array have to say. Do you like the performance? Do you have any instances of ""good latency""? Is there such a thing? Do you ever see 'network latency' on your vSAN VMKernel Adapter (because I can sometimes see 4-6ms network latency when running HCI Bench tests, this on 10G fiber)? Overall are you happy with what you're seeing out of all-flash vSAN clusters?"
1,Viperman,"Dec 9, 2018 9:32 AM",I have several customers running it in production with absolutely zero issues. 
2,Wolken,"Dec 12, 2018 5:13 AM","I didn't check it personally, but it looks like this blog, which I have discovered recently, could shed some light on your question: Challenging 4-node VMware vSAN cluster performance "
3,Darking,"Dec 12, 2018 10:04 PM","I am running an all-Nvme-Flash 8 node stretched cluster. Each hosts consists of 2 disk groups with 1 cache and 3 capacity disks.We have been testing running HCIbench.Due to the stretched cluster aspect I will be seeing higher latency than you might experience.Anyhow running a random 70/30 up test i will be seeing around 220.000 read iops and around 70k writes at a latency of around 1.2 ms reads and 2.2 for writes.I've also tested large seq. writes and capped our my 200gbit site interconnect.So the speed is definitely there. In my setup I've found that a stripe size of 3 gives me maximum performance but that it does not vary much from the default raid-1 policy.We have been testing without running Jumbo Frames, but will be switching to that now to bring down CPU utilization a bit"
,user,time,comment
0,whutchis@bucknell,"Nov 26, 2018 1:29 PM","I have a cluster of VxRails that are currently running on a single switch. We want to add a 2nd switch, and connect the two additional NICs for each host. Is there any issue adding the new uplinks to the existing VSAN?"
1,TheBobkin,"Nov 26, 2018 4:29 PM","Hello whutchis@bucknellWelcome to Communities,Having redundant switch-links is of course advised, but do note the following:- If you are going to go with multiple vSAN vmkernel ports then they need to be on different subnets from one another (and obviously from all other traffic too as multihoming is unsupported and can cause issues).- Active/Standby of the vmnics attached to each vmk (in my opinion) is preferable and functions best.- As with any major network change - schedule down-time if possible unless you are 100% confident you are not going to cause an outage (and or can tolerate one). Sure you could do one node at a time with FDM/EA Maintenance Mode but you need to be positive your validation of the network is solid before taking these hosts out of MM.Myles has some great points and details relating to the above here:https://blah.cloud/infrastructure/migrating-vsan-vmkernel-ports-new-subnet/ Bob"
2,sk84,"Nov 26, 2018 1:58 PM","TheBobkin is right. Basically, that's not a problem. To be on the safe side I would also recommend to put the host in the MM and configure the 2nd adapter as standby or unused adapter, check the cabling and switchport configuration and only when everything is correct let it on standby or switch to active, depending on what you have in mind. However, if you have multiple vmknics with different VLANs for vSAN, there may be an IO delay between 10 - 90 seconds if one of the two uplinks is not working properly.See: Configuration 2 "
,user,time,comment
0,vmrulz,"Sep 18, 2018 4:05 PM","We have a small 5 node vSAN 6.6 all flash (VxRail 4.5.x) cluster in a remote office with fully redundant access layer 10GE Cisco 4500 switches. We have network uplinks configured in active/standby (since DellEMC doesn't support port channels on vSAN networking :-( ) We were doing some failure testing before the site goes live by powering off A and B side switches one at time. Switch A uplinks went down and Switch B uplinks took over and all was good. The Switch A uplinks never came back up so our network guys thought they'd force the issue by powering off Switch B. Now we're in a full down down scenario. Since each host is partitioned network wise, vSAN is obviously unhealthy. Is it safe to just shut down the nodes and vSAN will recover including VM's riding on it once we have the network healthy?Thanks,Ron"
1,TheBobkin,"Sep 19, 2018 4:39 AM","Hello Ron,Shouldn't be an issue regarding the data - on a daily basis I help fix clusters where someone/something broke the networking between nodes and generally all that is required is to restore communication (and sometimes use vsan.fix_renamed_vms).No real benefit in powering down the nodes - having them all online and ready to sync is a better option.Out of interest, do you have failback set to No on the vSwitch?Bob"
2,srodenburg,"Sep 19, 2018 6:27 AM","Don't worry. vSAN reacts to a ""nobody can talk to anybody"" by freezing IO (just like any proper Storage-Cluster Solution would do in a full split-brain scenario).Your VM's will poop themselves and hang/go zombie because their disks suddenly disappeared. Poof gone.As soon as network-connectivity is restored, vSAN will re-sync and clean itself up. Give it some time to straighten things out.And don't shut anything down. There is no need.We actually do this sort of thing on purpose during ""Production approval testing"" at customers. We yank everything out, let vSAN freak out, plug it back in, let vSAN heal itself. It always works. No big deal.Oh, and fire that networking-person. He's a &lt;fill in something not so nice&gt;..."
3,vmrulz,"Sep 19, 2018 8:29 AM","Thanks for the replies,One interesting thing I've never seen before is that even after the access layer switches fully came back up none of the ESXi hosts would re-establish their uplink connections so they all stayed in a partitioned state. Reluctantly knowing vSAN was not happy I chose in the interest of time to reboot a host to see what would happen. Sure enough the host networking came back online and the host took forever to boot waiting on vSAN to initialize. I then rebooted the other hosts knowing the vSAN would be happier if all the kids came to play. Eventually all the hosts came back online. Guests were in an orphaned like state since they had their plugs pulled. Have you guys ever seen a situation where the ESXi hosts would not re-establish their uplink connections short of a reboot? It sounds like STP port blocking at the switch but the network guys didn't see that.We have a Cisco case open and I'm going to open a case through DellEMC since it is VxRail.Ron"
4,srodenburg,"Sep 27, 2018 12:33 PM","""since it is VxRail.""Uh oh. I have a VxRail customer with Cisco Nexus switches with the exact same problem. After a reboot, some ports on the switch crap out and go offline. Reason is, as it turned out, the driver in ESXi, in combination with the NIC firmware, is having issues with auto-negotiation. The switch sees this as a flapping interface and shuts it down.A VxRail upgrade is scheduled to upgrade both NIC Firmware and the driver inside ESXi.Until then, as a workaround, inside vCenter / ESXi we hard-set the link-speed to 10000 FD and have had no issues since."
,user,time,comment
0,GCCJay,"Sep 17, 2018 10:58 AM","I'm trying to find an answer on whether it is recommended to use the VMware Paravirtual SCSI Controller in a vSAN environment, or if the default LSI Logic SCSI controller should be used. I have a four node vSAN cluster (each node obviously has DAS storage for the vSAN datastore) in a DellEMC VxRail. The Guest OSes will all be either WS2012 R2 or WS2016 in vSphere 6.5. From what I've read:The PVSCSI Controller IS recommended for SAN environmentsThe PVSCSI Controller IS NOT recommended for DAS environmentsBut we technically don't fit into either category. Can anyone help with a definitive answer?Thanks in advance!"
1,Great_White_Tec,"Sep 17, 2018 11:18 AM","For a large number of workloads, the default controller is sufficient. We do recommend changing to PVSCSI controller for better performance when we are working with workloads that generate large number of IOs (Oracle, SQL, etc.) In such cases, you may also need additional controllers to avoid bottlenecks. "
2,GCCJay,"Sep 18, 2018 8:58 AM","Thanks for the information... I understand this, and is what I've read in the knowledgebase. But nowhere does it mention what to do with vSAN. Are there any vSAN-specific considerations or recommendations on which SAS driver to use for a Windows Server-based VM? Or just follow the general practices from the knowledgebase article that you mentioned?"
3,GCCJay,"Sep 18, 2018 9:51 AM","I guess a more direct question would be....Is there any real reason NOT to use the PVSCSI driver for all Windows Server VMs? I'd rather use all LSI Logic SAS, or ALL PVSCSI for all VMs simply for consistency's sake... Unless there is a reason not to.Thanks again."
4,brandong08,"Sep 18, 2018 12:30 PM",Im using PVSCSI with my VM's running on vSAN with no issues.
,user,time,comment
0,vmrulz,"Sep 13, 2018 11:40 AM","We have been standing up some VxRail 4.5.x (vSphere 6.5u2, vSAN 6.6) clusters. We are coming up to speed on properly operationalizing vSAN into a much larger ""standard"" vSphere infrastructure that uses external block storage. We currently use a dedicated external vCenter HA service for these vSAN clusters. One of the things I've gleaned from research and vmworld is to keep plenty of white space in the vSAN datastore for maintenance and re-balancing. My understanding is that once you reach about 20% free space remaining vSAN rebalancing operations can affect storage performance.I'd like to be warned well in advance of that via a vCenter alarm or otherwise but I can't seem to find anything specific to vSAN datastore usage.As a side note is anybody aware of some good guides on operationalizing vSAN clusters for those of us that have been doing VMware with external storage arrays for years?ThanksRon"
1,TheBobkin,"Sep 13, 2018 5:21 PM","Hello Ron,A hearty welcome to HCI.""One of the things I've gleaned from research and vmworld is to keep plenty of white space in the vSAN datastore for maintenance and re-balancing.""You don't just want to keep space free for resilience (e.g. rebuild data back to FTT=1), maintenance (e.g. maintaining FTT=1/2 during maintenance if required by the business), but also for Storage Policy changes and if some/most of your data is Thin-provisioned then to allow for this growth (unless you are just adding more storage/nodes/clusters as it grows).""My understanding is that once you reach about 20% free space remaining vSAN rebalancing operations can affect storage performance.""When any individual capacity-drive reaches 80% used (default value - can be changed, do so with care) vSAN starts 'Reactive' rebalancing data off these drives to spread the data more uniformly across available capacity-drives (e.g. *moves* the data to less utilised drives). This is akin to resync traffic (as opposed to intentionally very slow 'Proactive rebalance' traffic) and as such when added to the current load of the cluster could of course result in contention (depending on how loaded the cluster is already and how much is being rebalanced):https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.virtualsan.doc/GUID-2EC7054E-FBCC-4314-A457-3DCAEDBCBD32.html""I'd like to be warned well in advance of that via a vCenter alarm or otherwise but I can't seem to find anything specific to vSAN datastore usage.""The vSAN Health puts warning/alerts on the cluster when certain thresholds are reached, noted here:https://kb.vmware.com/s/article/2108907There is a configurable vCenter alarm associated with this - 'vSAN health alarm 'Disk capacity'.""As a side note is anybody aware of some good guides on operationalizing vSAN clusters for those of us that have been doing VMware with external storage arrays for years?""depping &amp; CHogan book is a great in-depth start and is available online:https://www.vsan-essentials.com/Their blogs (+ lamw ) also cover a lot of ground in great detail.Launch HOL, make it, play with it, break it, fix it http://labs.hol.vmware.com/HOL/catalogs/lab/4687RVC is still ever-useful and the vsan elements are formatted nicely here (also spbm. commands are useful): https://www.virten.net/2017/05/vsan-6-6-rvc-guide-part-1-basic-configuration/Tons of info here and most of the whitepapers/technical articles updated and formatted for utility:https://storagehub.vmware.com/t/vmware-vsan/Read Communities posts in vSAN sub-community - we have answered a ton of queries, troubleshooted a lot of scenarios and provided a lot of general info in the last few years here.Bob"
2,vmrulz,"Sep 18, 2018 9:57 AM",Thanks Bobkin.. I can't believe I missed that alarm.  Appreciate all the additional information. Ron
3,TheBobkin,"Sep 18, 2018 10:08 AM","Hello Ron,More than happy to help - if you ever can't find the answer to something vSAN-related don't be shy to ask on here or contact support (we don't bite ).Bob"
,user,time,comment
0,Zaib_Khan,"Jan 24, 2018 4:24 AM","Hi All, I am running VXRAIL VSAN solution in my organization and currently i am facing hectic issue with my VSAN datastore free Space. My current free space is 5 TB, but when i am going to delete the virtual machines from disk then space remains the same. Recently i deleted the VM comprises 2 TB space from Vcenter (Delete from Disk), as per expectations the VSAN Datastore free space will increase up to 7 TB from previous 5 TB, but it remains the same as 5 TB.But when i am going to create any VM in Vcenter, then VSAN free space starts decreasing that it decreases from 5TB to downwards. My concern is that why VSAN datastore is not increasing free space after deleting the Virtual machine. "
1,TheBobkin,"Jan 24, 2018 5:47 AM","Hello Zaib_Khan""Recently i deleted the VM comprises 2 TB space from Vcenter (Delete from Disk), as per expectations the VSAN Datastore free space will increase up to 7 TB from previous 5 TB, but it remains the same as 5 TB.""Was this VM actually using 2TB (or any) space though?This can be determined by observing how much space it is consuming on disk (VM &gt; Edit Settings &gt; Hard Disk &gt; Drop-down info) - e.g. a Thin-provisioned 2TB vmdk added to a VM will take up near zero GB until data has been placed/generated on it.You can get a much better idea of how much space is being utilised by each VM Object using RVC:&gt; vsan.vm_object_info localhost/DataCenterName/computers/ClusterName/resourcePool/vms/*(This will print out the info for ALL VMs so ensure the SSH session buffer is enough lines to print it all out)Individual VMs can be looked at one at a time using:&gt; vsan.vm_object_info localhost/DataCenterName/computers/ClusterName/resourcePool/vms/NameOfVM/(Note if you are using created Resource Pools other than default root pool then change paths accordingly)You can also pull all this (and more) information using this very nice tool that is available via CLI:# python /usr/lib/vmware/vsan/bin/vsan-health-status.pycWhere are you checking free space from?Does 'df -h' via CLI show the same output as in the Web Client?Try looking at before and after deleting a VM in RVC using vsan.disks_stats &lt;path_to_cluster&gt;You can even identify which exact capacity device the components you are deleting are located on using the tools above and or the 'Virtual Objects' tab in Web Client.I have colleagues in VxRail team that can assist you with looking into this further if need be.Bob"
2,Zaib_Khan,"Jan 26, 2018 4:10 AM","Hi TheBobkin, Thanks for the update and support. I have explore more deeply on this issue and i found that i have set the ""Number of failures to tolerate value"" = 1 in Default storage policy which makes the couple of redundant copies in VSAN datastore. So i created a test storage policy, and set the ""Number of failures to tolerate value"" = 0 and move some VM to this test policy. By doing this now 3 TB space is increasing in my VSAN datastore. Kindly suggest that am I doing the right thing? or should i have to do some other recommended steps as per VMware recommendations. "
3,cyberpaul,"Jan 26, 2018 5:50 AM","Hi,decreasing ""Number of failures to tolerate"" is not a good idea. Yes, you've bought yourself some space, but you might lose your data if one of your devices fails. Please take a look at this for more detailed info:About Virtual SAN Policies As for the original issue, I think TheBobkin was onto something. The important message was that VM with 2 TB of provisioned space might in fact occupy much less disk space than that. This feature is called thin provisioning. You should take a look at ""used space"" rather than ""provisioned space"". By deleting a VM, one would expect to free the ""used space"" amount of data.Regards, Pavel"
4,jjonesprh,"Aug 18, 2018 6:37 AM","I have the same issue. The disk was thick provisioned.I deleted a 1.5TB second disk on a vm.Looking at the files in the Vm folder, I still see the second disk so that would explain why there was no recovery of the space in vsan.When I deleted the disk, I still had replication enabled for the VM, so maybe that had something to do with it not cleaning itself up.I removed replication but now I'm assuming I need to manually delete the file because it is still there.Update: Deleting the file had no effect, I also don't see vsan resyncing anything. I will open a ticket with EMC later"
5,TheBobkin,"Aug 18, 2018 11:56 AM","Hello jjonesprh,How exactly did you 'delete' this vmdk Object?Potentially you just deleted the descriptor (which is just a text file that points to the Object).Check via RVC if you have an Unassociated Object that matches the expected number of components of a 1.5TB Object (Probably around 16 if FTT=1 and splitting per 200GB).You can also identify Objects that used to belong to a VM by pulling the info on all Objects and then narrowing these down to just Objects with the GroupUUID of this VM (e.g. the namespace Object UUID). If you are not so handy with cmmds-tool + objtool then I would advise using this script to pull the info /usr/lib/vmware/vsan/bin/vsan-health-status.pyc - you *should* even be able to search for Objects with the friendly-name of the VM from the information this generates (regardless of whether the descriptor was deleted or not).Bob"
6,jjonesprh,"Aug 19, 2018 7:01 AM","Bob, Thanks for the information. This morning when I checked the datastore size, about 1TB was released. Since things are mirrored and was thick provisioned, I expected more back but maybe it was dedupe.I ran the python script and didn't see any matching data for the second disk I deleted. Everything looked normal as best I could tell.This action of not releasing the space after drive deletion, occurred on two separate vxrail sites, so it must be a bug of some kind. The second site still shows no space recovered, but I didn't delete anything from that site yet either.To answer your question on the deletion, I deleted just the vmdk file that showed the largest file size."
7,jjonesprh,"Aug 27, 2018 11:34 AM",Updates:For the site where I deleted the pointer to the disk:VMware engineers found no lingering or inaccessible objects so the system must have cleaned itself up.For the second site where I took no actions beyond deleting the secondary disk using the web interface:VMware engineers located the lingering disks and manually deleted all of them.Space is recovering now.We don't really know why the system was not releasing the space
,user,time,comment
0,demhyts,"May 18, 2018 7:49 AM","Hi All,I have a question, I have 4 node vsan (Dell EMC VxRail G Series) with total around 21TB...Recently our apps team move the VM from Hyper-V to vxrail, they export the VMDK to VxRail and it's consume big space then I reapply storage policy on each of VMs and free space increase from 700GB to 2TB but when I saw the VSAN Datastore the VM Overreserved taking too much space...Is it normal?, I know I dont have dedupe and compression because this is hybrid configuration and I dont have any OSR enable (always use 0% OSR) on my storage policy...Any workaround to reduce the VM Overreserved capacity usage on VSAN?...appreaciate any advice, thank you"
1,TheBobkin,"May 18, 2018 9:52 AM","Hello demhyts""Overreserved taking too much space...Is it normal?""VxRails apply a Thick-provisioned Storage Policy to all service VMs (VxRail manager, vCSA, PSC, vRealize LogInsight) so likely a few of the TB are these..vswp files are also Thick-provisioned by default.Disk Objects can also unintentionally be Thick as a result of using workflows that don't fully understand SPBM such as migrating/cloning VMs via the C# Thick Client.""Any workaround to reduce the VM Overreserved capacity usage on VSAN?...appreaciate any advice, thank you""Identify what Objects Thick-provisioned and Thin them - unfortunately re-applying the same SP(Storage Policy) won't work if it is Thick but the SP states OSR=0, in this case what I usually do is clone the current SP and add a rule for 'IOPS Limit=0', apply this SP to thin the Object (*shouldn't* cause full rebuild of the Object) and then either re-apply the original SP or leave it as is.Identifying these Objects can be done in numerous ways using RVC, objtool+cmmds-tool or scripts against the data generated by vsan-health-status.pyc:Save the vsan-health-status.pyc output:# python ./usr/lib/vmware/vsan/bin/vsan-health-status.pyc &gt; /tmp/Health.txtShow all Thick Objects:# for i in $(python ./usr/lib/vmware/vsan/bin/vsan-health-status.pyc | grep ""'proportionalCapacity': 100"" | awk '{print $2}'); do grep -A1 $i /tmp/Health.txt; doneShow all Thick Objects that are not FP=1(as .vswp are by default):# for i in $(python ./usr/lib/vmware/vsan/bin/vsan-health-status.pyc | grep ""'proportionalCapacity': 100"" | grep -v ""'forceProvisioning': 1"" | awk '{print $2}'); do grep -A1 $i /tmp/Health.txt; doneScript a colleague of mine wrote to generate this information using objtool+cmmds-tool:# for i in `cmmds-tool find -f python | grep 'proportionalCapacity\\\"": 100' -B9 | grep uuid | cut -d ""\"""" -f4` ; do /usr/lib/vmware/osfs/bin/objtool getAttr -u $i | egrep '(^Object path:|^UUID:)' ; echo ; doneSetting .vswp default to Thin:https://cormachogan.com/2016/02/22/vsan-6-2-part-5-new-sparse-vm-swap-object/Bob"
2,Great_White_Tec,"May 18, 2018 8:59 AM","Hi Demhyts,I wrote a blog post about this a while back. Hope this helps clear things up. FYI. In vSAN the swapthickprovision option is disabled by default. Prior to 6.7, you have the option to disable manually, per host.  https://greatwhitetec.com/2017/06/08/what-is-vm-overreserved-and-why-is-it-taking-so-much-space/ "
3,demhyts,"May 21, 2018 7:51 AM","Hi BobThank you for your input, I will try...Cheers"
4,demhyts,"May 21, 2018 7:51 AM","Hi, thank you very much I will schedule the maintenance window for shutdown and power on VMs "
5,TheBobkin,"May 21, 2018 7:57 AM","Hello demhyts,Out of interest - did those scripts come back with a lot of Objects that are proportionalCapacity = 100 with a Thin storage Policy applied to them?If it did then almost certainly encountering that issue as what is applied to the Objects should match the Storage Policy if they are 'compliant' with it.Bob"
,user,time,comment
0,Zaib_Khan,"Apr 17, 2018 8:55 AM","Dear All Expert,I have recently upgraded my Vxrail VSAN environment to Esxi 6.5 from 6.0. In that upgrade activity i keep all my VMs UP and upgraded the complete environment with no Downtime by vmotion the VMs to different hosts.In that activity i only left my 1 VM to power OFF. After complete upgrade when i am going to power ON that respective VM, it gives my the following error. (mentioned in screenshots)I have searched the error, and as per most blogs, it is suggested to re-register the VM by removing it from inventory and back to inventory.I did it but got no luck,Issue still remains the sameKindly guide me how can i overcome this issue, Needs your expert opinion in this,Thanks and waiting for the response,Muhammad Aamir Zaib Khan"
1,TheBobkin,"Apr 17, 2018 11:41 AM","Hello Muhammad,Is there any particular reason that you have a VM called 'vmwareconvertor' with disks attached to it called 'VMware vCenter Server Appliance_1.vmdk'? - I am assuming these are not the disks belonging to the vCenter in use by this VxRail (as how else would you have vMotioned anything). If they ARE the disks that are in use by the active vCSA managing this environment then this VM shouldn't have them added as disks and there is a good reason it can't power on.- Check where your current vCSA disks descriptors are located. Web Client: Select VM &gt; Edit Settngs &gt; HardDisk2 &gt; copy the path- If this matches the namespace/path of the disk descriptor that 'vmwareconvertor' is trying to use (namespace: /d58ab759-bedd-f398-ebb1-54ab3a773e5a/) then you should find out why this VM is pointing to disks that are in use and don't belong to it,Potentially this is an old VM that was created to convert an old vCenter?Did you create this VM and/or what is its function?Bob"
2,Zaib_Khan,"Apr 18, 2018 12:44 AM","Hi Bob, Thanks for the response.I have checked Vcenter Server Appliance descriptor file and that file is different as compare with vm ""VMwareconvertor"" Descriptor file the machine is using and pointing the file. I have deleted that corrupt descriptor file of Hardisk2 of machine vmware converter and it power ON successfully with no errors with its own descriptor file of Harddisk1. Issue has been resolved now, Thanks for the support,. Much appreciated. "
,user,time,comment
0,vmwarerati,"Nov 2, 2016 11:22 AM","Hi All, We are in process of evaluating a VXRAIL G Series Gen-4 (I guess that's vxrail 3.5 - released in June 2016?) - it's a hybrid configuration with 1 x 400GB SSD and 3 x 2TB per node in a 3 Node configuration to start with. I have the following questions:a) Does the vSAN 6.2 design and sizing guide applies to VXRAIL - when designing /sizing?b) VXRAIL G Series Gen-4 - does that comes pre-installed with vSphere 6u2 and vSAN 6.2?c) I have read some articles where a 3 Node VSAN cluster is not always the recommended option and a 4 Node is preferred / recommended - is this the case with VXRAIL as well?d) Calculating the usable capacity on VXRAIL (3 Nodes) with 1 x 400GB SSD and 3 x 2TB per node, that should reveal RAW 18TB (Base-10) capacity? with FTT=1 that will be 9TiB Base - 2 usable capacity - as that will make another copy of the data blocks across the cluster - Right?e) I will be hosting about 50 - 70 Virtual machines on this VXRAIL, as per my understanding the FTT=1 is on a per disk group basis, If I create a single disk-group across the entire 3 node cluster - I will have all my virtual machines protected with FTT=1 for a single failure ( of a host / disk in the disk group)?Will be grateful if someone can assist here - there's alot to consider when sizing the VSAN - even with vxrail - thanksBest regardsUmar "
1,MBrownWFP,"Nov 2, 2016 12:53 PM","I do not have direct experience with VXRAIL but my understanding is it is essentially a marketing name for Dell VSAN-Ready nodes that were designed and optimized specifically for VMware HCI.a) As far as I know yesb) Not sure this but should be easy to find an answer in VXRAIL documentationc) Yes 4-node is recommended but this is not a hard limit, we run a 3-node cluster in our DR datacenter (HPE gear not VXRAIL) and it functions fine. Maintenance can be a little tricky as there are not enough members to maintain duplicate objects plus a witness if one node needs to be evacuated but VSAN can rebuild the mirror objects once full membership is restored. For a primary site I would not recommend as you lose redundancy temporarily (but not availability).d) Yes, FTT=1 will duplicate all VSAN objects within the cluster resulting in 1/2 usable capacity compared to raw.e) FTT is configured within a storage policy which is then applied at VSAN datastore level (a second policy with different FTT can be created and set for individual VMs if required). You will have a single VSAN datastore the spans the cluster, this is where your VMs will live. Disk groups are created at the host level. Given your disk layout you will have one disk group per host comprised of your single 400GB SSD plus 3x 2TB HDD.Hope that helps.Matt"
2,aleksey,"Nov 7, 2016 9:46 AM","Matt is right on the money for all of the answers. I work closely with VXRAIL engineering teams, so I can confirm some of the answers.re: A and B are also firm YES.re: C. 3 node configurations aren't supported with VXRAIL 3.5 release, but will be supported with 4.0 (look for announcements from a couple of weeks ago at Dell World) "
3,IvanATDell,"Nov 8, 2016 1:10 PM","c) 3 node cluster is now supported on VXRail 4.0. Here is the link to official announcement: Dell EMC VxRail 4.0 Announcement  New Models, More Use Cases - Virtual Blocks"
4,BAURCHK,"Feb 28, 2018 6:17 PM",HIWe are evaluating to deploy almost the same configuration and spec.Is there any issues you have experienced so far that you could share with us ?Thanks.
5,depping,"Feb 28, 2018 11:26 PM","if there were issues, than they were more than likely software based, and I would assume that those have been solved by now.VxRail and vSAN has come a long way since 2016...."
6,srodenburg,"Mar 2, 2018 12:05 AM",Can confirm VxRail 4.x working fine.Would stay away from 3-nodes configs though (for all the reason already mentioned). Rather go with 4 smaller nodes than 3 bigger ones. Makes daily admin life better ;-)
,user,time,comment
0,hkg2581,"Dec 20, 2017 11:10 PM","If someone is looking forward to see and compare build numbers for VMware and VXRAIl manager here is the link which has the details in a single pane view . There are lot of new fixes with respect to vSAN /ESXI and Vcenter server it is always good to be in the latest version , however all upgrades for VXRAIL appliances is handled thru VXRAIL manager .Compare VXRAIL version with Vmware Build Numbers - VirtuallySensei"
,user,time,comment
0,mcbrowneps,"Nov 28, 2017 10:13 AM",We recently purchased a VxRail cluster running VMWare/vSAN. For all of our Windows clients we redirect all documents and desktop folders to a share that is hosted on a 2 node Windows Failover Cluster. I wanted to know if there was anyway to host the actual share directly on the VxRail/vSAN cluster. Since the VxRail/vSAN is already configured with multiple levels of failover I just don't see the need to set up a Windows Failover Cluster. I will still need to set ntfs permissions on user folders within the share. Is it at all possible to set up a share directly on the vSAN cluster that can have ntfs permissions applied to it? 
1,daphnissov,"Nov 28, 2017 10:19 AM","It can't be set up directly on vSAN because it doesn't offer file services, but you could provision a Windows VM and a separate drive which consumes storage from vSAN and share it out that way."
2,mcbrowneps,"Nov 28, 2017 10:30 AM","Only thing is, because this share will be for user files, I didn't want the drive to be directly linked to one VM in case I need to restart the VM or the VM becomes corrupt or something of that nature. Is there anyway to get the drive to act like a NAS? and then I can use DFS point to the storage/share?"
3,daphnissov,"Nov 28, 2017 10:47 AM","Since vSAN doesn't offer native file services, if you wanted to provide file services on vSAN it has to be done as part of a VM in some shape or fashion. Otherwise, you would have to create an iSCSI target on vSAN and mount it from a physical machine(s). vSAN doesn't support consuming those iSCSI targets by vSphere machines (or other VMs for that matter)."
4,mcbrowneps,"Nov 28, 2017 11:50 AM",Is it at all possible to setup two Windows VMs in a WSFC that share access to storage on the vSAN? 
5,daphnissov,"Nov 28, 2017 12:04 PM","SQL AAGs are, but not shared disks on vSAN."
6,TheBobkin,"Nov 28, 2017 12:06 PM","Hello mcbrowneps,As far as I am aware only AO-AG with non-shared disks is supported:""Microsoft Windows Server Failover Clustering (WSFC) is supported with VMware vSAN version 6.1 and later for SQL Server Always On AG (non-shared storage).""https://kb.vmware.com/s/article/2147661Bob"
7,mcbrowneps,"Nov 30, 2017 8:14 AM",So is my only option to set up a single Windows file server and attach storage from the vSAN pool to that VM? Could storage spaces be an option to get some kind of redundancy on the Windows side of things?
8,srodenburg,"Dec 3, 2017 10:49 AM","""Could storage spaces be an option to get some kind of redundancy on the Windows side of things?""In theory yes. It would protect against a failure with the VMDK housing the files. But I think it's overly complicated to be honest. In my long experience with VMware, I yet have to see a VMDK that just goes ""POOF"" by itself. If anything does say goodbye, it's mostly the NTFS filesystem which Storage-spaces does not protect against. A combination of Storage Spaces and REFS has a better chance of surviving filesystem errors and VMDK issues. REFS has a lot of self-healing logic that NTFS lacks. It's rather similar to ZFS in that respect.Forget about Windows Clustering. It's junk. Always has been. The times that a cluster-node did not take over from the other... I lost count...To answer your original question: vSAN / ESXi is not a NAS.Don't forget that a NAS also has a filesystem laid out over the disks (often EXT4 or ZFS) and despite RAID or Soft-RAID, if the filesystem goes corrupt, it's goodbye no matter how much hardware you throw against it."
9,mcbrowneps,"Dec 5, 2017 9:04 AM",Gotcha. So sounds like I should stop over thinking this and set up one Server 2016 file server VM and add some Virtual Disks to the VM and call it a day. Does that sound right?
10,virtualg_uk,"Dec 7, 2017 4:32 PM","That is what I would do.If you want ""overkill"" then take a look at this video from VMworld last year STO9423 - File Services on Virtual SAN"
11,hkg2581,"Dec 18, 2017 8:27 AM","@mcbrownepsI suggest you can setup a location VM replication within the same vSAN datastore using vsphere-replication appliance by keeping 3 or more instances of snapshot . So if you encounter a problem at the Guest level (ex: OS corruption), you can recover an older version of this VM from VR and use it again .Another suggestion is to keep your OS and File share on two different VMDKs (drives within your guest) and donot mix them , doing this if your OS is courrpt you may attach your file-share VMDK to a different Windows box running same flavor/release you should still be able to get back your data . Thirdly take backups backups are important .vSAN being an Object based storage , doesn't support hosting any NFS/CIFS share naively , however supports creating and export iSCSI targets from vSAN , which I believe you can present it to two different VMs using scsi bus sahre or something . I have never tried this before .Regards,Hareesh K G(Blog : virtuallysensei.com )"
,user,time,comment
0,habs3,"Dec 15, 2017 7:56 AM","Good Day,I received the following alarm 2 days in a row for the a 4 node vxrail cluster. I was wondering what it means and any info that would point to what is going on.Virtual SAN Health Alarm 'Memory pools (heaps)'thank you "
1,hkg2581,"Dec 18, 2017 7:04 AM","Poor All Flash vSAN Performance Please let me know if you are running a VXRAIL on newer Dell Models , there seemed to be know issue conflict between DELL-PTA-agent and a lsu plugin vib installed on the ESXi .See :VXRAIL hosts going non-responsive due to a plugin conflict Can you also let me know what you see in vmkernel.log and hostd.log during the time frame when these alerts are triggered . If you are seeing ramdisk full message , re-point the vsantraces from /vsantraces to your local sata-dom datastore See below example :esxcli vsan trace get  VSAN Traces Directory: /scratch/vsantraces Number Of Files To Rotate: 8 Maximum Trace File Size: 45 MB Log Urgent Traces To Syslog: true[root@esxi01:/vmfs/volumes/593eecd8-406d32ad-b242-0cc47ac2b410] esxcli vsan trace set -p /vmfs/volumes/593eecd8-406d32ad-b242-0cc47ac2b410/vsan-trace-esx-101/  After configuration :  [root@esxi01:/vmfs/volumes/593eecd8-406d32ad-b242-0cc47ac2b410] esxcli vsan trace get  VSAN Traces Directory: /vmfs/volumes/593eecd8-406d32ad-b242-0cc47ac2b410/vsan-trace-esx-101/ Number Of Files To Rotate: 8 Maximum Trace File Size: 45 MB Log Urgent Traces To Syslog: true [root@esxi01:/vmfs/volumes/593eecd8-406d32ad-b242-0cc47ac2b410]"
,user,time,comment
0,gopireddy,"Dec 28, 2016 1:25 AM","Hi, we are doing scp copy form one external ESXi to VxRail VSAN data store . when we tried SCP copy, VSAN data store doesn't allowed to copy the data due to permissions. so we have created a folder in VSAN Marvin data store and then we moved the couple of VM's successfully. when try move more VM's ( 3 Vm's of 150Gb , 250Gb and 1.5Tb were tried to copied in same folder under VSAN Data store and it failed.) through SCP then the error is coming no space left on Data store even though in VSAN data store 10TB of free space available.Each ESXi in VxRAIL 4 node cluster have one drive of 3.84TB data drive and 400GB SDD Cache each.Thanks in advance for your inputs.Thank you,Gopi Reddy"
1,depping,"Dec 28, 2016 6:58 AM","you shouldn't use SCP with vSAN, do a Storage vMotion instead!"
2,gopireddy,"Dec 28, 2016 8:52 AM","Hi, Thanks for your response. we can't use storage Vmotion, due to license issue in source ESXi.we are able to SCP VM from external host to VSAN data store and copied sucessfully, but when tried multiple VM SCP in same folder, it failed "" no space left"" where as total space available is 9TB , data trying to copy is 2 TB, any resolution to this ?"
3,MBrownWFP,"Dec 28, 2016 4:12 PM","This behaviour is caused by VSAN's object storage handling.""VSAN storage employs object-level storage, which is different from traditional block-level storage. The VSAN objects are managed through a storage policy which, for example, can allow for greater redundancy for some virtual machines over others. Because the reference in the VMDK file points to a VSAN DOM object, it cannot be copied through traditional means (SCP).""Source: Copying App Volumes AppStacks Between Environments That Use vSAN Storage - VMware Consulting Blog - VMware BlogsYou could use VMware Converter to do a V2V conversion of the source VM onto VSAN storage.Matt"
4,gopireddy,"Dec 29, 2016 2:09 AM",Thank you very much Matt.
,user,time,comment
0,CoTServerTeam,"Feb 6, 2019 6:39 AM","Scenario: Trying to either prove or disprove to management that we can bring VMs from AWS to on-prem vCenter.Conditions:- AWS instances originated in AWS (they were not migrated from our on-prem to AWS), so standard AWS processes won't work per AWS restrictions on exporting instances.- On-prem vCenter running 6.5, ESXi hosts running 6.5- Followed process from here to create VMX and VMDK files from AWS instance: Import an Amazon EC2 Windows Instance into VMware Workstation- Followed process from here to import to vCenter: Select a VMware Hosted Virtual Machine to Convert - All AWS instances are Windows 2012 R2 or higherCurrent State:The two processes used above work pretty well, right up to the point where I power up the VM in my on-prem environment. The Windows VM just sits at the loading screen with a spinning ""wheel"" (don't know what else to call it - the small dots that spin around in a circle). The logs aren't very helpful, as they really only show the VM logs from the ESXi perspective, not the guest OS perspective. I've reviewed the resulting VMX file that was produced during the ""export"" process and nothing really jumped out at me, and I also compared it to a working VM (not from AWS). There were a lot of lines in the working VM that were not present in the AWS VM, and some lines in the AWS VM that were not present in the working VM. I also compared them after the vCenter registration with the same results, just more lines that were added to the AWS VMX after the VM was registered in my vCenter. I've tried to tweak some of the hardware settings (remove the floppy, add the CD drive, change RAM/CPU, etc.) with no changes to the outcome. I've also tried the import process on two different vCenter environments (normal vCenter and VXRail environment) with same results.I'm attaching the pre-registered and post-registered VMX files if they can be useful. Any other information that I can provide that could shed some light on this, I'd be more than happy to provide."
1,mangydog,"Feb 7, 2019 4:52 PM",We use Zerto. Get a demo license for a POC! You can migrate cross hypervisor and into and back out of AWS and Azure.Orchestrated failback out of AWS. Pretty Sweet!Zerto Virtual Replication for Amazon Web Services (AWS) | Zerto 
,user,time,comment
0,Mervinraj,"Jul 13, 2019 12:06 AM","Hi,How to take the backup the Vcenter appliance in Dell VXRail.The following items need to be taken backup which are visible only on Vcentre Management Console VMware VCenter Server ApplianceVMware VCenter server Platform Service ControllerVMware vRealize Log InsightVxRail ManagerI can not able to take the backup in the VXrail appliance.Kindly help us to solve the issue."
1,sk84,"Jul 13, 2019 3:17 AM","At first, if you have specific VxRail questions, contact Dell EMC Hyper-Converged Support. Because only general VMware questions are answered here and HCI systems can be special in some ways. Second, it's always good to tell the people here what versions you're using, as there may be differences in how to do something depending on the version.To give you a general answer:The vCenter of VxRails is not really different from a normal vCenter. So you can make normal file based backups using the VAMI backend:File-Based Backup and Restore of vCenter Server Appliance "
,user,time,comment
0,Anthony1Robinson,"Apr 29, 2019 9:51 AM","Alright, I will try to make as much sense out of this as I can. I have a VXRail with 4 Nodes, tons of space, power, etc. From vSphere Web Client, I have a VM named ""Ticketsystem"" , I can move it between nodes all day without any issues. However, anytime I move around or do any work on any other VM, it will sometimes cause Ticketsystem to become unresponsive. Sometimes I can console in and do a proper reboot, other times I have to Reset the VM from vSphere. I cannot find an answer for this happening, nor a fix that seems to take. No other VM I have seems to be affected by this, the TicketSystem VM is the only one having this issue.Ticketsystem Specs: Win Server 2012r2 - 8 Sockets, 8 Cores per Socket. 16GB Ram &amp; 275GB HD. (Started with CPU 4/2 &amp; memory 8GB, gave it more power a couple months ago hoping that would help)I'm sure there is a ton of more info I should have put, let me know what else can help, and thank you!"
1,itsmattmagbee,"Apr 29, 2019 9:59 AM",Have you looked at the NUMA core/socket ratio? I have run into similar issues with my vm's until I started to follow this rule. I have found large 12 cpu vm's are best with 1 socket and 12 cores. After 12 cores you go up 1 socket to 2 sockets 2 cores and so on. If you can give that a shot ... I think you will find a possible solution. 
2,Anthony1Robinson,"Apr 29, 2019 11:37 AM",Thank you! I will give this a shot and let you know.
,user,time,comment
0,Baldygb,"Apr 20, 2019 12:36 PM","Help with vCenter converter standaloneWe just stood up our VxRails with a vSphere environment and now we are attempting to convert our existing Hyper-V virtual machines running on a failover cluster on 3 Windows server 2012R2 nodes using the free utility vCenter converter standalone to convert the Hyper-V vms to VMware VM on vSphere infrastructure. In the converter, after selecting a source Hyper-V server with credentials and viewing the VMs running on this node, i selected a virtual machine that is powered off then click next but i get an error: ""Permission to perform this operation was denied"".Another article i read suggested to edit a User Account Control in Security Options in Local Security Policy on the source VM but this didn't work either.Anyone have a tip on converting Hyper-V vm to wmare vm?ThanksBaldygb"
1,ThompsG,"Apr 21, 2019 3:02 AM","Hi Baldygb and welcome the community!Sorry if this has already been tried but can you make sure you are running VMware Converter by using the ""Run as Administrator"" option or have UAC disabled on the machine that you are running Converter from?Kind regards."
2,Baldygb,"Apr 23, 2019 10:58 AM","The solution was to login to the host Hyper-V server, enable the built-in Administrator account and give it a password. Running the converter from the Hyper-V or from a VM in vSphere we were able to clone and convert the VM.Thanks"
,user,time,comment
0,vmrulz,"Oct 22, 2018 5:09 PM","Greetings,We are attempting to configure dual Tesla M10 GPU cards in our VxRail E570F servers (Dell 14G 2U box) to be used for a Citrix Xendesktop VDI cluster. We are finding that we cannot get ESXi to properly allocate memory to these cards with the default MMIO setting = 56T. Changing the setting to 12T or 512G per this Dell article causes the server to purple screen a few minutes after full boot. This is using the latest available VIB from NVIDIA on ESXi 6.5u3c. https://www.dell.com/support/article/ht/en/htdhs1/sln308065/dell-poweredge-14g-esxi-returns-failed-to-initialize-nvml-unknown-error-with-nvidia-gpu?lang=enNVIDIA support portal is down so I'm attempting to get knowledge elsewhere on this tech.Thanks for any advice."
1,vmrulz,"Oct 31, 2018 10:53 AM",Interesting lack of responses on this tech. We found that a card was not properly seated causing the PSOD. We still however have a problem with the card and it is being replaced.Should be an interesting adventure.
2,flynmooney,"Oct 31, 2018 11:30 AM",We had a NVidia Tesla M6 card in a host running 6.0U3 for a while. It was a total pain to setup and get working. I had to enlist both VMware and NVidia for support to finally get it working. If I remember right there was something messed up in a the xorg config file which VMware support found and sent me an updated file.
3,vmrulz,"Nov 2, 2018 9:45 AM",Yeah and the caveats continue to pop up.. The current incarnation of Nvidia drivers and ESXi do not allow for VMware HA or DRS or vmotion. So throw out all the things we've become accustomed for HA and load balancing for better graphics performance. Argh.
4,Dave_the_Wave,"Nov 2, 2018 3:32 PM","So basically who is really doing this?Does this mean Nvidia has sold about total 10 of these cards worldwide?Maybe that's why they are so expensive. You order one, pay a deposit, and they go build one for you like a Maybach.I'm getting the feeling the buyer pays tens of thousands of dollars to join a hardware beta program."
5,TheBobkin,"Nov 2, 2018 4:10 PM","Hello vmrulz,""The current incarnation of Nvidia drivers and ESXi do not allow for VMware HA or DRS or vmotion.Yes, as you are essentially using passthrough of a (part of a) piece of hardware - making a functional method of 'vMotioning' a current state graphics card IO (without interruption) isn't a trivial task.""So throw out all the things we've become accustomed for HA and load balancing for better graphics performance. Argh.""Sorry to hear that you are frustrated regarding this, but I assume you have been in IT some time and understand that in many spheres often benefits/features come with trade-offs/caveats based on the fundamentals of how these features function (e.g. expecting 10x compression with no negative IOPS impact).@Dave_the_Wave""So basically who is really doing this? Does this mean Nvidia has sold about total 10 of these cards worldwide?""Actually they are fairly common - look at any Horizon View cluster or similar VDI product (schools/colleges is a big market) and they will generally be utilising GRID cards.Bob"
,user,time,comment
0,ebagini,"Sep 21, 2018 12:48 PM","Author : william rogersURL : http:////docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.virtualsan.doc/GUID-05C1737A-5FBA-4AEE-BDB8-3BF5DE569E0A.html Topic Name : Deploying a vSAN Witness Appliance Publication Name : Administering VMware vSANProduct/Version : VMware vSphere/6.5Question :Prezados, em um cen&aacute;rio onde temos o seguintes: 01 Site com 04 nodes vxrail com vsan e o segundo site com a mesma infraestrutura. O meu servidor witness para qu&oacute;run em um 3 site &eacute; obriga&acute;t&oacute;rio? Qual seria a op&atilde;o de montar um ambiente em streched cluster com apenas 2 sites? att."
1,Diego Oliveira,"Sep 22, 2018 7:52 AM","OiVoc&ecirc; deve tr&ecirc;s sites para usar um cluster estendido do vSAN. Isso n&atilde;o &eacute; apenas para fins de funcionalidade, mas para fins de pr&aacute;tica recomendada, se um local fsico inteiro falhar. Veja esses artigos:https://t.co/VtEUYpqPJP Understanding the vSAN Witness Host - Virtual Blocks "
,user,time,comment
0,billdossett,"Jul 2, 2018 10:36 AM","Hi, I though I had found an answer on this but now can't find the post...I am migration from our old hardware to a new VXrail. I did this once before but the VXRail was on VSphere 6.0 and the old hardware is on 5.1 ESXi hosts.New VXrail is 6.5 and I can't add a 5.1 host to the datacenter so I can't use migration.I have used replication before, but not sure if I can replicate from a VR version 6.1 to VR version 6.5... does 6.x to 6.x work or does it have to be 6.5?Would like to use replication to migrate if possible anyway... but if not guess I will have to upgrade a host.ThanksBill"
1,Diego Oliveira,"Jul 2, 2018 11:04 AM","Hi, Requirements for Site Recovery Manager 6.1:You must run the same version of Site Recovery Manager on both sites.You must run the same version of vCenter Server on both sites.About Protected Sites and Recovery SitesThis is the only officially tested and supported configuration.If this answer has helped you, please mark it as answered"
2,vFouad,"Aug 21, 2018 7:48 AM","This is no longer the case, I know this is an older post, but you can now be a little more version agnostic with vSphere Replication 8.1So this question would now have a simple answer."
,user,time,comment
0,vincenjw,"Jan 16, 2018 2:27 AM","Hello,I deployed at a customer offices several Dell R740 servers (laste BIOS) with vSpehre 6.5U1 and vSAN.I'm using Intel X710 DA 10Gb (2 ports) NIC cards.My client would like to use SR IOV for somes VMs. These NICS support SR IOV (with i40e and i40en drivers). It's the first time I have to use SR IOV.I activated ""SR-IOV Global"" in the Dell BIOS servers and I uses the ""esxcli system module parameters set -m NIC_Driver_Module -p ""max_vfs=n"""" commands on esxi hosts.I tried with a i40e driver and the i40en drivers.But after the reboot vSphere still tell me the cards are not supported for SR IOV. I did the same on a VXrail cluster (vSphere 6.0) with x540 NIC Intel it works well... Do you have any ideas ?Thanks for your help Vincent"
1,aatsemet,"Jun 25, 2018 9:38 AM","When you enable the ""SR-IOV Global"" on the BIOS for the Dell Servers the configuration is not pushed to the NIC setting so you will need to enable SR-IOV on the NIC as well. Boot to BIOS Setup:- Go to System BIOS &gt;&gt; Device Settings &gt;&gt; Ethernet Converged Network Adapter X710 &gt;&gt; Device Level Configuration &gt;&gt; Virtualization Mode = SR-IOV.- Save settings and reboot. "
,user,time,comment
0,ichayan,"Jan 22, 2018 9:14 AM","Hi All,Thanks in advance. I am in the process of migrating from our esx 5.5 to VXRail-vSAN. We have a two node physical windows storage server 2008 file cluster. The nodes have all their disk, including boot disk residing on compellent SAN storage over iSCSI. The current esx 5.5 cluster is connected to the same compellent storage. The VXrail cluster can access the the same compellent.I am wondering the best option to convert this physical cluster to a VM, there is no requirement for this to be a cluster in the new world. I was thinking of the below.The two nodes arenas01 - passivenas02 - active1. P2V the OS partition of the passive node to the old esx environment.2. Before powering up the VM do a Raw Device Mapping of the file server LUNS and quorum LUN on the VM.3. Shutdown the passive physical node.4. Power up the VM, install vmware tools. At this point this will be part of the same cluster the active node being the physical nas02 and the passive one now virtualized.5. Once happy with the virtual server, move the resources from physical to the virtual and the virtual server now becomes the active node.6. Once happy with the services, break the cluster.7. Map the LUNS to VXRrail.8. Powerdown the server at the old esx server and register in VXrail.9. Power up the VM and do a storage vmotion========================================1. Does the above seems like a workable solution?My question is around the RDM should I do physical or virtual? If I do virtual will that affect the existing data on the LUNs. Would it be easy to map the LUNs to the VXRail and register the VM and do storage vMotion.2. What would happen to the cluster IP?At the moment the cluser is being accessed via the cluster hostname/IP for shares. What would happen to this when I break the cluster?Any help is highly appreciated."
1,daphnissov,"Jan 22, 2018 8:47 AM","If there's no requirement for this to remain a cluster, why not break the cluster now while it's on physical and do a P2V of the entire system (OS + data)? VMware Converter will capture all available partitions and convert them to virtual disks, which gives you much more flexibility in what you can do with it without reliance upon pRDMs."
2,ichayan,"Jan 22, 2018 9:12 AM","This file server holds around 5TB of data. The P2V like you have suggested has to take place over a 1Gbps link. But if I can get it working at the SAN level, I will have a 10gbps connectivity. Since I am working on the passive node less downtime as well."
3,daphnissov,"Jan 22, 2018 9:38 AM","Well, yes it'll take more time, but the conversion process will capture the changes to the files on the last pass at which time you can do the cutover."
4,ichayan,"Jan 22, 2018 11:06 PM",How do I break the cluster without dataloss and do the cleanup so that the p2v machine looks clean? 
,user,time,comment
0,joeraja,"Jul 12, 2019 11:49 PM","Hi Team,Im using the Dell VXRAIL.. Can any one tell, how to take backup of Vcenter appliance in Dell VXRAIL...."
1,sk84,"Jul 13, 2019 3:18 AM","At first, if you have specific VxRail questions, contact Dell EMC Hyper-Converged Support. Because only general VMware questions are answered here and HCI systems can be special in some ways. Second, it's always good to tell the people here what versions you're using, as there may be differences in how to do something depending on the version.To give you a general answer:The vCenter of VxRails is not really different from a normal vCenter. So you can make normal file based backups using the VAMI backend:File-Based Backup and Restore of vCenter Server Appliance "
,user,time,comment
0,haggisnneeps,"Jun 10, 2019 12:35 PM","Hi,We have an old system and a new systemOld is IBM v7000 with all FC hosts attached. VCenter1.oldsystem.local. mix of esxi 5.5 and 6. old hosts. vcenter 5.5New is VxRail VSAN. NewVcenter2.newsystem.local vcenter 6We would like to migrate a load of VMs off the old onto the new. We only have enterprise plus licensing for the esxi/vcenter 6We cant do shared nothing migration because of different SSO domains and licensingSince the IBM storage supports ISCSI too, we would like to create a LUN on old system and migrate VMs in batches by storage vmotion then unregister from old and register on new then storage vmotion to VSAN disksTo do this though, requires we have a LUN presented as FC to Old VCenter hosts/clusters and ALSO presented to new VCenter hosts as ISCSI LUNIs this going to casue corruption on the datastore?We would prefer, if possible, to migrate all the VMS ""Hot"" and have no downtime - is there a way to do it? We have VEEAM and VMWare Converter available"
1,daphnissov,"Jun 10, 2019 1:00 PM","We cant do shared nothing migration because of different SSO domains and licensingWell, you wouldn't be able to do this regardless if your source vCenter is at v5.5.To do this though, requires we have a LUN presented as FC to Old VCenter hosts/clusters and ALSO presented to new VCenter hosts as ISCSI LUNIs this going to casue corruption on the datastore?I've not simultaneously presented the same LUN to two separate sets of hosts via two separate block protocols. Are you sure your IBM system will even support this with ESXi? From a data integrity standpoint, VMFS is cluster aware and performs low-level locking. I would not think corruption would be an issue here.We would prefer, if possible, to migrate all the VMS ""Hot"" and have no downtime - is there a way to do it?No, that's not going to happen. Even if you used VMware Converter or a Veeam Quick Migrate, you're still going to have switchover time."
2,haggisnneeps,"Jun 10, 2019 1:20 PM","Yeah coz basically the VEEAM and Converter methods are the same as manually unregistering/re-registering?Was hoping there was maybe something we didnt know about to do this or we are missing somethingCame across an article about VSAN iscsi target service - would this allow us to migrate directly onto the VSAN instead of the in-between hop of the ""shared"" LUN?Thanks"
3,daphnissov,"Jun 10, 2019 1:23 PM","Yeah coz basically the VEEAM and Converter methods are the same as manually unregistering/re-registering?There's a lot more going on here than just that, but the quickest approach will still have you powering off and registering on your VxRail cluster.Came across an article about VSAN iscsi target service - would this allow us to migrate directly onto the VSAN instead of the in-between hop of the ""shared"" LUN?No, that's separate functionality altogether."
4,sk84,"Jun 10, 2019 1:31 PM","Came across an article about VSAN iscsi target service - would this allow us to migrate directly onto the VSAN instead of the in-between hop of the ""shared"" LUN?No, that's separate functionality altogether.In addition: It's not supported to connect a vSAN datastore to another ESXi host via iSCSI."
5,haggisnneeps,"Jun 10, 2019 1:45 PM","ok but if i am connecting a vsan host to another datastore thats ok?via iscsi on a single host in the vsan cluster - sharing 25GB nics and that datastore contains VMs that are in use on the old vcenter and those hosts connect to that datastore via FCThen once the VMs are on that shared storage, we do the unregister/and register and then storage vmotion them onto the vsan (as per the Original Post)If all thats OK then i suppose its all we gotThanks again"
6,daphnissov,"Jun 10, 2019 2:08 PM","ok but if i am connecting a vsan host to another datastore thats ok?via iscsi on a single host in the vsan cluster - sharing 25GB nics and that datastore contains VMs that are in use on the old vcenter and those hosts connect to that datastore via FCWould it technically work? Yes, it should. Is it supported by DellEMC on VxRail even as part of a migration? Dunno, that you'd definitely want to ask.But I think the more critical questions are regarding the storage situation on your IBM array:Will it work to connect to a given LUN via FCP and iSCSI simultaneously? Don't know. Probably.And is it supported? Really don't know at all.Some big question marks for you to investigate through official channels here if you want to go about this migration in a safe way."
7,haggisnneeps,"Jun 11, 2019 1:15 AM","So could we then just disconnect all these hosts from the ""old"" vcenter and connect them to the new vcenter and transfer the licenses across? Then we can just do normal vmotions?"
8,haggisnneeps,"Jun 11, 2019 4:05 AM",ok so i suppose the last loose end to tie up would be nestingCould i nest a 6.0 or 6.5 host inside a vcenter 6.7 managed VM Host environment and migrate 5.5 hosts into it?Would there even be any benefit other than everything would now be on the VxRail and we could then do other migrations at leisure?clutching here :-)
9,daphnissov,"Jun 11, 2019 4:45 AM","You can't do ""normal"" vMotions because you still wouldn't have the same storage. See my previous responses for challenges there. The other challenge even if you did is the matter of different networks. You can't join the vDS on the new vCenter side because it will be at a higher version than the 5.5 hosts can use, which means the old hosts would be on vSS. And you can't vMotion and change the port groups simultaneously because those APIs didn't exist until 6.0."
10,daphnissov,"Jun 11, 2019 4:47 AM","Could i nest a 6.0 or 6.5 host inside a vcenter 6.7 managed VM Host environment and migrate 5.5 hosts into it?I don't know what you're asking here, but it doesn't matter.We've already fleshed out all your options at this point and you've got some homework to do yourself."
11,haggisnneeps,"Jun 11, 2019 7:52 AM","Hi Thanks agai for ll your help it has been incredibly usefulIt has pushed us down a path where we think we have found a way to do this - or a couple of ways. Maybe.1) Fling seems to be able to do it and also says there is a way to do it using PowerCLI ..We will try Fling but not PowerCLI (yet)2) VEEAM can also do a quick migration ( we haven't figured out whether its a version limitation that stops us from doing a hot migration) but when it takes the VMs across, it cant automatically assign a mapped NIC to one of the port groups on the DVSwitch so there is manual intervention to get the NIC upwe can alleviate this by creating Standard vSwitches on the VxRail that will be the same as the originating Standard vSwitches but it will still be cold migration (its the community version of VEEAM - like i say i don't know if the full Enterprise version would allow a hot migration)So Fling is what we are about to try - albeit not ""supported""So we know there is no ""supported"" method of doing this. We will see if there is a ""working"" way of doing itIll let you know how it goes"
12,haggisnneeps,"Jun 11, 2019 7:58 AM","FYI presenting a LUN via FC, putting the files on there and then presenting to the other side via ISCSI didn't work very well. Didn't work at all actually.Both sides saw the disk and locked it and we had to reboot the VxRail node it was presented to via ISCSI to remove it"
13,haggisnneeps,"Jun 11, 2019 8:00 AM",And the fling in question is the cross VCenter Workload Migration Utility
14,daphnissov,"Jun 11, 2019 8:19 AM","As I said earlier, due to your old version of vCenter, the Fling will not work. PowerCLI the same. They all use the same underlying APIs and those don't exist in 5.5."
,user,time,comment
0,rainbowj,"Mar 28, 2019 3:42 AM","I failed to upgrade PSC using Vxrail because vxManager is rebooted. Then I go to PSC console to double check if is completed or not, it does not completed. So I reboot PSC again, and plan to manaully upgrade PSC via mount ISO to CD follow below steps: 1) upload patch to VSAN 2) go VC UI, and select PSC, select ""Edit settings""-&gt; CD/DVD-&gt;Datastore ISO File, and choose the patch, select ""Connected"" and ""Connect At Power on"" 3) login psc vami: HTTPS://PSC_IP:5480, ""Update"" tab-&gt;check CD, it will not display the mounted ISO Go to SSH, input ""mount /dev/cdrom /tmp/cdrom"", and list /tmp/cdrom, it can list packaged rpmIf I use same step to mount ISO patch to VCSA, and got VCSA VAMI page, it will display this patch on Update tab.Question: When Can I find some clues or logs why it does display? Any suggestion or workaround is welcomed."
1,BenediktFrenzel,"Mar 28, 2019 4:21 AM","As you pointed out that it is a vxrail System, I would highly recommend to open a Support Request with DellEMC first.-- Ben "
2,rainbowj,"Mar 29, 2019 1:20 AM","After 1 day, PSC is upgraded by itself to the new version, and I can mount another ISO again on VAMI UI. I promise I do not change anything on PSC. So I think it should be PSC issue, it is in a instable status, but UI does not show any message tell user information, which log should I check to identify this status?"
3,BenediktFrenzel,"Apr 1, 2019 4:57 AM","If you still suspect something is wrong with the PSC in your VXrail environment, I still think opening an Support Request with DellEMC is your best choice, as they do the support for VXrail."
,user,time,comment
0,TheSwitcher,"Dec 27, 2018 12:31 PM","Hello VMware community!So I have been working with vSphere and Veeam for a few years now. I recently was hired to a new organization which utilizes new technologies that I am not really familiar with - vDS and vSAN (VxRail). Basically, I have been pretty much used to a more simple environment with embedded VCSA and now working with a physical server running vCenter 6.5 update 2 on top of Windows Server 2008r2 sp1. The virtual environment is backed up by Veeam VMs.My issue is the lack of patch management for the current severs here...vCenter being one of them. I purchased a physical host license from Veeam to backup the server and my question is not regarding this. My question is the availability of vSAN and the vDS during the potential vCenter outages that may happen during the patching of this server. I have done some research on this I wanted to post this to get some input on this. Even links to KBs would be appreciated.I am no stranger to fixing things I break but proprietary equipment (VxRail) with technology I haven't worked with to much (vSAN, vDS) scares me a little.Thanks"
1,daphnissov,"Dec 27, 2018 12:38 PM","You don't have to worry about vSAN or vDS ""breaking"" if vCenter is unavailable. They continue to run if vCenter itself is down. What is more concerning is the fact that this employer continues to run vCenter not only on Windows (which is a dead end), but on a physical server. First order of business, I think, would be (after ensuring the environment is sufficiently protected) to get them upgraded to vSphere 6.7 (provided support and compatibility is verified from end-to-end) where you can migrate off the physical server to the vCenter Server Appliance (vCSA)."
2,TheSwitcher,"Dec 27, 2018 3:06 PM","I think this was a product of an upgrade where some engineers did not convey the importance to migrate off of a physical windows server or someone got a little upgrade happy and upgraded the physical server to 6.5.I do agree with you. Looking into this, there is a lot of compatibility requirements when doing this process. Updating VxRail, then vCenter, then all ESXi hosts, then witness host I assume as it is a stretched cluster. Frustrating because this was all done around July and we will have to go through a major upgrade again to get the vCenter off of a physical machine. Sigh..."
3,ChrisFD2,"Dec 27, 2018 11:45 PM","If you are upgrading to vCenter 6.7, be mindful that as of the time of this post Veeam doesn't work with Update 1.KB2784: Consistent backup failures after installing vSphere 6.7 U1 There is a workaround documented above, although I wouldn't personally upgrade in an enterprise environment until patch 4 is released (any time soon). 6.7 build 9451876 would be fine, providing everything in your environment is compatible.Please check the VMware Compatibility Guide - System Search and the VMware Product Interoperability Matrices before going ahead with any upgrade.As mentioned above the best method would be to upgrade to the vCenter appliance, it should be relatively straight forward - deploy new appliance VM, transfer settings from existing server, check everything works and then decommission the old server. Ensure you take a full backup before you start.Good luck and feel free to ask any questions."
4,sk84,"Dec 28, 2018 4:01 AM","As others have mentioned, vDS and vSAN are not affected when you patch your vCenter. They will continue to work even if the vCenter has failed. Only the management functionalities are limited in this case.However, since you are using VxRails with an external vCenter, the patch management process may be slightly different. I assume you have a VxRail Manager appliance and a VxRail web frontend with automatic upgrade process. In this case the upgrade must be done in the following steps: - Update your external vCenter via VAMI (but please check the release notes of the VxRail software what vCenter build numbers are supported for the current VxRail release).- Update your complete VxRail cluster via the update process in the VxRail Manager web frontend (this includes all server firmware updates, ESXi version, vSAN and vDS)."
,user,time,comment
0,Bruticusmaximus,"May 15, 2017 1:20 PM","How can I manage multiple Virtual Centers from one interface without Linked Mode? Like back in the Virtual Center Administrator Portal days? We seem to have a Virtual Center Sprawl issue. Horizon View? ....... you need a separate VC. VxRail? ...... another VC. Vblock? ...... another VC. Our DR site? .... another VC. With VxRail and Vblock, they won't even let us use Linked Mode or a shared PSC or anything like that. I was wondering if there's some over-arching web interface that could manage them all. Even a third party solution. When a tech needs to do something with a VM through VC, they've got to guess which VC they need to connect to. It's kind of a pain.Just basic functionality really. Reboot VM, get a console, etc. I don't need to patch hosts or do snapshots."
,user,time,comment
0,RyanC777,"Apr 16, 2019 1:49 PM","Background:My company is getting two groupings of VxRail hosts in two different locations. Both of these VxRail groups will have their own vCenter. We are building Horizon 7 on these vCenters. The big thing is that if one of the sites goes down, the other site is able to run the virtual desktops.So my question is, Is it possible to build Horizon (using Linked Clones) in such a way that allows a user to turn on a machine from either location. Is it possible to have a replication of a composer between both sites? Any info would be helpful, Thanks,Ryan"
1,vJoeG,"Apr 17, 2019 7:46 AM","Is there a reason you're going linked clones vs Instant clones?You have 2 sites, let's call them SiteA and SiteB You will install/configure Horizon in each place and create pools of VM's. Example ICPool1 (50 users) Using Cloud Pod Architecture you will link the two sites together so they are aware of each other. Then using global entitlements assign users to each pool. Then using Home Site you can ensure that users will always connect to an Instant Clone VDI in SiteA as long as the site is available. If something happens to SiteA the Cloud Pod (CPA) will check entitlements, see that users are also entitled to the pool in SiteB and direct them there for an Instant Clone VDI while you research what's happening in SiteA.Here are a couple of links for Cloud Pod Architecture that might provide some additional details.Cloud Pod Architecture Overview VMware Horizon 7 v7.5: Cloud Pod Architecture - Feature Walk-through - YouTube "
2,RyanC777,"Apr 17, 2019 7:55 AM","The reason behind Linked Clones is that we were only able to get Horizon Advanced instead of Enterprise. Therefore, we can't use Instant clones and have to use Linked clones. (Correct me if i'm wrong)Originally, we were trying to get Enterprise and that exact example setup you talked about is how we were going to build it. Is it possible to do something similar with Linked Clones? Or even a way to have failover if a pod goes down, and users able to access a version of there desktop on another pod.Thanks for the response."
3,vJoeG,"Apr 17, 2019 8:07 AM","Ah I see, yes you need Enterprise licensing for Instant clones.You can still create a setup like this using Cloud Pod, global entitlements with floating linked clone pools. This way users are not tied to any one machine then the same scenario would apply.During setup, you would just need two composer servers (one in each site) That should work well too. "
4,RyanC777,"Apr 17, 2019 8:09 AM","Awesome, thank you Joe. This is exactly what i wanted to know."
5,vJoeG,"Apr 17, 2019 8:12 AM",You're very welcome. Glad I could help!
,user,time,comment
0,RSEngineer,"Jan 8, 2019 5:54 PM","IHAC with mixed workloads with a great disparity in their desktop composition. Have some 3D CAD graphics virtual desktops and then some lower-level knowledge and task workers who do not require the same kind of CPU speed. For CAD desktops, we are talking about 6 vCPUs per desktop. And with CAD, high speed cores are needed, like 3.2Ghz, at least. High speed cores dont usually come in high quantities, so CPU is a limiting factor when it comes to consolidation ratios.That said, there will be the smaller task and knowledge worker desktops sharing the same V-Series VxRail platforms with the CAD desktops. Each type will use 2 vCPUs per desktop. That will allow me to increase the consolidation ratio more than it would be if all the desktops were the 3D CAD ones. BUT it seems like a colossal waste to have them using the 3.2 Ghz cores. Besides building a totally separate cluster for the task and knowledge worker desktops with slower CPU cores (and more of them), can they be deployed on the same cluster with the CAD desktops BUT in a way so as to use only a portion of the 3.2 Ghz cores and perhaps allow me to double up their number (or something like that) and further increase consolidation ratios?Am I making sense or talking kaka?"
1,techguy129,"Jan 9, 2019 5:10 AM","When a VM needs CPU cycles, it needs all available cores, based on its vCPU, available all at once on the host to schedule. It's not necessarily based on CPU speed. When mixing VM's with different vCPU counts on the same host, your biggest concern is the co-scheduling. You don't want contention between the VMs waiting for available CPUs. Since this is VDI, you can consolidate at a higher rate then typical server workloads. Something like 10vCPU to 1 physical core. You have to watch your CPU ready % and Co-Stop % to see if CPU is really an overallocated. With that all said...With your scenario, if you have a few CAD machines at 6vCPU and a lot of 2vCPU, you have potential to have the above conflict. As far as a waste, if it has higher Ghz then the smaller workloads will be completed faster."
2,RSEngineer,"Jan 10, 2019 4:47 AM","Hi and thanks. I cant get 10:1 overcommit because the CAD app requires high CPU speed, and because of that, the core counts are low. All I can get on VxRail V-Series are 2s/24 cores @ 3.2Ghz. With 2:1 overcommit, all I can get are 8 CAD desktops per appliance.Or a combination of a fewer (than 8) CAD desktops and a small handful of others.  I wouldnt do more tan 2:1 precisely for the reasons you mentioned above. Imagine having, say, 4:1 overcommit while still having to schedule 6 vCPUs for every workload to execute...So would it make more sense to build a separate physical cluster just for the CAD VDIs given the limitation on overall core count they impose? If I separate the normal workloads in their own cluster and use CPUs at slower speeds, then I can leverage like 6:1 overcommit (I wouldnt go higher than that to start off with), and get CPU with much higher core counts to boot, and get like 50 or more VMs on the same appliance."
,user,time,comment
0,pchapman,"Aug 22, 2018 6:26 AM","I am curious if anyone has any information on this KB article. I came across it while troubleshooting another issue, and found that it makes sense to implement on a new environment I'm building. The new environment is a VXRail 4.5.212, Horizon 7.5.1 with GRID M10 cards all using the M10-1B Profile.I enabled the feature per the article, but it broke provisioning in a spectacular fashion. I have some screenshots at home I can attach later, but I found it very odd. After disabling the feature per the article, I ended up having to manually reboot ESX hosts before instant clone provisioning would start working again.Can anyone explain what exactly this feature does to enable ""max performance mode""? I thought that setting the VM's to distribute VM's across GPU's in vSphere was all that was needed?"
,user,time,comment
0,GarTomlon,"Jun 18, 2018 7:18 AM","We have horizon view 7 running in a VXRail / VSAN environment. This is a single cluster. To assist with additional capacity until we can expand our VSAn and VXRail, we have introduced two Cisco UCS blades into this environment. We have created a new cluster for these two blades. These two blades are obviously not part of the VSAN. We have zoned two Luns to these hosts for storage. Our question is can we provision RDSH farms over to this new cluster? IF so, how would we do this? "
