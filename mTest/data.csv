1.json,t3_92301q,Senior VXRail administrator,2018-07-26 23:00:52,I've never supported VXRail infrastructure before - but I'm fairly savy at VMWare and pretty comfortable with it.  Most of my support on VMWare has involved Dell equipment both traditional servers and blade infrastructure.  With that I'm fairly confident about learning VXRail and supporting it as a SME - until I read some of the negative posts on here about VXRail and it's shenanigans. With that said i'm in the final stages of an interview process to become the SME VMWare guy for a company that is converting everything to VXRail (1000 vm servers) and the CIO pretty much bet her job on it - should I continue with the process?
1.json,t3_92301q,t1_e32n9fb,2018-07-26 23:24:10,"The way i normally handle something like that is give a pros and cons and let the CIO pretty much know that i do not recommend it but that does not mean it is a deal breaker. I try to find i guess a kind way of saying ""I might be wrong but i feel it may not be the best choice"" Normally managers will dismiss your recommendations especially if his/her mind is made up about a specific product.When it blows up i know your left to deal with it and i feel your pain about having to re-evaluate but depending on your friendship/relationship with your CIO but normally i say ""i just don't want this to reflect badly on me"" and usually the response will be positive depending on your CIO of course."
1.json,t3_92301q,t1_e32o88e,2018-07-26 23:37:43,"Just FYI it is VMware not ""VMWare"" and VxRail not ""VXRail"""
1.json,t3_92301q,t1_e32pk2w,2018-07-26 23:55:54,"Congratulations to that EMC salesperson. That salesperson is getting a nice commission and the CIO is going to get a few good business dinners and entertainment...I wouldn’t bet my job on a successful VxRail deployment and migration. CIO better get an acceptance testing phase written into the contract and then test the performance before signing off and accepting the product.Benchmark the hell out of storage performance, especially if planning to use it for anything write intensive (SQL.) VxRail VSAN performance on the expensive SSDs EMC sells has been less than stellar in my experience."
1.json,t3_92301q,t1_e32s0lb,2018-07-27 00:29:17,Yes your posts on the subject have contributed to my thought process.  It's already a done deal.  They didn't want to buy a SAN/NAS - they already have four servers (with four nodes each) and have two more on order - they already have about half their VMs running on it and the manager claims they are not having performance issues.  I'm concerned because they have a lot of SQL - not sure how taxed the SQL is yet - It's hard to play consultant when you are being interviewed.
1.json,t3_92301q,t1_e32s3a3,2018-07-27 00:30:18,Not really a friendship thing - it's basically a new job.  The former employee died of natural causes and they have an opening.  It would be taking over an existing infrastructure that's already past the POC and testing phase.
1.json,t3_92301q,t1_e32yvnx,2018-07-27 02:01:38,VxRail is nice and easy to support. It has few caveats to worry about as its interface is very simple to manage and underneath its just regular vCenter and ESXi hosts.Im currently implementing a 1000 user VDI solution on top of VxRail hardware and quite honestly its one of the smoothest installs yet.Don't be afraid of it if the EMC Sales person bundled the right hardware for the org.
1.json,t3_92301q,t1_e339itt,2018-07-27 04:28:23,"Guaranteed if you walk away from this you will walk into another job that has 3-4 year old pieces of shit hardware that can’t even support the latest version on VMware. HCI is the way forward and Vxrail will be just fine. For everyone that says it’s shit, there are hundreds that don’t moan because they have no reason to moan."
1.json,t3_92301q,t1_e33f8g0,2018-07-27 05:51:59,"My current job is jack-of-all-trades.  I’m technically senior Windows/VMware admin, but I spend more time supporting voip phones, network issues, desktop issues the desktop guys can’t figure out then I do supporting my server infrastructure."
1.json,t3_92301q,t1_e33murw,2018-07-27 07:55:07,"Almost all of the off-the-shelf HCI products (VxRail included) have poor performance because vendors apparently can't seem to move beyond 10G Ethernet. If you want decent performance, it's just better to custom spec the servers and use whatever software you need. VxRail is just VMware VSAN with some added doohickeys that you never use after setting up the node initially."
1.json,t3_92301q,t1_e33pu9h,2018-07-27 08:46:23,"VxRail has 25Gb adapter options, but they didn't find 10Gb networking properly implemented (non blocking fabric) to be a constraint."
10.json,t3_7uuxrq,How do I Update ESXi hosts on VXRails?,2018-02-03 06:01:51,I got thrown into managing our VXRails (that we just got installed last week)...with no training at all. Someone mentioned to me that we can only patch the esxi hosts from the VXRails manager only and not through VUM. Is that true?  For the life of me I can’t find where to update the esxi hosts on under the VXRails manager.
10.json,t3_7uuxrq,t1_dtnftb5,2018-02-03 07:04:09,If this is VXRail you upgrade in VXRail manager  > Config.  VXRail is a fully tested and validated solution so an upgrade upgrades the VXRail appliances and all since VSAN and other components are validated to work with all other software in the package.  You do not update any component manually unless you are running an external VCenter server but the upgrade should also do a check to make sure that is at the correct version.
10.json,t3_7uuxrq,t1_dtnfvsj,2018-02-03 07:05:24,If you dont have DRS set to automatic it will prompt you to drain the hosts as the upgrade script progresses.
10.json,t3_7uuxrq,t1_dtng1gc,2018-02-03 07:08:17,I would also suggest to get ahold of your Dell / EMC Rep they can usually get you a resources or maybe even a virtual lab on VXRail.
10.json,t3_7uuxrq,t1_dtnoptp,2018-02-03 10:01:12,"VxRail manager provides lifecycle of at a minimum esx/VSAN and the firmware of the box. If it was installed with a vCenter on the VxRail then it also controls the lifecycle of vCenter, if the install was done against an existing external vCenter then VxRail Manager won't touch that.I'm on mobile but if you Google VxRail TechBook you will find one of the comprehensive VxRail guides. If you DM me I will send you the administration guide.Make sure you don't delete the VxRail VMs. Don't change the name of things created by the automation (like the vDS). There is a procedure to change things created by the automation, if you don't follow it you will have problems expanding the cluster later.If you don't know who you Dell EMC SE is DM me and I can try and find out."
10.json,t3_7uuxrq,t1_dtnppme,2018-02-03 10:22:24,Here is a VxRail interactive demo. It has a click through of the upgrade as an option if you want to learn your way around XvRail manager.http://interactivedemos.democenter.dell.com/vxrail45/story_html5.html
10.json,t3_7uuxrq,t1_dtntbcp,2018-02-03 11:40:47,"I see that under config. But this makes no sense, I have to find the patches that it needs in order to update the esxi hosts vs in VUM where it will search for updates. How am I suppose to know if a new update is out?"
10.json,t3_7uuxrq,t1_dtntf53,2018-02-03 11:43:11,Vcenter is installed on the VXRails. Messaged you.
10.json,t3_7uuxrq,t1_dtnzlj5,2018-02-03 14:32:30,"If it’s not too late sell it on EBay and replace it with ANY other solution. Hyper-converged systems still aren’t ready for prime time. Dell throws out updates every other week. We have a 32 node vxRAil environment and it takes up to 18 hrs you push an update. Dell has been out to our site many different times to try and fix the problem. The standard answer, wait for the next code release.  We are too far invested to go back now. Sorry about your luck.  Half my day is spent wading through the hundreds of mostly false positive alerts the pos system constantly spits out. Finding a legit alert has become a game of sorts with the other sysadmin’s.  Node x is down, guess what? No it’s not. Failed disk on node y, guess again? The disk is fine.  Memory errors seem to be 50/50. We’ve had more than a few memory DIMMs go bad.  Our system is less than 6 months old.  For the first 3 months, even though we had redundant power supplies with each side on a separate circuit, only 1 power supply was actually supplying power.  Yup, Dell put out a patch for that.Good Luck."
10.json,t3_7uuxrq,t1_dtogoyt,2018-02-04 00:14:42,How true is this?
10.json,t3_7uuxrq,t1_dtpjlnh,2018-02-04 13:33:16,"I've seen HCI used for natural gas pressure monitoring, medical EMR,  and mission-critical places where if stuff has issues people can die for what it's worth.I can't speak for Dell but VMware doesn't have a patch cadence remotely near this.I can't speak for Dell, but I know vSAN alright. ESXi 6.5U2 (vSAN 6.6's newest patch) has resync fairness, as well as 6.6, in general, has write throughput and re-sync improvements that were pretty significant. Other things like which MM policy you use, raising the force full rebuild timeout from the default one hour (Especially with large hybrid nodes), and not running the system at 95% utilization tend to help immensely with rebuild times. If you are on vSAN 6.6 PM me your vCenter UUID and a time where you tried to do an update in the last week or two and I'll see if I can take a quick look.Older vSAN would take longer on the log replay from a host reboot, but if you're seeing that take more than 15 minutes on the current vSAN release open a ticket (The last phone home data showed this to be the case these days for current releases).Can you PM me the specifics and any SRs you have with VMware on this. Especially if it's a false positive with vSAN health UI. When it first came out in the 6.0 branch we had some false positives, but that should have been mostly cleaned up at this point. If you are still having any I'll get that straight to the dev team. The newer releases have done a good job with nested cases here.Are you using LogInsight? If you want to know What happened when it's like a damn DVR. I feel like I'm a precog from Minority report when I use it. PM me more info on this. If you are on a REALLY old release DDHv1 (Dying Device Handling) was a bit trigger happy in calling drive failures (We are on something like our 4th generation of that logic and it's actually quite good now). LogInsight has detailed stats on why stuff would be flagged if you filter for Dying Device to get an idea of what triggers may be causing it.Active/passive redundant power units historically have been more common than you think. Keeping PDU draw balanced is a bit more work than you would think, and Active/Passive also makes failover planning easier (12AMP's draw on RAIL A, and Zero on B, means If A drops I don't have to worry about B being able to handle the load, vs 50/50 where people run both to 75%). Now if it didn't fail over that's not great, but device firmware bugs happen. What matters is vendors go fix them...Full disclosure I work for the VMware vSAN team. I used to be a customer. If you PM me some more info, a UUID, any ticket SR's with VMware you have had opened I can take a look into this. I also know some of the Dell-EMC folks and have a chat with them."
100.json,t3_aa9sw1,How to Pass E22-285 Exam with New E22-285 Dumps,2018-12-28 20:10:36,"The Context of EMC E22-285 Exam Dumps: Are you enrolled in the EMC E22-285 exam? One of the most searched inquiry nowadays on the web is ""where to obtain EMC E22-285 exam dumps?"" and you keep looking for the VxRail Appliance 4.x Deployment and Implementation E22-285 braindumps online. You are looking for numerous VxRail Appliance 4.x Deployment and Implementation training material websites and companies which are providing you the E22-285 exam preparation product. EMC E22-285 test questions require tough and also difficult practice to survive. So you must try to find an exam code practice test that makes sure the full course testimonial. We are using you the E22-285 test questions which is most required by the EMC VxRail Appliance experts around the globe. We offer you the very best and excellent EMC E22-285 exam dumps that will cover all the aspect of the VxRail Appliance 4.x Deployment and Implementation exam. Produmps E22-285 Exam Dumps: We at Produmps.com provides  Our ultimate study guide for VxRail Appliance 4.x Deployment and Implementation exam: We provides superior EMC E22-285 dumps pdf that will certainly help you in passing your E22-285 exam in the initial count. Produmps have actually made E22-285 practice test to judge the performance of the prospect in review exam this will certainly help you in self evaluation. Our 24/7 customer support is constantly there for your assistance in E22-285 questions answers. We have the most effective E22-285 dumps according to the real exam that you will locate understandable as well as almost feasible. For more details, please visit "
101.json,t3_88ivbo,Has anyone here done the hyperconverged infrastructure thing and not regretted it?,2018-03-31 22:12:42,"I have been with at least 3 companies that did and each of them regretted it. tl;dr the performance wasnt there. it simply was not as fast as Host/SAN combo. if it worked well for you, what did you use and what were the workloads."
101.json,t3_88ivbo,t1_dwkwhht,2018-03-31 22:25:11,"I've never seen any company build anything on their own that's better than what AWS offers right now. Building your own anything - Just don't. It will cost more, perform worse, and have more downtime."
101.json,t3_88ivbo,t1_dwkwlhk,2018-03-31 22:27:36,Ok I'm going to pretend I didn't just read that...
101.json,t3_88ivbo,t1_dwkwps1,2018-03-31 22:30:13,"That's debatable. If done properly, this isn't an issue."
101.json,t3_88ivbo,t1_dwkwu3s,2018-03-31 22:32:48,My company is going down the Nutanix route. Curious to see what way this conversation goes.
101.json,t3_88ivbo,t1_dwkx142,2018-03-31 22:37:04,"""Friends don't let friends build datacenters"".A lot of people simply don't get the advantages of going cloud and have the ""get of my lawn"" type attitude, so they refuse to learn and diversify. Their loss, i guess."
101.json,t3_88ivbo,t1_dwkx6g0,2018-03-31 22:40:14,"We're on Simplivity (we did it prior to the HPE acquisition) with a mix of Cisco and Lenovo nodes.For the most part it makes our DR/Backups much easier. We replicate critical  VM's to our DR site, and the backups are very quick.We had a period of time where we had NIC issues on our Cisco nodes which caused some instabity. But we've been on good firmware for almost a year now, and everything is good."
101.json,t3_88ivbo,t1_dwkxd0u,2018-03-31 22:44:07,Maybe you've never seen a company do it right.
101.json,t3_88ivbo,t1_dwkxfqs,2018-03-31 22:45:41,"The company I work for investigated this with three options, Vmware vSAN and ready nodes (not really considered at all by leadership) Nutanix, and VxRail.For our application, Nutanix backed out and said they didn't think they could provide a product that would make us happy.I couldn't be more proud of a company willing to admit such a thing and will definitely go back to consider them in the future as a provider for a rolled HC environment in situations where it makes more sense.EMC VxRail, however, are up to any challenge where they think they might scrape out a dollar.They managed to convince our leadership that if the nodes for VxRail were not all in the exact same rack together, sharing the same rack switch (or switch pair) that not only is that not best practice, but they wouldn't support a proof of concept test of the hardware for our solution.I pushed them on the best practice document that outlines all gear be in the same rack together, but they couldn't produce one. Just a bunch of handwaving of ""experience"". Later on, at the install date, the guy making the claim that it was experience that necessitated the single rack config, admitted he had not received his ""certification"" from EMC to do installations, and would not/could not be present for the activity.I was livid.But my leadership quickly waved the requirement that vendors show best practice (and I suppose, our internal requirement that we follow one) and it was all racked in the same rack anyway. I suppose Cover-Your-Ass and Protect-Your-Career means more than Do-Whats-Best these days.I shouldn't blame EMC though. They're just complete fucking idiots, barely one rung above car salesman, that only roughly know the product they pitch and will do anything to make the sale -- even if in four months the customer is unhappy, which is typical.If you want hyper-converged, it strongly depends upon the type of your workload. If your environment is mixed general purpose, with relatively light database utilization, then deploying Vmware vSAN Ready Nodes is the right way to go in my opinon. You can get ready nodes from half a dozen vendors too.I wouldn't use hyper converged for OLTP workloads unless you built a specific cluster to do the job, and bought specific disk hardware, possibly NVMe stuff. The risk of saturating a single node with one busy database is there, but I will admit not having enough direct experience to say exactly what to purchase, only that the storage backplane is far more critical to get right with OLTP workloads and far less flexible on HC environments than it is if you use your storage as a separate tier such as with an external SAN."
101.json,t3_88ivbo,t1_dwkxinc,2018-03-31 22:47:23,"We're at (probably) the same path, meaning choosing between traditional vs hyper vs cloud for our upgrade.We're SMB though, looking forward to this thread ;)"
101.json,t3_88ivbo,t1_dwkxxf5,2018-03-31 22:55:43,"We’re currently entertaining the idea of moving our data center to a colo and migrating our prod stuff to vxrail as a part of it. We’re not a super high-performance shop or anything—we’re healthcare, so we need disk and disk and disk and disk to have the capacity to store all these patient records for 7 years (aka basically forever), and we only have one sort of high-performance database, while everything else is just chilling with one or two procs and like 2-4gb of ram. So. Like, EMC sales guys are saying vxrail should be good for us (of course), but (of course) I’m like, I dunno, man. Once all was said and done in your environment, did it suck? Or was the bad experience more during the implementation side and now you’re good?"
101.json,t3_88ivbo,t1_dwkye71,2018-03-31 23:05:11,Be curious to know the workloads and I/O profile. HCI isn’t just a niche case anymore. Nutanix has come a long way with their I/O stack and it’s the odd corner cases that don’t fit anymore.
101.json,t3_88ivbo,t1_dwkyw2l,2018-03-31 23:15:19,"Nutanix convert about 18mo ago.Started by replacing an aging 5 server/NetApp/VMware cluster with 1 block of Nutanix.  (Going from 9U to 2U and having more compute and storage is pretty amazing).Once we were satisfied with that, we began replacing all of our remote office stacks with small 3 node clusters.  And now we're planning our primary data center refresh which will be 3 blocks replacing a full cabinet.Absolute 0 complaints.  Management is better.  Reporting is better.  Performance is perfectly find for our needs.  Upgrades/updates/resiliency is fantastic.One note, we're not running VERY intensive workloads on prem.  Email and ERP are hosted as well as data wharehouse databases (AWS).  As such, our workloads internally are relatively light weight outside of VDI."
101.json,t3_88ivbo,t1_dwkz1fk,2018-03-31 23:18:16,"Anyone doing S2D ready modes? I wasn’t happy with the cost of VXRail, even with heavy discounts. Hyper-v day enter licensing is tempting."
101.json,t3_88ivbo,t1_dwkz6lq,2018-03-31 23:21:10,"At my previous place of employment we replaced our 10Gb iSCSI SAN with a couple Nutanix clusters. The performance was insane compared to our iSCI SAN. Migrated our entire production vSphere environment over eith relative ease and loved it. Setting up the Nutanix clusters and DR was a breeze. I left that place and now work in a vSphere environment leveraging a NetApp AFF. While I still love Nutanix HCI for it's simplicity, decent performance and ease of administration and upgrades, I'm absolutely loving the performance I'm getting out of this NetApp. That being said, if I had the choice I'd probably have both in my environment, because why not?"
101.json,t3_88ivbo,t1_dwkzhjr,2018-03-31 23:27:15,"What the hell does hyperconverged mean? If we stop getting people to sell us stuff, maybe we'll get more done."
101.json,t3_88ivbo,t1_dwkzpi0,2018-03-31 23:31:31,Typically it means storage and compute on the same node and split evenly over a large group.
101.json,t3_88ivbo,t1_dwkzwfq,2018-03-31 23:35:17,"Just means all your shit lives in the same boxes. The components are all still there, just squashed in and centrally managed (usually)."
101.json,t3_88ivbo,t1_dwl09jz,2018-03-31 23:42:22,The last school I worked at used NutanixFantastic support and it wasn't ridiculously expensive. Worked great too.
101.json,t3_88ivbo,t1_dwl0c9d,2018-03-31 23:43:50,"Hosts and SAN all in one box, usually made by the same provider"
101.json,t3_88ivbo,t1_dwl0s9x,2018-03-31 23:52:11,"We use nutanix and do not regret it.   We use it for VDI , to host our SCADA application , it’s our host at our back up data center and DR.   And we have it running at 3 remote offices.edit We have been using them for 5 years."
101.json,t3_88ivbo,t1_dwl1ndl,2018-04-01 00:08:32,We have been using them for 5 years and have had little issues.
101.json,t3_88ivbo,t1_dwl1zfp,2018-04-01 00:14:52,"3 node Nutanix cluster for a medium sized insurance firm. Works well, performs well. I'm actually impressed with the nutanix software and how it works. No major faults or issues. I do believe it's much more expensive than Host/SAN so really there was no benefit as either solution would have worked."
101.json,t3_88ivbo,t1_dwl2oul,2018-04-01 00:28:19,"Yup... all of this.HCI is really good at general-purpose workloads in my experience. It's really a game-changer where you have small environments and upgrades where you want something pretty simple, but for big databases and OLTP it's a shit-show. At least at the moment. The performance just doesn't stack up as well. Basically my perspective is; if you feel that your environment would be perfectly suited to iSCSI for its storage behind the hosts to the SAN then you're a good fit for HCI, but if you honestly feel Fiber-Channel is the way to go you should probably stick to it and a SAN/Host infrastructure.Bear in mind the backend of all of these HCI platforms is still Ethernet... and it's still over TCP. This adds a chunk of overhead that you don't have with FC. Yeah, I know FC looks like it's lagging at 16Gb and 32Gb/s speeds, the truth is that I can probably match or beat the same traffic that you're pushing over a 100Gb/s Ethernet. And to add insult to injury for HCI I don't know of anyone right now who supports anything over 10Gb/s for the backend inter-node communication, and that becomes a pretty chatty backend when you've got a lot of nodes. 100Gb/s cards work, but in lab testing I've never seen HCI nodes talk at anything above about 8Gb/s or so even during heavy loads. I suspect that's a kernel thing or possibly an interrupt thing with the hardware, so that might improve in time... but I also suspect there's some code in there doing some stuff.The ""amazingly low latency"" of HCI never really materializes... hell I've pitted a VxRail stack up against a 4-host stack connected to an FC SC5020 and it just smoked the VxRail. Yeah it was only 4 nodes, but my experience says more nodes increases latency, not decreases it. But having said all that, when client-testing against these two stacks it was Impossible to tell the difference... this was with the same VM's on both stacks and included some relatively small SQL (two MySQL and one Microsoft SQL database). When I dialed up to busier and heavier databases the cracks appeared in the VxRail but that was a pretty specialized use case (a genomic sequencing workload). As a result I can honestly say for general workloads such as you might find in any office environment, HCI works great... but throw special-purpose workloads at it and you're in for a bad time. But I do have a certain amount of faith that this will get better in time; the vendors are constantly improving their stacks.As for the EMC folks... good god those people drive me crazy right now. It's like they've been given this nice hammer (VxRail and VxRack SDDC... both VSAN under the hood) and now every problem is a goddamned nail. Except for one or two instances where they break out their sledgehammer (VMax). I mean, they have some great products in their portfolio; I obviously have a soft spot for SC (nee Compellent) and the VMax is good at the very top end... but X2 is a pretty solid platform if you don't need a lot of features... hell it still doesn't even have replication, but if you're using host-side replication anyway then who cares?Don't even get me started on the stupidity of dropping their ScaleIO software product... for those who missed the memo they have stopped selling it as a standalone software product and now you can only buy it as part of their VxRack Flex ""appliance"" model. You know, the one that they can't even give away because no one wants it?"
101.json,t3_88ivbo,t1_dwl334k,2018-04-01 00:35:46,We just upgraded from Cisco UCS nodes and an EMC VNX to Cisco Hyperflex - all SSD.So far the performance has been fantastic but I'm sure that has about 95 percent to do with being all flash. It does make the storage management a lot easier to manage and that part is worth it.
101.json,t3_88ivbo,t1_dwl35t8,2018-04-01 00:37:07,[deleted]
101.json,t3_88ivbo,t1_dwl3qrf,2018-04-01 00:47:35,Same here. Been running VDI and a number of non-high performance servers on a Nutanix array for about 4 years. No real complaints
101.json,t3_88ivbo,t1_dwl3vtk,2018-04-01 00:50:10,"Wow. Totally different experience with Nutanix. Maybe regional sales practices are different, but once these guys got a meeting it was all hyper-aggressive sales tactics all the time. They would've tried to sell me a robot girlfriend if I ever mentioned that I stayed home the weekend before instead of going out."
101.json,t3_88ivbo,t1_dwl4cc2,2018-04-01 00:58:38,Anyone tried Huawei gear?
101.json,t3_88ivbo,t1_dwl4gy5,2018-04-01 01:00:56,"Started using nutanix about a year or two ago and love it. Easy to use, good API, great support, and cost is really not that bad. Running AHV and it's been great. Would recommend 10/10."
101.json,t3_88ivbo,t1_dwl5hl1,2018-04-01 01:19:41,"Wow. How much storage did you have then, and how much did you replace it with?"
101.json,t3_88ivbo,t1_dwl6spk,2018-04-01 01:43:54,"Gosh don't even get me started on EMC. We made a large purchase order of workstations on my last position about a year ago. Completely wrong hardware, etc etc. Their 'fix' was sending out 'technicians' to replace the hardware with what we ordered. By 'technicians' they meant whoever they could scrounge up at the regional office. They arrived and I showed them where the tools were. One of the guys replied ""You aren't going to help us? We don't turn many screws back at the office, we're sales guys and don't know how to install this stuff."" So guess who had to hand-hold and be part of the hardware replacement process for that shit..."
101.json,t3_88ivbo,t1_dwl6uwb,2018-04-01 01:45:02,"Using nutanix at scale for mixed workloads on vSphere 6.x.  Have seen some major performance problems that have been difficult to resolve.We're not replacing the nutanix we have, but for our second region we're going with a more traditional approach of commodity x86 with SAN (Pure, local to the racks in this case)."
101.json,t3_88ivbo,t1_dwl7bhw,2018-04-01 01:53:29,Just wait till you get a 7M per month bill from AWS.  Then tell me not to do it myself.
101.json,t3_88ivbo,t1_dwl7pg3,2018-04-01 02:00:42,This is not hyperconverged
101.json,t3_88ivbo,t1_dwl7ubx,2018-04-01 02:03:12,"Similar to my experience overall - we run some very intense workloads on prem, get noticeably better performance from our traditional clusters, but we're using hyperconverged here and there for drop-and-use stuff with known general-compute needs to minimize build and support needs fairly successfully."
101.json,t3_88ivbo,t1_dwl7ulo,2018-04-01 02:03:19,"IMO (and I'm a network guy, not a server guy), I think hyperconverged is better suited for small locations, branch offices, etc.  Places where you want redundancy and need AD/DNS and maybe a closer instance of an app server or two.  Not the enterprise datacenter.  Thats still the place for consolidated storage platforms."
101.json,t3_88ivbo,t1_dwl8zic,2018-04-01 02:24:35,"I'll probably be the only Scale Computing customer on here but I migrated a small vmware 3 node and SAN environment supporting about 20 windows VMs to their hc3 stuff a year or two ago and haven't had any complaints. We saved a bunch of money, management is almost non-existent. I don't regret it for a second and recommend them to small - mid size companies all the time."
101.json,t3_88ivbo,t1_dwl980p,2018-04-01 02:29:04,Right because you can buy terabytes of flash storage at AWS for less than a Maserati
101.json,t3_88ivbo,t1_dwl9wgg,2018-04-01 02:41:56,Use hyperconverged for virtualization management services and remote sites.   Anything else is stupid
101.json,t3_88ivbo,t1_dwl9xid,2018-04-01 02:42:29,We're just now starting to migrate our workloads to our 4 node Nutanix and it's been smooth sailing. Very happy with the product and very happy to finally retire the aging hosts and SANs.
101.json,t3_88ivbo,t1_dwla2ze,2018-04-01 02:45:23,I'd say it would struggle with the noisy neighbor effect. Very high continuous I/O could starve other workloads on the same node and having a dedicated block to one app kinda defeats the purpose of HCI.
101.json,t3_88ivbo,t1_dwlahre,2018-04-01 02:53:14,"It means the way we used to build servers before SAN. Everything old is new again, but this time with fancier branding."
101.json,t3_88ivbo,t1_dwlalgl,2018-04-01 02:55:14,+1 for Scale
101.json,t3_88ivbo,t1_dwlbbqj,2018-04-01 03:09:08,"We went from traditional metal servers to a 6 node Nutanix cluster ( one for dr ). We love it. The SSD's bring incredible speeds to our applications. And having that peace of mind of data resiliency makes us happy. We did not have a SAN so I can't tell you anything when it comes to that. But really, Nutanix just sits there and does it's job. Their support has always been there for us when we needed it."
101.json,t3_88ivbo,t1_dwlbdii,2018-04-01 03:10:05,"I guess it only nearly counts as HCI, but I have a VRTX or two and like it."
101.json,t3_88ivbo,t1_dwlbfms,2018-04-01 03:11:12,"Do we live in the same dimension? By any measure at almost any scale, cloud is more expensive than owned gear."
101.json,t3_88ivbo,t1_dwlbgza,2018-04-01 03:11:54,"We are in the process of migrating from a traditional vmware 3-2-1 stack at 2 sites to 2 x 4 node S2D clusters. Performance is great with NVME cache and SSD, HDD tiering.Definately more cost effective in the hyperconverged space but we have hit a potential bug that is setting our migration back a bit.Hopefully I find out some good news this week and we get a resolution."
101.json,t3_88ivbo,t1_dwlcavd,2018-04-01 03:27:55,Simplivity currently I'm pleased with it for the most part backups are great. Restores are quick too..
101.json,t3_88ivbo,t1_dwlcfp5,2018-04-01 03:30:28,"We have some m1000e chassis. As a network guy, I hate the switches we have. What model switches are you using? Any tips or tricks? Lessons learned?"
101.json,t3_88ivbo,t1_dwldc51,2018-04-01 03:47:47,"Moved from a 5 node vsphere to a Nutanix cluster. Annoyed at first because I felt I need to tweak stuff, realized I was wrong, and had a much nicer time.The vSphere cluster required 10hrs a week of my time.  Nutanix, when I left that job at 16 nodes, took <1hr a week.  This was for CPU-heavy analytics.Trying to build a cluster at a new job on vSAN. I want my Nutanix back."
101.json,t3_88ivbo,t1_dwleesf,2018-04-01 04:08:16,"We did a POC on a all flash vxrail last year.  The sales pitch was impressive, but the install was not.  Took the better part of a month with multiple engineers vs the 15 minutes they promised.  Required quite a bit of setting changes on our network and did not perform as well as our UCS/AFA POC.  I know it's changed some now, but GPU and encryption support was lacking at the time.  I was really hoping for a single vendor solution, but with most EMC solutions, it was over priced, over hyped, and under delivered."
101.json,t3_88ivbo,t1_dwlerq1,2018-04-01 04:15:04,"Hyperconverged is just a buzzword anyway. Yes, let's put everything into one basket, that's never been a bad idea.Edit: This comment is a gross oversimplification to be taken with a grain of salt."
101.json,t3_88ivbo,t1_dwlf1rb,2018-04-01 04:20:21,"It certainly depends on your workload. I have done installs of Cisco HX, VxRail, and vSAN.All in all, they are blazing fast and insanely easy compared to any type of converged infrastructure for most of your general purpose workloads.Now if you are doing a lot of sequential data writing then yes, they will be slower. Because by design you are injecting a process that holds the writes in memory until the SCSI commands are confirmed from your minimum node resiliency set. You can design around this to improve niche scenarios, but that is not the point of HCI. HCI is meant to make storage easy."
101.json,t3_88ivbo,t1_dwlf9hz,2018-04-01 04:24:22,"A previous company I worked for switched to Nutanix for their VDI and was much happier with it, they also used it for their Splunk implantation I was not part of that but I heard it went very well. I think it really depends on your use case."
101.json,t3_88ivbo,t1_dwlfjcu,2018-04-01 04:29:29,Upvote for simplivity. Makes my life much easier.
101.json,t3_88ivbo,t1_dwlfu66,2018-04-01 04:34:55,"We used Pivot3 for a Surveillance application and it was definitely an experience to say the least. We found out that HCI, at least as far as P3 is concerned, is very picky. We had some stability issues which lead to us  actually losing a SUBSTANTIAL amount of archived footage (petabyte range) and it took forever to get it lined out.It’s a good idea in theory, but read/write intensive applications seem to be the weakness. We got it stable, we got everything working but it took some time."
101.json,t3_88ivbo,t1_dwlg98e,2018-04-01 04:42:28,"Yeah, we deployed a VSA cluster back in the 5.1 days. We completely met there HCL for VSA, but we couldn't update to VSAN. Therefore we couldn't upgrade to 6.0. We now have a San attached to each cluster and are using the hosts as just hosts."
101.json,t3_88ivbo,t1_dwlglvb,2018-04-01 04:48:50,"We're migrating to hyperconverged.  VxRack by Dell.We did extensive testing before deciding on it.  Put it through it's paces.  saturating the I/O bus, cpu cycles, vmotioning, storage vmotioning while under extreme load.We're happy with it."
101.json,t3_88ivbo,t1_dwlgy1w,2018-04-01 04:54:49,"The mistake here was discounting vSAN Ready Nodes in favor of VxRail...which is just pre-packaged vSAN with a bunch of hard limitations to ensure 'support' from EMC.  I'd also only work with a VAR for stuff like that, EMC direct is going to almost always be trash.  Not that any other vendor is different, I worked for a top regional HP VAR and we did most of the 3PAR installs after they were bought by HP because HP would just sub-contract to us.  They sucked and they knew it, so they let us take a cut to do it right."
101.json,t3_88ivbo,t1_dwlha1p,2018-04-01 05:00:50,"As a VMware guy that works for an EMC VAR, I'll be as honest as I can be here.First off, I don't think vxRail is a one-size fits all product.  This seems to be the best fit for large robo offices and VDI.Secondly, once it is installed, vxRail is pretty great as a VMware/VSAN product.  The problem is getting it installed and expanding it.  I won't get into the myriad of issues that we've had installing these (because it could out me), but suffice to say it's a PITA.  Support from EMC has been generally terrible on these issues (of course, support from EMC has been generally terrible for awhile now), so there's that as well."
101.json,t3_88ivbo,t1_dwlhrk2,2018-04-01 05:09:48,Same experience in a Higher-Ed with Nutanix and ESXI.  Nutanix and our vendor WEI have been outstanding and we are definitely getting everything we hoped.
101.json,t3_88ivbo,t1_dwli9y5,2018-04-01 05:19:26,"Hadn't heard of Nutanix before today but they seem well regarded in this thread.I was talking to Dell about some of the vxRail stuff, but it was pretty costly and the feedback in here seems kinda meh, so now I'm curious about Nutanix's offerings - mind if I ask what your setup is more specifically?"
101.json,t3_88ivbo,t1_dwlihmt,2018-04-01 05:23:32,We have a 2 host setup using Starwind on Dell R730s. Runs about 50 vms. No complaints at all.  Runs fast and Starwind support is great.
101.json,t3_88ivbo,t1_dwlisdr,2018-04-01 05:29:17,Last place I worked had an EMC san. Reportedly it cost almost a million pounds and there was only one specialist in the country that was certified to service it. It was very unreliable and constantly down (I mean like all of our production web infrastructure and internal web apps offline for extended periods of time). We decided the best option was to buy another SAN rather than fix this one.Management bought another EMC SAN.smh.
101.json,t3_88ivbo,t1_dwlj8lc,2018-04-01 05:37:56,"Well, we went with Atlantis as our HCI and it was a shit show from the start. It turns out that initial tech set up the machines to use the spinning disks as their primary storage, rather than the RAM virtualized disks that are supposed to service the infrastructure. So it ran like ass for the first three months we had it.Next, there was four months of random errors, system failures and other bullshit. It was apparently a problem with the SuperMicro platform it was based off that they blamed on SuperMicro. Personally, I didn't buy it and thought they should have done more extensive testing on their hardware before they shipped it to us.Finally, when they did send us new hardware after we threatened to pull the plug on the project, they sent us used Cisco UCS systems to replace the Supermicro. Fine, I guess. New would have been nice, but whatever. Also, the never sent the damn rails for the things so I had to buy them myself.The performance is there, since we're running a Citrix XenDesktop environment on top of a XenServer installation, and it works well enough for our needs, but it took 9 months to get it there and god help you if you try to reboot any of the hosts without prayers to the machine gods.All in all, I'd rather redo the system with SANS and Hosts. So to answer your question, no. I regret the shit out of it."
101.json,t3_88ivbo,t1_dwljers,2018-04-01 05:41:08,"We use Nutanix for our VDI implementation, and for the most part we've been really happy with it.  There are some gotchas to watch out for, but for the performance has been great, and for the most part it's been easy to administer."
101.json,t3_88ivbo,t1_dwlki0z,2018-04-01 06:01:56,I'm having to do a couple of VXLAN's now so the vxrails can replicate between three sites.
101.json,t3_88ivbo,t1_dwlkvjp,2018-04-01 06:09:12,I'm curious if anyone has any experience with scaleio in here.
101.json,t3_88ivbo,t1_dwll6x6,2018-04-01 06:15:18,Scale surpassed every one of our expectations thus far.
101.json,t3_88ivbo,t1_dwll95s,2018-04-01 06:16:29,"I wrote part of VxRail, does that count?"
101.json,t3_88ivbo,t1_dwlle7b,2018-04-01 06:19:06,Check out Datrium vms run on flash on the host. Nothing better
101.json,t3_88ivbo,t1_dwlmtsk,2018-04-01 06:45:52,Came here to see if anyone had me tioned Starwind. 2 years for us so far and its great for small-medium businesses.Support has been great too.
101.json,t3_88ivbo,t1_dwlmybd,2018-04-01 06:48:10,[deleted]
101.json,t3_88ivbo,t1_dwloh1f,2018-04-01 07:17:23,"We sold a client HPE HC380's.  6 weeks later HPE bought Simplivity.  Because we had been working on that sale for like 9 months it didn't qualify for Simplivity upgrade thing.It's a mess.  I hate it.  We have vowed to sell it as the last option.We have another one in our lab.  There is too much to go wrong all in 1-2 boxes.  What can fail has.  I am not impressed.I am not big on the HPE sale of IT in a box.  Which for a relativelytech minded person they can spin up VMs and have a backups, but that also puts our advanced vmware guys and zerto guys out of jobs so when we sell it to an inhouse IT team that's cool but as an outside contractor not so awesome.When the clients 6 node system in 1 building failed, the option to migrate the other vms over to the replicated node was not available.  HPE wasn't even sure what the issue was for about 9 hours.  Their solution was to reinstall the hypverconverge system and restore from backups."
101.json,t3_88ivbo,t1_dwlpljx,2018-04-01 07:39:20,"Is your name Mitch?? We literally went through this exact dance with Dell on a VxRail system at a capital cost well outside of what we could have just done with vSAN.  The split-cluster setup we wanted to keep for redundancy just could not be  done.  All because  my direct supervisor needed full CYA since he allowed our senior admin to make far too many changes to the environment at once (resulting in a failure-prone iSCSI SAN setup), and Dell promised an ""easy button"" to support. I think HC has potential in the right scenario, and to be fair, outside of a weird Oracle DB issue that took ""easy button"" support 3 weeks to ""solve"", the system has been rock solid."
101.json,t3_88ivbo,t1_dwlplzu,2018-04-01 07:39:34,"Been using Nutanix/Acropolis for about 2 years now.  Zero issues with support, only one minor incident (over provisioned a VM, completely my fault), and they handled that with ease."
101.json,t3_88ivbo,t1_dwlpo30,2018-04-01 07:40:41,Storage in the same boxes as CPU.But make sure there is redundancy in the storage. That you can loose one node of the set and all vm's keep running or can be migrated/restarted. If a vm is locked to a host that's crap.
101.json,t3_88ivbo,t1_dwlpvry,2018-04-01 07:44:49,"Yes.  Guess I am the only one using Scale here?  The support from Scale is great and the hardware has worked well.  We did have one hiccup where a drive rebuild dramatically slowed down the systems, but support corrected it."
101.json,t3_88ivbo,t1_dwlpzb1,2018-04-01 07:46:46,"Same. Just deployed one. It's not ""real"" HCI but I've been very impressed so far, particularly at the price point for what we bought."
101.json,t3_88ivbo,t1_dwlpzp1,2018-04-01 07:47:00,"We have had a lot of success with a Nutanix setup for one of our lines of business. But when we had them look at another line they were, as with you, very up-front that they didn't think they could make it work. And after investigating it fully, they confirmed that.Like you, I'm real impressed by any company that will say no to a deal. Especially one like Nutanix that is still in the early stages of growth."
101.json,t3_88ivbo,t1_dwlq3eu,2018-04-01 07:49:02,"We deployed Nutanix to handle a web application. Very intensive on the storage side (both IOPS and bits on disk). Standard three their architecture and all that.It works beyond our wildest expectations.On top of the performance gains, we also reduced the physical footprint by two thirds. And we're now running about 5 times the number of VMs."
101.json,t3_88ivbo,t1_dwlq99e,2018-04-01 07:52:16,"HyperConverged is taking the Compute/Storage/Network separate layers and consolidating as much as possible in a single node. VSAN and Nutanix AHV HCI architectures are very similar: Each node has compute and storage for VMs hosted on box. Performance is great because rather than jumping through expensive switching to an expensive SAN and back for your data, you go to a local SSD which has hot data cached. Data locality is the concept both use to keep the data for your VM local to the host. Extra copies of the data reside in other nodes, but the purpose of the other nodes are simply to create fail-over copies of your data in the event of a host failure."
101.json,t3_88ivbo,t1_dwlqcew,2018-04-01 07:53:56,"one basket? the architecture is fault tolerant by design. Compute, storage, network, disk failure and you're still up in HCI done right. It sounds like you had a bad architect or a bad engineer pitch HCI."
101.json,t3_88ivbo,t1_dwlqr6t,2018-04-01 08:01:55,"I have implemented an OceanStor SAN for a major service center that provides services for school districts. It was a 600 TB, dual-rack, quad controller, hybrid setup with SSDs, SAS, and NL SAS setup for tiered storage. After 5 years, it has been one hell of a work horse. No major issues and still going strong. The only problem is that the dashboard requires you to understand storage (building volume groups, raid groups, provisioning disks, host groups, LUNs, etc etc)"
101.json,t3_88ivbo,t1_dwlqs9y,2018-04-01 08:02:31,How's support been for you recently?  It was pretty terrible for my last company after the HP acquisition.
101.json,t3_88ivbo,t1_dwlr40m,2018-04-01 08:08:39,Why does it seem everyone picks off the shelf ways of doing things? Why not a RH ceph cluster instead of a single point of failure SAN?
101.json,t3_88ivbo,t1_dwlr7qv,2018-04-01 08:10:35,[deleted]
101.json,t3_88ivbo,t1_dwlrr1j,2018-04-01 08:20:43,"We moved ~500 VMs from to Nutanix from Cisco UCS/ VMWare/ EMC VNX using FC about 6 months ago.  The work load included basically everything including web servers, application servers, exchange with an obscene amount of storage, and a good many SQL servers.We went from two racks of VNX and basically 42 U worth of UCS to about 40 U of Dell XC Nutanix nodes. We’ve had one host be problematic with some yet to be identified hardware issues but Dell is replacing the entire node. Otherwise it’s been a great experience thus far.I’ll say we usually bypass Dell tech support and call Nutanix directly for software issues or questions because EMC sucks the sweat off a dead mans balls. We are able to do this because we have a few nodes of Nutanix branded gear in DR which we played with prior to moving production over.One major complaint I have is the pricing all the sudden got really steep once we needed to add more nodes as part of an upcoming project.  We raised hell and made some threats and all the sudden the pricing was a little more inline with what we paid on our initial investment. I’ve been told this was because EMC/ Dell is no longer incentivizing their sales force to sale Nutanix so the discounting isn’t as aggressive as it once was."
101.json,t3_88ivbo,t1_dwlrxbz,2018-04-01 08:24:05,The VRTX is a very under underrated piece of hardware for the hyperconverged deployment model.
101.json,t3_88ivbo,t1_dwls9nh,2018-04-01 08:30:36,"I also came here to see if Starwind is mentioned, I’m going on 1 year on a pair of R530s in my lab and a little under 1 year on a pair of DL385g8s in production and really like their product.The performance is great and the free product has all the features you would need to build out a 2 node deployment."
101.json,t3_88ivbo,t1_dwlsazd,2018-04-01 08:31:19,"As a DellEMC (or Dell Technologies) and Nutanix partner, I agree with you 100% about these two products/orgs.  From my POV, block storage goes to Pure, HCI goes to Nutanix, change my mind!"
101.json,t3_88ivbo,t1_dwlss5r,2018-04-01 08:40:44,Can't say I'm seeing it and we are in the middle of a big deployment of ceph and openstack. Ceph we are seeing over 30gb/s when needed and openstack is handling more than enough VMS.Scale wise ours is a fairly  big deployment over 3pb storage and over 3000 cores.Compared to our San and hyperv deployments for other work flows it blows them out of the water. We are lucky to see over 1gbps on a full ssd front compelant dell sanOur workflows are Jenkins build servers. Scientific data analysis batch jobs and everything in between with some fairly big data bases on them too
101.json,t3_88ivbo,t1_dwlt56o,2018-04-01 08:47:49,"We just installed HP Simplivity in a large datacenter environment with one host off-site in a city about 100 miles away.  So far, it's pretty remarkable.  Had many reservations going in, not the least of which it being an HP product.  But the backup ""deduplication"" rate is absolutely incredible.  No regrets thus far."
101.json,t3_88ivbo,t1_dwlu0t4,2018-04-01 09:05:27,Where is oVirt and Glusterfs in this conversation?  Any love for their solution?
101.json,t3_88ivbo,t1_dwlx2e4,2018-04-01 10:05:47,I’m interested in Starwind. What’s the cost like for your setup?
101.json,t3_88ivbo,t1_dwlx535,2018-04-01 10:07:20,Check out Datrium. I'm a Nutanix focus VAR but in some cases it's not a good fit and open converged is more appropriate.edit - you can also attach Datrium to Nutanix and we've seen perf improvements.
101.json,t3_88ivbo,t1_dwlx6ph,2018-04-01 10:08:15,If you're AHV check out Plexxi and HYCU!
101.json,t3_88ivbo,t1_dwlxvfl,2018-04-01 10:22:10,Check out Plexxi and HYCU if you haven't already.
101.json,t3_88ivbo,t1_dwly1l4,2018-04-01 10:25:41,"True HCI is compute, storage, and virtualization recognized as commodity and managed together, allowing flexibility across hypervisor and hardware."
101.json,t3_88ivbo,t1_dwlycrx,2018-04-01 10:32:05,Who'd you buy Nutanix from? We can usually beat 3 tier pricing.
101.json,t3_88ivbo,t1_dwlyp49,2018-04-01 10:39:01,"Lots of good comments here.  I’ll throw out my two cents.We went with Nutanix several years ago, coming up our 5 year anniversary I believe.  We’ve purchased somewhere around 18-20 nodes and as of last year have went all flash.It’s absolutely been worth it for us.  I would never go back to hosts + SAN."
101.json,t3_88ivbo,t1_dwm0mwa,2018-04-01 11:18:40,"Just piggybacking on the Nutanix train...Small/medium manufacturer here that just started investing in any real internal services.  We chose Nutanix after a pretty lengthy evaluation phase.  Nutanix just checked all the boxes for me; I'm a solo sysadmin so minimizing the management overhead was really important to me.  We've been up and running for 1 year with Nutanix now and I really couldn't ask for anything more.  We're running basic stuff, nothing too intense, but the IO performance is more than we need for ERP and internal services.I did run a cluster-wide IO performance test when I first got setup and I was impressed by the numbers but I don't remember them off-hand.  However, I have no idea how that translates to actual intense workloads.  So, your mileage may vary, but I'd recommend it in a heartbeat to anyone in a small/medium business."
101.json,t3_88ivbo,t1_dwm3mrx,2018-04-01 12:25:10,I guess Hyper-V adoption still pretty low as not much comment for Starwind or even using own Microsoft Server Storage option.
101.json,t3_88ivbo,t1_dwm3nyb,2018-04-01 12:25:56,[removed]
101.json,t3_88ivbo,t1_dwm3o1y,2018-04-01 12:26:00,"It also depends what you're running.  Need to make an entire replica of PROD, but it doesn't have to serve the same load?  Great.  New software stack getting built up  that doesn't take much loading?  Perfect place to put it all.I see HC infrastructure as a very convenient and fairly resilient way of packing a whole bunch of stupid stuff into one place, in a way that helps keep it easy to manage.  If something requires an appreciable amount of resources (cpu, mem, disk), perhaps HC might not be the best place for that... but in my experience, the vast majority of ""stuff"" really doesn't use much at all."
101.json,t3_88ivbo,t1_dwm701p,2018-04-01 13:54:33,Anyone have experience with the Microsoft equivalents? I refer to Hyper-V combined with Storage Spaces Direct.
101.json,t3_88ivbo,t1_dwm7exg,2018-04-01 14:07:11,"It's not just a SAN, like others have said Nutanix and other companies put everything you need in a 2U chassis. I say chassis because sometimes there are really four compute nodes in a 2U chassis, each with 2x CPUs (Xeon E5-2620), a shitload of ram on the board, and five or six disks in front for storage. There's replication across the nodes, so if VMs go down there's replication not only with the data in RAID, but also being able to balance loads between nodes. You basically BYO hypervisor, like VMware vSphere, or Nutanix Acropolis, and 10G networking and you're set to go."
101.json,t3_88ivbo,t1_dwm7xhk,2018-04-01 14:23:11,We have 200 vms of mixed purpose workloads sitting on a few nutanix nodes that we bought late last year. We don’t run anything especially special and certainly no chatty databases. Mostly Citrix and our document management system.It performs well enough and is simple to manage. I’m a single man sysadmin looking after everything after some redundancies and people leaving so I’m kinda glad it’s HCI as it’s simpler. Having said that the guys that were passionate about HCI who set it up have left and I’d much rather be more cloud focussed than HCI (in some form of hybrid).
101.json,t3_88ivbo,t1_dwm7ymm,2018-04-01 14:24:13,"Every reply below that mentions similar issues also mentions install issues with the vendor, underspec solutions, etc. That’s not a fault of the hyperconvereged ‘concept’ as such.There’s a lot of people peddling crap and lots of credulous people out there just itching to chug down the kool-aid and of course that’s all going to end badly. If a poor sales dude tries to tell you that you can replace a busy, complex setup with a couple of boxes full of crap and somehow the magic will happen because it’s hyperconverged crap then yeah, it won’t end well.We switched from a traditional SAN based setup to a Hyperconverged setup and been happy with it, but we made the effort to understand our workload and plan accordingly."
101.json,t3_88ivbo,t1_dwm8cx5,2018-04-01 14:37:29,"Surprised to hear you’re doing a new deployment.Isn’t equallogic EOL? Based on what Dell’s sales people have told me, I’m surprised anyone would be buying it new right now. Even though they might technically have to provide support for 7 years from sale date, clearly they don’t want to support equallogic and haven’t for a while."
101.json,t3_88ivbo,t1_dwm9c4f,2018-04-01 15:11:50,"I'd agree with this.This thread is mostly comprised of the following points of view;Essentially what people need to realise is that SAN's and Networks aren't magic - but they aren't easy either. Once you understand how storage is built, how networks impact things - then work with your HCI solution, you can deploy whatever you want and achieve the result you want, probably inside of budget.Point blank I think 20k for a shelf of 10k disk with redundant controllers/psu (which is a cheap san) is far too expensive for what it is. Particularly when I need to buy 10g switching to really make that storage work properly. In small 2-node setups I can simply direct connect the hosts on 10/25/40G, then serve everything else on 1G teams. So I like HCI at that end of the scale.At the bigger end of town, I think like everything, you just really need to understand your workload above all else - then ensure your solution fits your need and growth expectations."
101.json,t3_88ivbo,t1_dwm9zwn,2018-04-01 15:35:53,"We are running non-HPE nodes on SimpliVity as well. Products updates will not be delivered for non+HPE hardware after 2019, and all support will end in 2021.I'm happy with the solution, pissed at how HPE has handled the purchase. We're currently working towards a trade-in to migrate to proper HPE nodes."
101.json,t3_88ivbo,t1_dwm9zxz,2018-04-01 15:35:56,That's not how you use tl;dr
101.json,t3_88ivbo,t1_dwmd9cb,2018-04-01 17:48:31,We’ve deployed these about about 80 of our smaller sites in the last 8 months. Incredible little boxes that give us so much flexibility at a site. Definitely an underrated machine.
101.json,t3_88ivbo,t1_dwme3ht,2018-04-01 18:24:28,"I’ve spoke to a few people who have deployed vxRail and the overall vibe is ‘great once it’s set up.... but good luck with that’.It’s being pushed heavily by the “we add value” resellers hard, rather than the box shifters. A cynic might wonder if vxRail is actually designed to be a Dell/EMC job creation scheme for VARs."
101.json,t3_88ivbo,t1_dwme4yv,2018-04-01 18:26:16,"Are you asking why people buy supported, enterprise grade solutions,  by large ""proven"" companies? Because I think it's pretty self evident."
101.json,t3_88ivbo,t1_dwmehhp,2018-04-01 18:41:16,"S2D has a pretty high barrier of entry. 4 Nodes, NVME and SSD's, RDMA Nic's, typically better than 10G switching and Datacenter licensing.Then it runs Hyper-V - which whilst its pretty much feature-parity with vSphere - tons of vendors simply don't offer an appliance for it (but do offer an ova).MS just posted that they now have 10k S2D production clusters out there in wild - and thats just what they can see via telemetry. Tons of Cluster infrastructure doesn't have web connectivity. Given that MS didn't have HCI platform - and now they do with 2016 - things are going quite swimmingly i'd say."
101.json,t3_88ivbo,t1_dwmgc3e,2018-04-01 19:55:22,"Unless you're spinning up and down and your workloads are not long term, then a good HCI solution will smoke cloud year over year from a cost comparison perspective."
101.json,t3_88ivbo,t1_dwmgd5w,2018-04-01 19:56:28,Nutanix will have some new features aimed directly at Netapp soon :)
101.json,t3_88ivbo,t1_dwmgeo8,2018-04-01 19:57:55,Are you using AHV for virtualization layer?
101.json,t3_88ivbo,t1_dwmghqa,2018-04-01 20:00:56,Make sure you get a VAR involved that has a Nutanix business when ready (shameless self promotion). Most resellers are pushing services through distributors and can't tie in products that enhance the Nutanix foundation.
101.json,t3_88ivbo,t1_dwmnu7r,2018-04-01 23:04:45,Aren't they gone?
101.json,t3_88ivbo,t1_dwmnwlk,2018-04-01 23:06:05,Few hundreds of nodes deployed world-wide. No major issues so far.
101.json,t3_88ivbo,t1_dwmo0rz,2018-04-01 23:08:25,"We decommissioned all of our S2D ""experiments"" we tried to put into production. There's no support basically, and every single roadblock you hit gets escalated to their R&D team."
101.json,t3_88ivbo,t1_dwmo5s8,2018-04-01 23:11:13,"We use StarWind with VMware as well. Major drawbacks are Windows ""controller"" VMs, and weird licensing. You make a point about Hyper-V adoption. Since late 2016 Hyper-V isn't on the rise anymore. KVM is king."
101.json,t3_88ivbo,t1_dwmokkb,2018-04-01 23:19:24,How do you compare them? Did you benchmark vSAN against Nutanix?
101.json,t3_88ivbo,t1_dwmopfw,2018-04-01 23:22:02,"What's the main problem that hyperconverged is solving that traditional silod data center storage is failing to provide?  My idea of Hyperconverged is that they sell it as, ""oh just throw a few nodes here....and there.....and there""  But I don't get why it is so important to have hyperconverged storage at every micro branch location.  I thoughtbandwidth was getting cheaper.?"
101.json,t3_88ivbo,t1_dwmsrqy,2018-04-02 00:37:18,"we run their switches in eu for eu customers ,there’s no much huawei in ‘merica"
101.json,t3_88ivbo,t1_dwmsw35,2018-04-02 00:39:26,"we use lots of starwinds vsan with hyper-v incl. free versions for both .. s2d is another story ,pretty san one tbh;("
101.json,t3_88ivbo,t1_dwmt983,2018-04-02 00:45:59,+1 from me for both !
101.json,t3_88ivbo,t1_dwmtin7,2018-04-02 00:50:38,"if under experience you mean clusterfuck then yes ,we had to deal with scaleio :( did you know dellemc had ripped them from software skus recently ? they aren’t available for download anymore ,and there’s a good reason for that - nobody wants it ,even for free !!"
101.json,t3_88ivbo,t1_dwn1sfx,2018-04-02 03:20:57,So many nutanix posts here and I am really surprised. We had them in for a discussion and eventually got to quoting. Laughably high compared to vsan through Dell. Nutanix wanted 1 million while a similar spec vsan was in the 300k range. They seemed to walk out of the meeting knowing they had no chance after we mentioned we were in the late stage of quoting vsan.So we put in vsan instead. So far only one issue unrelated to vsan itself (driver / firmware issue with hba so same issue would happen independent of vsan). Great results so far and enough money left over to build another site compared to nutanix
101.json,t3_88ivbo,t1_dwn86ln,2018-04-02 05:21:14,"I don't regret my decision to go with StarWind for one second.  Before StarWind, I had hosts connected to an Equallogic SAN, and I also had hosts connected to an EMC SAN.  The move to StarWind was partially about cost, but more about simplicity.  With 2 nodes, I am running our whole Hyper-V infrastructure with 50+ VMs, including SQL, Exchange, Domain Controllers, and others.  The solution has stood up for 2+ years without a major problem.  Performance has been superb so far.  Reliability has been good.  As a matter of fact, if one node goes down, no one knows a thing because of how the sync works.  If I had to do it all over again tomorrow, I would happily use StarWind again as the backbone for Hyper-V or VMWare.I also starting to use StarWind VTL for backup purposes.  I can't comment much on it yet because I only installed it recently.  However, I am expecting it will replace my LTO/RDX backups.If anyone wants any specifics, I'll happily talk about them."
101.json,t3_88ivbo,t1_dwnerlk,2018-04-02 07:29:49,This was my second choice for a small branch; 2nd only to Nutanix.  I would have liked the play around with a VRTX...maybe I will someday.  It's sooo reasonably priced.
101.json,t3_88ivbo,t1_dwnkfhj,2018-04-02 09:23:35,"We went with a hybrid solution.  Hyperconverged for the application, and separate hosts with local storage for OLTP databases.  Fairly happy with it."
101.json,t3_88ivbo,t1_dwnnhzw,2018-04-02 10:25:05,"The selling point of VxRail is supposed to be that any idiot can set it up.I know for a fact it's much easier to setup a vSAN cluster than it is VxRail, and the pre-reqs are a lot lower too.VxRail is an inferior product to the core component it uses under the hood, and not only is that disgusting from a technology standpoint, the cost factor should literally make it a crime of grand theft."
101.json,t3_88ivbo,t1_dwnomro,2018-04-02 10:48:01,"You need to do a TON of research on this technology that is still fairly new.We decided to go with a home brewed solution using Supermicro servers and Hyper-V with Storage Spaces Direct.  They say ""it will work with anything!""Bullshit.We went through several types of NICs (ended up with Chelsio cards for RDMA) and several different RAID controllers to find the hardware that works.In the end, it was a learning experience, and we do have a solid platform.  This technology and hardware that supports it isn't well documented.  The marketing is still far ahead of what is actually developed.Would I do it again?  Probably.  Not needing to worry about a SAN as a single point of failure is nice.  Manufacturers and the software vendors really need to get together on this stuff."
101.json,t3_88ivbo,t1_dwnqz7b,2018-04-02 11:39:57,"From my research, all I've seen are people who regret it.I don't see the upside.Ease up upgrading ESXi? It's already easy.Ease of having one number to call when things break? I have zero numbers to call if I don't buy crap.The downside, I've read with hyper converged, if you need 100TB of storage, you need to buy (3) hyperconverged systems, each having 100TB of capacity. This is what I've read is the only way to make them redundant. Maybe I'm not understanding that correctly?Second downside, right now I can buy 500+gigs of ram off the internet to add my pizza boxes for ~cheap. Can you do that with hypercongerged? Maybe you can, I don't know."
101.json,t3_88ivbo,t1_dwnxgbl,2018-04-02 14:58:21,"There is a reason behind this.  Honestly, all hyperconverged platforms should be built on a high bandwidth clos network fabric.  This usually means getting an ACI or EVPN-VxLAN based backbone.  All too often these solutions are just purchased/sold by traditional server teams.  Then they'll cobble together some Netgear switches from BestBuy and wonder why they have performance issues....Have numbers to substantiate your NVMe view?  That's something I really only hear from Brocade/Broadcom people."
101.json,t3_88ivbo,t1_dwnz4ix,2018-04-02 16:11:34,"We've recently moved 35 servers from an IBM centre / IBM ds3x00 SAN / 4 host hs22 VMware 5.5 cluster over 8 g FC to SAN environment to a 4 host cluster and have had no issues but we don't have any heavy hitting servers yet ( moving solarwinds npm/Sam/nta over to the solution soon). So far no issues other than lack of p2v included in the solution. Xtract for vm worked smoothly to move over the esxi guests to AHV.Allowed us to reduce from 35 U storage and 9 U computer to 4U including networking ( dual cisco nexus 3172) so boss is happy. Performance wise seems faster and management is simple and not a chore ( including upgrading AHV and prism)We use HyCU for backup with an old quantum dxi6701 as the target with goal to replace with a qnap rack mount Nas and third copy / take out in awsWe're trying to get agreement to replace our datacentre with similar but also use nutanix and ABS to provide storage for a 15TB ms SQL server, not sure about nutanix performance in that role nor is my boss. Happy with the idea of iSCSI replacing FC but nervous about the Io performance of the nutanix storage heavy nodes being proposed. Just something new and interested in our environment so far and DB server has lot of visibility if things go wrong or performance isn't good enough ( currently it runs on two ibm hx5 servers and IBM v7000 san, plan is to run it on two hp dl360g10s with dedicated dual 10gb ports to storage vlan on the nexus switches and nutanix storage heavy nodes)"
101.json,t3_88ivbo,t1_dwnzhvp,2018-04-02 16:28:57,Anyone heard of ZeroStack?
101.json,t3_88ivbo,t1_dwo9316,2018-04-02 21:51:45,"Also on Simplivity here. purchased during the acquisitio, so all HPE gear.  2, 2 node clusters to start. Next year we're phasing out all of our aging Netapp (100+TB over 3 SAN's)  and UCS, and going 100% Simplivity. Probbaly 2 more storage nodes and 6-7 compute nodes.  It's performed great, their data processing using the accelerator cards really makes the difference IMO."
101.json,t3_88ivbo,t1_dwol55s,2018-04-03 01:11:40,"We're using Simplivity 4 node cluster. I really like it a lot, way less to manage. The initial deployment was pretty intense, but once we got past that, it is a really good solution."
101.json,t3_88ivbo,t1_dwq27nd,2018-04-03 19:08:18,You sure?
101.json,t3_88ivbo,t1_dwq2b88,2018-04-03 19:11:16,"This isn't true. None of the guys you list do page assembly in memory, except ex-SpringPath."
101.json,t3_88ivbo,t1_dwq4l8t,2018-04-03 20:11:10,Only 1GBps with a fullSSD SAN? I'd say something is broken. We have Infortrend disk arrays (10k SAS IIRC) benchmarked around 5GBps.
101.json,t3_88ivbo,t1_dwqpuc7,2018-04-04 01:38:49,"Yep. It looks like Hyper-V currently struggles competing with ESXi and KVM.That's being said, we've done a whole lot of Starwind installations as MSP and IMHO it is really one of the best solutions for SMBs. Fast, reliable and easy to use once you are done with configuration manual."
101.json,t3_88ivbo,t1_dwuqytk,2018-04-05 22:26:54,We are an established VMware partner and I found scale at spiceworld and was interested. Myself and my team were promised a group of nodes we could wheel into our offices and test on the promise of a “try and buy” we actually hated it as we stress tested it ourselves and completely lost our test VMs we performed tests such as cut the power to a node and that node never came backup after a simple power outage requiring a remote engineer to fix. In the end we had to completely rebuild. So we ended our try and buy and the company had engineered the small print to make this original offer not even exist so they forced us into a sale. We ended up hand delivering it back to their U.K. branch some miles away as they refused to take back. Replication of VMS is to a completely new cluster of nodes. Not within a node itself and mainstream backup tools like veeam are not compatible. You are best sticking with VMware but utilising the basic package with veeam backup and replication protecting either local storage VMs or the inbuilt replication appliances. You can do this on a budget. But my advice steer clear of scale
101.json,t3_88ivbo,t1_dwuy2ya,2018-04-06 00:09:47,"CALM and some of their acquisitions should help in near future. If you're single IT guy, check out Plexxi to bring your network management into the PRISM interface."
101.json,t3_88ivbo,t1_dxxyxc2,2018-04-25 22:19:09,Hi there. Sorry to hear you are having trouble. Is there anything we can do to help? Happy to help escalate. Please ping me an email: erin.monday@nutanix.com.
102.json,t3_8ukgem,Potential switch to HCI,2018-06-28 23:28:34,"So our 5 year hardware refresh is coming soon and I've started looking into potential replacements, mainly switching to hyperconverged environment (but not completely sure yet). The reasons are that this environment is relatively small (slightly under 100 assets in total) but heavily complianced and we have two locations that have to be identical and have to keep track of every single software/firmware patch monthly so having an all in one type environment will help out with that. Ive went to training seminars on HyperFlex and VxRail and so far HyperFlex seems better. Does anyone currently use HCI and have any good input on the good/bad? Should I stay with the current more traditional route? As an extra note I'm currently a JSA here so I dont by any means have deciding/last say power but I'm treated like the SA in the environment. This is just to assist in my research with people that have actually used HCI hopefully."
102.json,t3_8ukgem,t1_e1fzth8,2018-06-28 23:41:58,"I run HyperFlex HX. We are a heavy UCS and VMware shop so extending our knowledge base to the Cisco solution was a natural progression. Currently we only run HyperFlex HX at branches that require onsite services. Our data centers still run full UCS infrastructures. Our HX system contains 3 c240M4Xs with 15TBs of storage. The 15TBs is front ended with 1.5 TB SSDs. The SSDs get hit a lot which makes the storage seem a lot faster than it actually is but this is pretty obvious since most of the work the branches do hit frequently used files on file servers.I like the HyperFlex HX system and my gripes mostly come from security best practices that the HX system doesn't adhere to and the rigid structure that the UCS piece of the HX system has to use for things like VLANs and vNIC templates. Our original intention was to attempt to run VDI over the HX system but the price point/ROI to run the HX systems versus simply buying more disks and extending our AFA and purchasing new UCS blades was not optimal. Ultimately it's way cheaper for us to buy new B200s and SSDs for our arrays then it is to purchase new HX systems.A lot of people try to convince me that Nutanix shits all over Cisco and HyperFlex HX but to be honest with you, the performance is pretty good and UCS is probably one of the most stable compute platforms I've ever run with the only rival being old IBM X-Series servers (dead serious). The TCO for us for HyperFlex HX is really good because we get deep discounts due to how heavily invested in Cisco technology we are.TL;DR:"
102.json,t3_8ukgem,t1_e1g16u5,2018-06-29 00:00:16,Thanks for your input! We are a VMWare shop but we use HPE servers. Yeah Nutanix and Simplivity were other top HCI that have been recommended so Ill also give them a deeper look.
102.json,t3_8ukgem,t1_e1gb8fl,2018-06-29 02:17:16,"VxRail - pretend it doesn't exist. Hate this platform with a passion.Cisco UCS - so much complexity for little value unless you're running at a huge scale.Traditional FC SAN - it does storage, it does storage well.Dumb as possible 1Us for CPU and Memory (compute) - does compute well. Install hypervisor, connect to storage, add to cluster, and go.We're running away from HCI because VxRail delivered anti-value (same relationship as matter and anti-matter.) One VxRail cascading failure across multiple appliances. Just over half our nodes have been replaced for hardware problems, none of them have been plug and play swap outs. All of them required on-site support from EMC with a special USB install to get the hypevisor loaded and joined to VxRail. All flash storage writes at USB2 disk speed. EMC VxRail support constantly schedules remote and onsite work and then no shows. We have to call them for apologies and rescheduling. The VxRail management appliances (VMs) have common username/passwords for EMC support, like they learned nothing about how back doors are bad over the past 30 years.VxRail is probably better now that it's running on Dell hardware instead of Quanta, but it's still just VMware running VSAN with EMC's anti-value VxRail software running. I might consider VMware and VSAN without VxRail, but not for long if a proper SAN was on the table."
102.json,t3_8ukgem,t1_e1gb9k0,2018-06-29 02:17:44,"We had old servers and needed expansion during a new hospital project and ended up going with Nutanix all flash with VMware. We're a small shop with one sysadmin (me) and a few techs. Dealing with UCS looked like a nightmare, and we had been hearing of some large organizations we stay in touch with absolutely hating their UCS and replacing it.If Nutanix AHV (Nutanix hypervisor included with Nutanix, but not required to use) matures enough (it's really close) We will be able to cut out VMware products (except for the VDI nodes) and save a lot of money. Veeam just announced AHV compatibility which will start to change the game from here on out."
102.json,t3_8ukgem,t1_e1gc96b,2018-06-29 02:31:20,"Cisco UCS is not a comparable HCI. While it is a HyperConverged platform, it's really only HyperConverged from a compute standpoint. A more fair comparison is Cisco HyperFlex HX which combines compute, memory, and storage into a single solution even though it is built on UCS technology.I agree with you that Cisco UCS is for bigger scale, but again, not a good comparison to Vx/Nutanix/Simplivity."
102.json,t3_8ukgem,t1_e1gcio7,2018-06-29 02:34:59,"Absolutely not a nightmare. I am a single engineer in charge of about 80 - 100 blades in UCS. UCS takes up exactly 5% of my time weekly and most of that is reviewing statistics and hardware statuses. When I need to add a blade, I simply spin up a new service profile and pop in the blade and I have a new server ready to go. As soon as it's provisioned I apply a VMware host profile and I have a new ESXi host in less than 30 minutes fully compliant and secure according to our internal documentation. In fact, it takes me longer to go through the documentation and fill out the necessary collateral than it does to provision the server."
102.json,t3_8ukgem,t1_e1ghpdz,2018-06-29 03:46:38,"VMware vSAN. It’s the market leader for a reason. If you are already a VMware house then it just makes sense. Why buy another product and then run it on top of VMware anyway. Just buy the vSAN license and off you go. Obviously you need the correct underlying hardware but that’s easy too as you just buy the vSAN ReadyNodes from any of your favorite major hardware vendors. VMware are also running good discounts on vSAN licenses at the moment as an FYI.Also if you have a forward looking cloud strategy, VMware are building their own cloud on top of AWS using vSAN technology so you will be perfectly poised to move to a hybrid cloud platform whenever you ready."
102.json,t3_8ukgem,t1_e1gotoi,2018-06-29 05:28:26,"Hi KhueCisco UCS does have a bit of a learning curve on the front end of the ownership cycle. This is true with most any technology. After decisions are made and policies are built it becomes a very simple solution to administrate over time. Cisco has several reference architecture documents, called Cisco Validated Designs, to help with the learning curve.By itself Cisco UCS is not an HCI solution, but adding the HyperFlex feature to UCS makes it into an HCI solution. HyperFlex can be thought of as a feature of UCS, the same as the Virtual Interface Card (VIC) is a feature of UCS.The HyperFlex solution helps with the learning curve that happens on the front end of UCS ownership. When HyperFlex is deployed it automatically configures most of the UCS policies based on its own reference architecture. These policies include network, compute, and storage configurations. They also include virtualization configuration for both VMware vSphere and Microsoft Hyper-V.After HyperFlex is deployed the management of HyperFlex can be accomplished locally with the HyperFlex Connect HTML5 interface, with a vCenter plug-in, or via Cisco’s web portal called Cisco Intersight. Making operations, upgrades, and troubleshooting more simple than competing solutions like VxRail or Nutanix.Ultimately there are lots of choices for HCI solutions and Cisco’s UCS with HyperFlex is a really good option. Hardware and software backed by Cisco TAC is a big plus. But the real comparison that needs to be done is whether HCI makes sense at all for your organization. Generally this is an economic decision. The cost of traditional compute, network, and storage stacks and their associated maintenance contracts need to be compared to the cost of an HCI solution and maintenance. Because traditional storage array maintenance is very expensive an, there is usually a clear ROI for HCI because the storage is inside of the x86 compute. And x86 compute maintenance contracts are a lot less expensive than storage array maintenance contracts. The analysis should still be done for each organization to see if that generalization holds true."
102.json,t3_8ukgem,t1_e1rwr97,2018-07-04 20:41:12,"Check out StarWind HyperConverged Appliance: https://www.starwindsoftware.com/starwind-hyperconverged-appliance. You will get hyperconverged environment based on at least 2 servers without breaking a bank. The appliance is covered by ProActive Support, thus you will not spent a lot of time to monitor system health. Also, support assists to move production environment to the new platform with the minimum downtime, which is extremely useful. I have a bunch of customers who are completely happy with their choice."
103.json,t3_ag0eyz,Horizon View setup,2019-01-15 05:08:21,"So I've inherited a Horizon View environment.  I've got some older HP hardware currently running the environment but we recently purchased a VXRail setup from Dell.  We started from scratch.  I've currently got 4 VLANs. 25 - Management In the old environment, there was alot of crosstalk between the management and VDI subnets.  We had vcenter on the management VLAN but the view brokers were dual homed on Mgmt and VDI.  Thin Clients existed on the VDI network along with the VMs.  So, in the new environment, trying to get it right the first time around.  Where should I be putting the thin clients and the brokers?  This is a small environment, less than 50 thin clients, primarily using Instant Clone."
103.json,t3_ag0eyz,t1_ee2rqzi,2019-01-15 05:57:42,Separate everything. Dont forget that your clones and your thinclients EACH get IP addresses. You can run out of addresses fast.
103.json,t3_ag0eyz,t1_ee2sdfa,2019-01-15 06:05:10,"I always put the clients and brokers on the VDI VLAN. I don't want any routing between clients and the Connection Servers.EDIT: I'll edit this by showing this with a separate VLAN for each desktop pool. Look at the VLAN Summary section. However for smaller deployments, it's fine to keep them in the same VLAN.https://docs.vmware.com/en/VMware-Horizon-7/7.4/horizon-architecture-planning.pdf"
103.json,t3_ag0eyz,t1_ee36g6x,2019-01-15 09:08:25,"Brokers and all other management on management VLAN, VDI VLAN should be for VDI desktops only. This is mostly for security and just common sense in my opinion.Also separate routable VLAN for the thin clients. Make sure to look up PCOIP Secure Gateway and make sure you aren’t routing PCoIP through the brokers unless needed due to routing."
103.json,t3_ag0eyz,t1_ee39alc,2019-01-15 09:45:41,"This is correct.We rolled out a 4 Node VxRail v570F cluster last summer, running vSphere 6.5 and Horizon View 7 with about 250 VMs.ENABLE DEDUP AND COMPRESSION ON YOUR VSAN DS BEFORE ROLLING INTO PRODUCTIONSome of the other advice basically said “separate everything with VLANs”That’s not necessarily true. If you have a strong L3 Router you can get away with more Inter VLAN routing.Keep your VSAN traffic on a separate, non-routed VLAN. This VLAN should live on your storage switches only.Your management network should be where your ESXi management, vCenter, VXRail Mgr, and ESRS interfaces sit.We separate our management servers on another VLAN. Composer and connection servers sit on this VLAN.  You could move this into the VDI VLAN I suppose, to reduce routing.VDI VM Interfaces on their own VLAN.Our Thin Clients are on the standard access VLANs in the buildings. And route to the VDI VLAN"
104.json,t3_b73vuy,Which HCI?,2019-03-30 06:52:12,"For the consultants and Contractors out there, do you see more vxrail or Nutanix (or something else)?  Any major problems on either platform?"
104.json,t3_b73vuy,t1_ejp52ch,2019-03-30 07:32:01,"only nutanix, no issues. MCS, PVS, with or without nvidia grid. Works beautiful."
104.json,t3_b73vuy,t1_ejp9d74,2019-03-30 08:29:58,"If it's North America, Nutanix is a cult. We're trying to undo all the brainwash Nutanix has done. Super expensive and overrated. Has issues on clustering (iSCSI). vSAN had some issues some years ago and it was not really suited for XXL environments. Now I see it more and more stable and issues being fixed. VxRail is vSAN with a support layer on top. Super expensive. Does not worth the money when you can get vSAN Ready Nodes at a fraction of the price.So what we see in general is still Block Storage. Mostly Pure Storage but when it comes to VDI on HCI, vSAN is second in North America after Nutanix and first in other parts of the Globe.PS: XXL is > 10k machines"
104.json,t3_b73vuy,t1_ejpbirr,2019-03-30 08:59:37,Look at netapp hci  - solid solution. Runs our 5500 desktop pvs farm
104.json,t3_b73vuy,t1_ejpddhr,2019-03-30 09:24:47,I would do an in depth price comparison for normal architecture with a flash San vs HCI flash prior to making a decision. I find HCI not to be comparable in pricing.
104.json,t3_b73vuy,t1_ejpgl1p,2019-03-30 10:09:50,“Money is no object”
104.json,t3_b73vuy,t1_ejsk7l8,2019-03-31 16:16:55,"In HCI market now only Nutanix and VxRail/VSAN has significant market share. So, why choose not proven solution with doubtful perspectives, if you can choose proven one with clean and straight future?"
104.json,t3_b73vuy,t1_ek4i518,2019-04-05 03:42:00,"Dell/EMC VxRail.  we have a 6,000 desktop environment hosted on it.  I have no issues with the software, and the way it is run.  the whole platform is relatively stable, fast, and all around awesome.We have had some hardware issues, however.  Its all covered under warranty but, leaves a bit of a sour taste in the mouth.  Id recommend it though."
105.json,t3_7xq8x3,"Dell 14G vXRAIL, user experiences / impressions?",2018-02-15 21:11:28,"Hi If anyone has invested in Dell latest HCI boxes, I'd like to hear your initial thoughts. One selling point for these instead of buying cheaper VSAN ready nodes is the supposedly simpler patching / upgrade method - I'd like to know how they have managed the whole spectre/meltdown updates recently? Thanks"
105.json,t3_7xq8x3,t1_duab7w4,2018-02-15 22:18:16,I can't really go into details but we bought one and they screwed us on our warranty.  There are better options out there.Edit: spelling
105.json,t3_7xq8x3,t1_duabr3u,2018-02-15 22:29:01,I wonder if there really is an advantage to not just go for ready nodes. I thought the only thing they made easier is the initial setup process with some vXrail specific wizard.
105.json,t3_7xq8x3,t1_duacl94,2018-02-15 22:45:18,"I get the impression you need decent vmware expertise in-house if you bought VSAN Ready nodes, you'd have to managed VCSA/ESXI/VSAN/NSX versions and compatibility issues yourself and in this new age of shoddy, rushed out patches, that could be quite arduous. Where as with vxrail, you install a single update that is fully certified and tested against your stack.That's the marketing BS selling point, the reality is what I'd trying to find in this thread."
105.json,t3_7xq8x3,t1_duacp7l,2018-02-15 22:47:26,"Ongoing maintenance costs are rather large with vxrail, a 7 year TCO compared to Ready Nodes would no doubt be much higher."
105.json,t3_7xq8x3,t1_duae5cu,2018-02-15 23:14:16,"I know someone who is doing a big VXrail deployment, and they picked it over VSAN because everything is handled by Dell, single point of support etc, not having to go back and forth between vendors and argue over stuff.Everything is controlled and  verified before it gets released, this for them was the main reason they went over VSAn/ready nodes"
105.json,t3_7xq8x3,t1_duaeht5,2018-02-15 23:20:23,"It a about risk really. I sell alot of Vxrail, so I just want you to know that. If your a smaller IT shop or want to move away from day to day management, it's a great platform. Dell EMC and VMware co-devloped this product and maintain it. Meaning all disk firmware, vsan software and vsphere updates will always work with no downtime seamlessly. If you had to manage all separate components it's alot if heavy lifting to research and deploy without and comparability issues. With Vxrail you download one software pack and literally one click upgrade NDU. However if you are  more a build shop and want to get hands dirty vsan ready nodes are also a great option. If it was me personally I would do that, but I'm also well versed on all the components. It comes down to how much time and risk are you willing to give up or take."
105.json,t3_7xq8x3,t1_duaf3nb,2018-02-15 23:30:50,Good luck getting pricing.Maybe I've been unlucky but talk about a drag...
106.json,t3_a3djyx,Anyone thin-provisioning across the board?,2018-12-05 23:58:40,"Hello.  I have an environment that consists of hundreds of VMs consisting of both Windows and REHL.  I am working to move most if not all of them from VMAX to XtremIO X2 storage and ultimately VxRail All Flash when the arrays are up for lifecycle.  New datastores are VMFS6. I have done a lot of investigation into guest UNMAP with Server 2012 and up and it's ability to reclaim ""dead space"" on the array when passed through the ESX host with EnableBlockDelete set to 1.  Since I'm moving from spinning disk with no data reduction to all flash with data reduction,  I want to maximize the space efficiency by not having to dedup and compress already deleted files on the guest.  The problem is in order for the guest OS to automatically issue UNMAP / TRIM commands to the underlying disk, it needs to detect as thin-provisioned. Having said that, is anyone currently deploying most or all VMs in their environment as thin provisioned and if so, how are you managing datastore space so they don't fill up with larger VMs? I am already using datastore clusters and was thinking of doing the same with thin provisioned but set the free space threshold for the datastore to something like 25% for a 4TB datastore and turn on automated storage DRS so VMs can be automatically moved as their VMDKs grow / shrink.  Depending on the rate of migration of VMs, I don't want that to create performance overhead. Alternatively I could stick with Thick VMs and somehow automate sdelete to zero out dead space on VMs on a schedule but that seems more clunkly and possibly higher performance overhead. Thanks!"
106.json,t3_a3djyx,t1_eb5gda8,2018-12-06 01:31:33,"every VM i deploy is thin.that being said, you still need to issue unmap commands to allow the array to trim reclaimed space.  (although 6.7 finally natively does this)# esxcli storage vmfs unmap -l dataStoreName"
106.json,t3_a3djyx,t1_eb5ibrj,2018-12-06 01:52:58,"Almost every VM we deploy is thin provisioned. The exceptions are typically file servers, and SQL servers. It really depends on the use case for the VM. Things like webservers, and application servers don't tend to grow as much, so we keep them thin provisioned, and monitor disk growth."
106.json,t3_a3djyx,t1_eb5ixnz,2018-12-06 01:59:39,how are you managing disk space on the datastore?  are you not over committing storage on the datastore so it can't fill up?
106.json,t3_a3djyx,t1_eb5iypd,2018-12-06 01:59:58,
106.json,t3_a3djyx,t1_eb5jjv4,2018-12-06 02:06:33,How often were you running this command?
106.json,t3_a3djyx,t1_eb5kbkm,2018-12-06 02:14:46,[deleted]
106.json,t3_a3djyx,t1_eb5llya,2018-12-06 02:28:29,"I believe the function for this to happen automatically is a VMFS 6 thing, not a vSphere 6.7 thing.  You have to enable ""Space Reclamation"" for the datastore.  I have this working automatically in 6.5"
106.json,t3_a3djyx,t1_eb5lvda,2018-12-06 02:31:11,"We are 100% thin on thin.  Full automation with SDRS clusters.  That said, we also stay well ahead of our capacity growth by having storage ready to be provisioned as/where needed."
106.json,t3_a3djyx,t1_eb5meen,2018-12-06 02:36:44,"All thin provisioned. I have storage DRS running every weekend, rebalancing the datastores as needed. It seems to work fine."
106.json,t3_a3djyx,t1_eb5mo1d,2018-12-06 02:39:31,"Yes, we do thin provisioning across the board with hundreds of VMs. We started thin provisioning most everything around 5 years ago. We use Storage DRS groups of datastores, and set thresholds for freespace (80% due to backup requirements).  We also have alarms that indicate warning levels at 85/95/99%, but the storage DRS automation usually balances out before we ever hit those numbers. For our 6.5 implementation, we finally allow sDRS to split VMs across multiple datastores, which allows it to be a little more efficient balancing smaller pieces of VMs across datastores as opposed to balancing larger VMs.There are around 5-6 VMs that we DONT thin provision, larger multi-TB VMs that need their own datastores anyway, things like Splunk, SQL databases, etc. The basic rule for us is if its less than 500GB, it gets thin provisioned. Anything more gets its own LUN based on possible growth.VMFS 6 supports storage reclamation, and we're already seeing that working for most of our Windows servers, although we're not exactly sure if it's working for all of our RHEL6/7 systems, as the command to check on it is per-datastore and I haven't seen that data down at the individual VM level (haven't dug too deep for it either)."
106.json,t3_a3djyx,t1_eb5mykz,2018-12-06 02:42:36,"Sure. I see no reason to not use thin. Much more efficient use of the storage, and the performance impact I've seen so far is near to nil.I suppose if you absolutely must have the maximum, then thick.Just one guy's opinion."
106.json,t3_a3djyx,t1_eb5whpx,2018-12-06 04:27:47,"When we don't eager-zero our THICK provisioned disks/flash, (write) performance (initial block write) is near to nil.As for the main question: thin on the array: yes, saves space and gives flexibility. Thin inside esx (again): nope, too many layers where sth can go wrong and would need meticulous monitoring."
106.json,t3_a3djyx,t1_eb5zp70,2018-12-06 05:04:31,"We've been thin for over 7 years with 600 or so VMs and a mix of RHEL and Windows workloads.  We've used exlusive thin provisioning on several models of VNX, a VMAX 10k and now a Pure X50.  We use a combination of monitoring and SDRS to insure no one volume ever fills.  Since moving to VMFS6 unmap has been a non-issue.One thing I do have in place are separate datastores for ERP database servers and core vSphere components such as vCenter, PSCs, NSX, vSOM, etc."
106.json,t3_a3djyx,t1_eb603ua,2018-12-06 05:09:08,Great to hear!  Do you see any performance impact from SDRS running on the weekend?  What free space threshold do you try to keep your datastores at?
106.json,t3_a3djyx,t1_eb6141i,2018-12-06 05:20:42,"Thats great to hear!  How often do you have SDRS run?  Do you see any impact from when it balances?  How big are your datastores for non-large VMs like SQL?I'll probably stick with standalone datastores for larger SQL vms and keep them thick as well since once the data files grow, they shouldn't shrink.For RHEL, I believe you need to add the discard option to the files system mount for the OS to automatically issue UNMAP command upon file deletion.Thanks for the info!"
106.json,t3_a3djyx,t1_eb66gdq,2018-12-06 06:23:20,We found the best approach is to thin provision at the underlying storage array and thick eager zero at the VM level (thick on thin).  When you thin on thin provisioned storage there is a significant performance hit especially with your high I/O systems.
106.json,t3_a3djyx,t1_eb672ht,2018-12-06 06:30:39,VxRail’s newest release (VSAN 6.7U1) now supports UNMAP.
106.json,t3_a3djyx,t1_eb6emgt,2018-12-06 07:58:54,Yep I was excited to see that announcement especially since the ultimate goal is to move to vSAN once the arrays are up for lifecycle.
106.json,t3_a3djyx,t1_eb9dcbg,2018-12-07 09:55:04,Another 100% thin environment here. No issues except when the new guys create a VM with Thick provisioning. Such a waste...
106.json,t3_a3djyx,t1_ebatauf,2018-12-08 01:02:57,Do you do thin with SQL servers and other large servers?  How do you manage / maintain adequate free space on datastores?
107.json,t3_984ozq,"upgrading vCenter, vSphere, and vSAN",2018-08-18 02:25:15,"Well, It's like the title says, I need to upgrade all 3 to the newest version. I know vCenter needs to be a higher version than the hosts under it, and therefore has to be updated first. My real question is, do I upgrade vSAN or vSphere next? I've been putting this off for months because our vSAN storage has been hovering around the threshold of being able to survive losing a single host worth of storage (4 hosts at the moment), but we are adding a 5th node to the cluster which will expand the storage. The other part I'm unsure of is how to approach adding that 5th host. Should it be completely up to date before being added, or at the same baseline as the other 4, and upgraded along with them? Also, will the ""Ensure Availability"" evacuation mode be the right way to go when taking the hosts down (one at a time of course) for the vSphere upgrades, or should I use ""Full Migration""? Thanks in advance!"
107.json,t3_984ozq,t1_e4d8k93,2018-08-18 02:29:57,Here's the path for 6.5.  https://kb.vmware.com/s/article/2147289
107.json,t3_984ozq,t1_e4ddprp,2018-08-18 03:44:15,"That KB has save my life more than once, thank you for posting!"
107.json,t3_984ozq,t1_e4dk29q,2018-08-18 05:19:56,FYI - if you buy VXRail it does all the updates and node add for you automatically :)Just a thought to consider as you scaleDisclaimer - I am a DellEMC Employee
107.json,t3_984ozq,t1_e4dkcgd,2018-08-18 05:24:21,Still really pissed I got these Dell blades for a Ready Node right before this became the standard.
107.json,t3_984ozq,t1_e4dnsj1,2018-08-18 06:19:06,"yes, and pay a 40% premium for the pleasure of installing free updates."
107.json,t3_984ozq,t1_e4dukfa,2018-08-18 08:11:39,"Before we get too far... What version are you on now, and are you running the Windows edition of vCenter Server or have you already migrated to the VCSA?In general, you're going to upgrade in the following order:These are only generalities! Your specific environment may need tweaks inside that general path.You can add the 5th host either way: mirror the current systems or wait, but I don't believe you will be able to bring it into the vSAN cluster at a new/higher version if vCenter is the only thing you've completed (ie: this is your first host going to the new version). IMO, it will be easiest if you add it to the current cluster at the current version--either before or after upgrading vCenter--then after validating that the vSAN datastore has no errors and has incorporated the new host into the datastore, upgrade it first. The likelihood that it will have a bunch of objects to migrate off is minimal, so it'll be the fastest to upgrade.The Maintenance Mode option you choose is dependent on your capacity (is full evacuation even an option?), your patience (full evacuation takes T I M E), and your risk tolerance (Ensure Availability has risks if another node has a problem when the one in maintenance is unavailable). This is why putting that 5th node in early makes sense: you should have the capacity for full evacuation (if the disk groups are sized identically as the other members), which can give you more confidence at the cost of time."
107.json,t3_984ozq,t1_e4e2lfx,2018-08-18 10:33:53,This is the correct answer.Maintenance mode on VSAN is like a crap shoot though. You never know if it'll be a 12 hour host evacuation or 15 mins. I marvel at this every time.Don't forget to rebalance your disks when done.
107.json,t3_984ozq,t1_e4f05qe,2018-08-19 00:35:50,"Currently running 6.5, not sure of the build though. We're running a Windows vCenter. For some reason the higher ups are resistant to the idea of going to VCSA.It sounds like getting the 5th node added to the cluster will make more sense. Time doesn't really matter as much as availability and data security. Granted, we have backups of everything on there in case anything goes wrong, but restoring from the backups would be pretty time consuming itself. I'll have to speak with my boss about the 5th host. He hasn't ordered it yet (as far as I'm aware), so I'll make sure he knows it needs to be identical. He said something about also possibly having to change out the RAID controllers in the hosts (Pretty sure the current ones aren't on the HCL. But that's another can of worms entirely."
108.json,t3_6forv6,10Gig Switches with RJ45 connections?,2017-06-07 05:05:47,"Hey guys, quick dumb question. I am looking for a 24-port 10Gig switch that uses RJ45 ports for a VXRail setup.  Most of what I see is SFP+ or fiber.  Does anyone have a recommendation of something that have used/configured before? First time posting so hopefully I formatted all of this right."
108.json,t3_6forv6,t1_dijsxzk,2017-06-07 05:10:14,"You're looking for a ""24-port 10GbE  Base-T"" switch. They do exist.Aka ""10Gbase-T""Example: Cisco SG350XG-24T"
108.json,t3_6forv6,t1_dijsz6c,2017-06-07 05:10:53,"10G over twisted pair does exist and is codified in IEEE 802.3an but be aware that latency is much worse than in other media because of the amount of signal conditioning and filtering that must be done.Also, 10G is not guaranteed to go over structured cabling particularly well, so if it's point to point, you're better off with SFP+ and (for preference) fibre.  I don't otherwise know anything about VXRail."
108.json,t3_6forv6,t1_dijt4ej,2017-06-07 05:13:35,"Depending on what port density you need, Brocade has a few options. Such as Brocade ICX 7750-48C..."
108.json,t3_6forv6,t1_dijt5i5,2017-06-07 05:14:11,"You're looking for 10GBase-T ports. Depending on your budget, some options:NETGEAR ProSAFE XS728TCisco SG350XG-24TCisco WS-C3850-24XU10G"
108.json,t3_6forv6,t1_dijt8go,2017-06-07 05:15:40,Cisco Nexus 3172 is also a good option with 6 QSFP ports on the end for extra flexibility.
108.json,t3_6forv6,t1_dijtie2,2017-06-07 05:20:51,I have read that over short distances the latency is noticeably worse (we are talking like MAX 8 feet).  In your experience has that been the case?
108.json,t3_6forv6,t1_dijtjkl,2017-06-07 05:21:28,What kind of latency increases are we talking about?
108.json,t3_6forv6,t1_dijtjuc,2017-06-07 05:21:37,"Awesome, I thought that was what I needed but I wasn't 100%.  I'm more on the software side of things and getting tossed into some hardware happy stuff :)"
108.json,t3_6forv6,t1_dijtk89,2017-06-07 05:21:49,Thanks for the recommendations!
108.json,t3_6forv6,t1_dik0c6g,2017-06-07 07:38:46,"If at all possible, avoid 10GBase-T. It has a raft of potential problems.Go with SFP+ ports and use twinax (DAC) cables for connection."
108.json,t3_6forv6,t1_dik40wd,2017-06-07 08:57:44,What problems are you referring to?
108.json,t3_6forv6,t1_dik4gf5,2017-06-07 09:06:58,We using Cisco WS-C3850-24XU10G. It cost a shitload for a pair of switches for HA.
108.json,t3_6forv6,t1_dik4jh6,2017-06-07 09:08:48,"VXRail are Dell/EMC systems where integrated storage is shared between physical systems. A ""converged"" solution if you must. This is why they strongly recommend the 10G backbone."
108.json,t3_6forv6,t1_dik8i95,2017-06-07 10:35:03,Nexus. We did a budget project using some Dell N4000 series switches and they've been rock solid too so might be an option if price is a thing.
109.json,t3_c0bp05,VCenter Migration with VXRail,2019-06-14 05:30:39,"Currently we have a Windows and SQL based VCenter 6.5 server (I know, I know). We have wanted to migrate to the VCSA but were told by multiple sales people, engineers, and support that we have to engage Dell Professional Services to do the migration. We were told that Dell has to make changes within the VXRail Manager database to support the change. We have been working with Dell PS but we are now getting mixed messages whether or not we truly need them to do this. Essentially this is an external to external vcenter migration but, according to Dell, it is not natively supported by VXRail and not something a customer can do on their own. Has anyone encountered doing a Windows -> VCSA migration with VXRail? Update: Long story short, Dell PS is not required to do an external to external VCenter migration with VXRail. Hopefully this helps someone else in case they are also told incorrect information."
109.json,t3_c0bp05,t1_er47gs8,2019-06-14 11:30:46,"As this is an external vCenter migration...then VxRail is not lifecycle managing vCenter, nor are Dell supporting it. If it was internal to external it would be a different matter and there are lengthy procedures on how to do this prior to 4.7.200.  As such you can do what you like with your own VC. Imagine if you had a VC that was managing multiple clusters not just VxRail, why would you have to use Dell PS? Back it up first though :)Edit: oh and read the release notes for the version of VxRail software. That will tell you what version of VC at a minimum is required, it’s just essentially to map the version of esx."
109.json,t3_c0bp05,t1_er52mno,2019-06-14 20:47:56,We are capable of doing the VCenter migration itself. We were told VXRail Manager would have issues with a migration of our VCenter and we would need to engage Dell PS.Thanks for the reminder of checking the compatibility matrix. We'd likely do a 6.5 to 6.7 migration/upgrade. VXRail is currently at 4.5.229 so we'd be setup for a 4.7 upgrade when we are ready.
11.json,t3_ajq9wj,Nutanix NDFS or VSAN,2019-01-25 23:45:33,"ok, a simple question, but maybe not a simple answer. Which one would you choose and why? Basically a Nutanix vs VXrail question? From what I've read, Nutanix wins mainly because of the maintenance and support. But both will do the job just fine."
11.json,t3_ajq9wj,t1_eexsgca,2019-01-26 00:14:20,"Our nutanix falls flat on its face performance wise constantly, and requires 2 people to manage it full time. vsan works great till you have to perform maintenance on a whole cluster, put one host into MM, take it out and go to put the next one in, and you have to wait for resyncs to finish, which can take hours.I personally would go with vsan, simply for the ease of management. I don't have vxrail, just vsan clusters."
11.json,t3_ajq9wj,t1_eexskhl,2019-01-26 00:15:23,Are we also talking AHV vs. vSphere or are you still planning on running VMware on both?Whats the use case?
11.json,t3_ajq9wj,t1_eexul7m,2019-01-26 00:34:43,"ESXI on both. General purpose VMs, mostly all low IOPs.I guess i should mention future hybrid cloud capability too."
11.json,t3_ajq9wj,t1_eexusw5,2019-01-26 00:36:45,Why 2 people? The whole USP for Nutanix is invisible infrastructure. I was under the impression if there's a issue you call support.
11.json,t3_ajq9wj,t1_eexv8ov,2019-01-26 00:40:53,"I used VxRail for the past year at my previous job.  It pretty much ran itself for the most part.  Management was very easy.  Upgrades were very easy.  I never had any issues that affected production.  The minor issues I did have were usually resolved very quickly.  VxRail uses the ESRS gateway to monitor the environment.  If an error happens, it will open a support ticket on your behalf.  I had some minor issues that were reported in the middle of the night, and their support team had usually resolved them by the time I got to the office the next morning.  My overall experience with VxRail was a very good one.Having said that, I recently changed jobs.  My new employer is looking to go hyper-converged, so now I'm getting to evaluate all of the different offerings again.  I recently attended a Nutanix bootcamp, and I was able to get my hands on AHV.  It was easy enough to use, but I have no experience with their maintenance and support.  We're also looking at Cisco HyperFlex, NetApp HCI,  HP, and Hitachi in addition to Nutanix and VxRail."
11.json,t3_ajq9wj,t1_eexvhek,2019-01-26 00:43:12,Really?  I think you are the first person I've seen to comment negatively about Nutanix.  Most people rave about it.  I've always been skeptical about it.  Could you give me some more details about your experience with it?  My employer is looking at it and is somewhat leaning toward it.  I'm trying my best to persuade them otherwise.
11.json,t3_ajq9wj,t1_eexy28w,2019-01-26 01:06:37,"I've had a Nutanix CE cluster up and running for a couple of months now, it's given me an overview of AHV.  IMO its not quite mature enough for us - The way you manage networks/uplinks using Open Vswitch is bad, guest customization isn't good, and backup support is lacking.I'll hopefully get a vxrail POC in the next couple of months."
11.json,t3_ajq9wj,t1_eey1xx4,2019-01-26 01:40:31,You're the first I've heard say this about NutanixHow many nodes?What Hypervisor?
11.json,t3_ajq9wj,t1_ef3tu73,2019-01-27 23:49:28,I just wanted to hop in because you were getting downvoted. We had a horrible experience scaling Nutanix. Here is some of our problems:Ultimately we switched to an All-Flash NetApp array and life is so much better now.Granted we already ran large NetApp arrays and a company that didn’t might find Nutanix easier.
11.json,t3_ajq9wj,t1_efp0yuv,2019-02-04 09:01:08,"Seconded. Set up hyperconverged environment over VMware ready nodes or make a DIY project. I do not know your exact requirements, having a guess that is more about management and storage question (how to make it work with VMware). Check Dell/HPE/Lenono ready nodes. If you do not want to run VMware VSAN, check HPE SimpliVity or StarWind Hyperconverged Appliances. Those vendors have their own VSAN-like solutions that can be integrated with VMware vCenter for sure."
110.json,t3_6xeixh,Migrating Citrix XenDesktop VDI persistent VMs on netapp vsc to Horizon on vxrail,2017-09-01 21:30:45,"Hi, I'm mostly a Citrix guy but I've been tasked with a migration of ~100 developer class persistent VMs to all flash vxrail poc with horizon as VDI, if the developers are happy we might scale the solution, This is the first time I've been tasked with start-to-finish VDI Horizon implementation and I have professional services as guidance but from my experience I know I can't count on them to know the organization as much as I do and that would be crucial for the success of it. Does anyone here have some sort of general walkthrough or any gotchas about this kind of migration? Here's what we have now: VMs have local profile and are assigned to specific users. no App Virtualization or Personal vDisks. I was think about going at it in the following manner: doing a v2v and having the VMs copied to the vxrail environment Create management servers for horizon like vcenter appliance Setup connection servers and composer - need to know if I'm missing anything here and what do the do, should I go for 1 of each? Remove Citrix VDA etc. Upgrading VM Tools to match newer ESX versions Install View agent In parallel set up UEM share -need best practices for performance, wondering how I can dedicate some vsan resources for it In parallel setup app volumes -not sure I have any use for it in the poc since the VMs already have the dev. Applications on them, perhaps when I upgraded to Windows 10 I should keep the image as lean as possible and have the applications served from app volumes, What benefits does app volumes give for VDI ? I've probably missed tons of stuff there I appreciate your help on the things I've pointed out and once I missed. Thanks!"
110.json,t3_6xeixh,t1_dmfykch,2017-09-02 05:30:51,"If you are using View Composer then you are probably going to utilize linked clones or Instant Clones. Either way it will simply be easier to build the VMs in the new environment and migrate any data over afterward. Otherwise you are going to be manually adding and managing a ton of VMs and not taking advantage of any benefits of the View platform.I'm glossing over a LOT of stuff but this is the basic flow. Biggest thing is to know what you are doing before hand, read the View admin guides and try a few test pools first, make changes, see how those will affect your devs and anticipate issues that might arise.Most common issues I've run into without app layering are corrupt snapshots on the golden VM which causes you to have to rebuild the entire thing usually and depending on how many different snaps and load outs you have that can be a lot of work that gets sprung on you all at once. We moved to Unidesk (now Citrix App Layering) but since they've been bought I'm concerned about future development on the platform for Horizon."
110.json,t3_6xeixh,t1_dmjfu78,2017-09-04 14:48:11,Thank you for the detailed insight!
111.json,t3_463x0v,VxRail is here! VCE's new Hyper-Converged Infrastructure product is something to be excited about.,2016-02-17 02:48:55,[]
111.json,t3_463x0v,t1_d029ga5,2016-02-17 03:49:54,"Looks like this is EVO:RAIL / EVO:RACK 2.0 with some EMC flavor added to it.I know EMC was one of the OEMs for EVO:RAIL, I wonder if they are continuing to both programs are halting EVO and solely pursuing VxRail.Overall, it'll be interesting to see how these products sell."
111.json,t3_463x0v,t1_d02afaj,2016-02-17 04:11:32,"Yes, it is very much like VSPEX Blue rolled into the VCE umbrella, which adds a lot of value IMO. It will be interesting to see if EMC continues to sell VSPEX Blue, though, especially after the merger."
111.json,t3_463x0v,t1_d02fhqw,2016-02-17 06:03:29,Exciting stuff :)
111.json,t3_463x0v,t1_d02fwne,2016-02-17 06:13:06,"I would bet that this is the new EVO Rail.  Just EMC, no other OEMs."
112.json,t3_ba3gc3,Unable to find virtual machine with name VMware vCenter Server Appliance,2019-04-06 19:38:08,"Trying to upgrade our E560F cluster from 4.5.152 to 4.5.229 (first upgrade since base install) getting the above error when trying to upgrade. Its like its trying to resolve the DNS name for the VCSA as ""VMware vCenter Server Appliance"" which is the default VM name but not the hostname supplied on build and therefore not the name being used in DNS. Anyone seen this before? its like hostnames are hard coded somewhere into the VxRail manager. I am about to open a case with Dell EMC"
112.json,t3_ba3gc3,t1_ek8txfj,2019-04-06 21:13:06,Looks similar to https://support.emc.com/kb/520715Possible UUID Mismatch. Not as straight forward with the vCenter.I’d open an SR.
112.json,t3_ba3gc3,t1_ek8xu7e,2019-04-06 22:19:23,"Thanks for the replyTLDR: UUID mismatch between vCenter and VxRail manager needed updating----------For the KB I get ""You Are Not Authorized to View The Content "" Would be interested in the content if you can share?I opened an SR and worked with support. it was indeed a UUID mismatch. Not sure how as nothing has changed on this setup in the year we have had it. Anyway they managed to update the VxRails virtual_machine table and runtime.properties and we are underway on upgrade now."
113.json,t3_c4x7d6,Dell vxRAIL good/bad?,2019-06-25 06:32:34,"We are looking at doing some modernization in our DC. Looking to possibly replace existing Cisco UCS/HP 3Par infrastructure with a Dell vxRAIL implementation. It seems like it would save alot of time with management, and improve our performance with faster CPUs/memory/and SSD storage, and it works out well that we are a VMware shop as well. Anyone have any experience with vxRAIL and have any advice or feedback on the product?"
113.json,t3_c4x7d6,t1_eryse58,2019-06-25 06:43:31,"Bad. Went back to pizza box servers and fiber channel SAN. Purging our racks of all EMC equipment bad.Life is good again when I don’t have to worry about what interesting way VxRail is going to take a dump and steal a couple weeks of productivity.Edit: Oh and DellEMC, please don’t PM me. Again."
113.json,t3_c4x7d6,t1_eryt1gr,2019-06-25 06:51:29,"I'd say that was the promise with Microsoft S2D (now Azure Stack HCI) which is a similar style of solution. Speaking for S2D, it's been nothing but downtime, difficult maintenance, major software bugs and everything else it was supposed to solve."
113.json,t3_c4x7d6,t1_eryvtb2,2019-06-25 07:26:46,Don't bother. Stick with traditional imo. Our VXRail had long running implementation issues and now upgrades etc aren't all they are cracked up to be. Support is still pretty shitty too.
113.json,t3_c4x7d6,t1_eryw1u8,2019-06-25 07:29:48,Stay the hell away from the VXRail. We have 4 nodes and its been an absolute disaster.
113.json,t3_c4x7d6,t1_erywpxj,2019-06-25 07:38:25,Don't do it.
113.json,t3_c4x7d6,t1_erywua7,2019-06-25 07:39:57,Nutanix here! For us it’s great!
113.json,t3_c4x7d6,t1_eryxj8q,2019-06-25 07:48:56,Boo to VXRAILRecommend Nutanix.
113.json,t3_c4x7d6,t1_eryz4ow,2019-06-25 08:09:27,If you have existing UCS why not look at HyperFlex?Those UCS systems could be extra compute if you need the elasticity.
113.json,t3_c4x7d6,t1_erz25xp,2019-06-25 08:48:09,"8 appliances, 32 nodes here. Black hole for time, money, and occasionally a VM or two."
113.json,t3_c4x7d6,t1_erz3nr9,2019-06-25 09:07:02,just vxrail or hci in general?
113.json,t3_c4x7d6,t1_erz4mw7,2019-06-25 09:19:22,I use it with 6 nodes in a stretch cluster and to me it has been 99 percent rock solid.  All ssd.  Great performance etc.The one click update is definitely a joke.  Not true at all.  But aside from that it's been good.
113.json,t3_c4x7d6,t1_erz5biu,2019-06-25 09:27:50,Don't even think about using VxFlex OS if you do.
113.json,t3_c4x7d6,t1_erz5f4v,2019-06-25 09:29:08,"Holy shit, this sounds exactly like VxFlex OS. We have been battling those same fights with it."
113.json,t3_c4x7d6,t1_erz6c88,2019-06-25 09:40:45,"Hmm, they won't pm me when I trash ScaleIO/VxFlex OS/FailIO."
113.json,t3_c4x7d6,t1_erzj7lo,2019-06-25 12:42:45,Nutanix storage nodes have enough smarts to talk to each other and coordinate VSAN activity without the need of another appliance to coordinate things.VMware VSAN in the other hand needs the VSphere appliance...and if you have the VSphere appliance on the same VSAN you’re managing you could have a chicken and egg problem. VxRail deploys that appliance onto VSAN... 🤦🏻‍♂️
113.json,t3_c4x7d6,t1_erzomm1,2019-06-25 14:24:36,"The problem with the appliance model is that it sounds good on paper, but when something breaks your hands are tied when it comes to trying to fix it.We had a 4node VxRail for eval purposes in my last job - it took a month plus a dell field tech visit just to get it up and running...As others have suggested sticking with traditional kit is likely to be a better investment of your time, not to mention good for your sanity..."
113.json,t3_c4x7d6,t1_es04lg0,2019-06-25 20:24:30,"+1 for NutanixLast year I reviewed HyperFlex, vxRAIL, Nutanix and Storage Spaces Direct. Narrowed to down to vxRAIL & Nutanix. In the end, went with Nutanix (on Dell hardware ironically) and haven't looked back."
113.json,t3_c4x7d6,t1_es06v5g,2019-06-25 20:58:37,"EMC professional services quality aside, I like our VxRail.  Greatly simplified our overall footprint.  I'm coming from vBlock, though, so take that as you will."
113.json,t3_c4x7d6,t1_es0hux0,2019-06-25 23:14:26,"Other than the major issues we had when setting it up, it has been fine. We had to reset back to factory because of the problems, but it was fine after that. The engineer said that he's never had that many issues with a new installation. It is not too difficult to set up from scratch yourself, but I would recommend having a Dell engineer assisting with the installation. Dell will send a document that you need to fill out and it exports all of the settings for the VX into a JSON file that you'll need to import in during the VX installation. The project manager with Dell was able to get a us 30-day VMware license so we could Storage Motion our VMs from our old Dell hosts to the VxRail. We finished the VXrail project about three months ago and we're happy with it so far."
113.json,t3_c4x7d6,t1_es0kwrj,2019-06-25 23:46:55,"Odd... While I don't administer it, the performance out of the 8 node vx rail i'm working with seems pretty damn good.  Rolling host updates work as advertised, and there really hasn't been much drama about it"
113.json,t3_c4x7d6,t1_esb6bpl,2019-06-29 11:39:29,"We just got our deployment done, not a single issue came up. Great so far. The one-click upgrade is actually 3 clicks.We've had performance testing done as well, so far so good, I'm quite impressed. All ssd flash.Next week, I'm going to start migrating workloads, that should be fun."
12.json,t3_9peyxt,HCI - Hyper Converged Infrastructure - Storage Spaces Direct - Who has the most bang for the buck?,2018-10-19 08:43:10,"I have 8 Hyper-V servers with DAS storage and it's about time to look at a HCI 3-node cluster or Microsoft Server 2019 Storage Spaces Direct. I'm getting quotes from Dell for VxRail, DataOn for Kepler (Microsoft S2D), Nutanix, Scale, and StarWind. So far DataOn looks really good at $47k for a 3 node, all NVMe cluster and that includes a dual Mellanox 25GB Switch bundle, but I would still be using System Center VMM where as VxRail and Nutanix is highly automated. I already know VMM really well, so I don't know if I really need the automation. VxRail seems to be on the high end quoting me $242K for a 5 node with 5yrs support. What are your thoughts?  What did you find that gave you the most bang for the buck?"
12.json,t3_9peyxt,t1_e819s8q,2018-10-19 09:05:01,"Speaking for HCI... Simplivity from HPE has been great for us. I will admit that it isn't for everyone, but the in-line deduplication is best in class."
12.json,t3_9peyxt,t1_e81c4u5,2018-10-19 09:40:30,"If you are planning on using Hyper-V, I would definitely go S2D over any 3rd party HCI solution.  DataOn will help build everything out for you, so it's not like you just get hardware and you have to do all the software side yourself.  Also, I would check out using Windows Admin Center as a supplement to VMM for managing S2D."
12.json,t3_9peyxt,t1_e81ch62,2018-10-19 09:45:48,"S2D - Hands down DataOn. They’re also the only WSSD that does 3-tier storage (NVMe cache, SSD hot tier, HDD cold tier). You’ll get lots of storage depth with great performance at an amazing costToday’s 2016 patches fixed a ton of S2D bugs that have been plaguing users for years. Huge fixes that should stabilize things going into the next year (Keep in mind these bugs aren’t in 2019).Also keep in mind that 2019 didn’t RTM, so it’ll be a few months before you see logo’d hardware. But I can tell you that our tests on 2016 hardware has been pretty damn good so far.Fantastic, non-biased (vulgar) comparison on HCI:http://www.cultofanarchy.org/microsoft-storage-spaces-direct-s2d-vs-vmware-virtual-san-vsan-battle-titans-part-ii-revenge/"
12.json,t3_9peyxt,t1_e81poz8,2018-10-19 13:49:05,"If S2D is your jam (because you're a Hyper-V shop) - DellEMC will sell you S2D Readynodes.These are purpose built and supported by DellEMC under the various Support SKU's (2 hour, 4 Hour, NBD, etc). Who is DataOn when you're playing at home?If you want it turn-key ProDeploy Plus will do that for you.P.s Don't buy a 3 Node Cluster. Buy a 4 or 5 Node Cluster.Nutanix is literally just a button that updates - and S2D will blow it out of the water perfomance wise.Full Disclosure - I work for a Dell Partner, but i'm an architect (amongst other things)."
12.json,t3_9peyxt,t1_e81s37f,2018-10-19 14:55:59,Care to explain why?
12.json,t3_9peyxt,t1_e81xn8u,2018-10-19 18:06:29,Its 2018. Flash is cheap. Genuinely. If you want capacity - dont' wrap it in S2D.
12.json,t3_9peyxt,t1_e8dlbvt,2018-10-25 03:17:17,"starwinds runs circles around s2d when it comes to performance !plus ,they will sell you all-nvme appliance for just a fraction of what hybrid dataon costs ..dataon made their name with sas jbods ,but now they aren’t standing out of ‘me too!’ s2d vendors line"
12.json,t3_9peyxt,t1_e8f7epf,2018-10-25 20:44:17,"Expensive as fuck.They will sell you rebadged cheap low-quality Quanta (Edit: now Intel) OE hardware, their engineering rolled back from custom blueprints to JavaScript / C# plugins for Windows Admin Center, ex-Honolulu. S2D itself is buggy as hell, Microsoft keeps fixing it with odd and breaking with every even update so far.Expensive. Performance isn't there at least yet.You can cross these guys out immediately as they simply don't support Hyper-V. The only hypervisor they work with is their proprietary fork out from Red Hat KVM.Fastest, and the most mature solution on your list. I'm not aware of their up-to-date appliances pricing though."
12.json,t3_9peyxt,t1_e8fol7p,2018-10-26 00:53:21,"I hear Nutanix has great, proactive and responsive customer service and they are happy to help if you have questions. -Signed, The Completely ""Unbiased"" (But Also Probably Really Biased) Nutanix Social Media Team"
12.json,t3_9peyxt,t1_e8geflq,2018-10-26 06:52:44,"Yes, this was the case with our CiBs in the past.  However, we've since shifted gears.  CiBs are EOL and now all our S2D products are Intel Select Solutions.  This means that DataON designs the platforms based on Microsoft's RA for S2D, but the servers themselves are built by Intel.Can you please clarify?"
13.json,t3_4chmm4,VSAN/vCenter requirements,2016-03-30 05:29:44,"Hi fellow vnerds. Looking at HCI replacements for a small, traditional 3-tier cluster in a remote office. VSAN is where it's at IMHO. In addition to spec'ing out a custom Dell 4-node VSAN solution, we also spoke to EMC regarding their VXRAIL solution. The engineer I spoke to said that a vCenter instance running on the cluster would be required for management. I can't find this requirement anywhere in the vmware documentation for VSAN. Can I not have an external vCenter appliance managing the new HCI cluster?"
13.json,t3_4chmm4,t1_d1ifsm8,2016-03-30 08:24:33,A VSAN cluster can be managed by an external vCenter appliance running on another cluster or on physical even. VXRAIL itself may have limitations on where VC runs (not sure on this) but it's not a VSAN restriction.
13.json,t3_4chmm4,t1_d1iggne,2016-03-30 08:40:36,That's what I figured.... thanks for confirming my suspicion!
13.json,t3_4chmm4,t1_d1inzye,2016-03-30 11:50:45,It may also just come prepackaged with a vcenter. The HP HC250 is like this
13.json,t3_4chmm4,t1_d1iuj7t,2016-03-30 16:46:27,"You can, although what's the rationale behind designing it like that? For a small cluster, I'm guessing you wont have a separate management cluster with HA/shared storage etc, so what are you hoping to achieve by having it physical?If you run it as a VM within the VSAN cluster, you get all the benefits that the other VMs have. You dont end up with a chicken/egg scenario - a VSAN will come online when ESXi boots and you can power on the vCenter instance from the fat client (or presumably the new Web client)"
13.json,t3_4chmm4,t1_d1j9doe,2016-03-31 00:54:26,I never said I wanted it to be physical... I just want my current VCSA running in my primary datacenter to manage this new VSAN enabled cluster.
13.json,t3_4chmm4,t1_d1jdd11,2016-03-31 02:19:12,"VxRAIL is built off of the EVO engine, and would come with its own vCenter.If you want your own VSAN cluster managed by your primary datacenter, leveraging VSAN ready nodes would be the most streamlined solutions.also if its a small remote office (and it runs under 25 VMs) you can leverage VSAN ROBO, with a 2 node cluster, as long as your latency between the primary DC and the remote office are within VSAN reqs."
13.json,t3_4chmm4,t1_d1je3uq,2016-03-31 02:35:05,"Ahhhh... That makes sense. Didn't realize EVO mandated local vcenter.And yeah, VSAN ROBO is a product we're closely investigating."
14.json,t3_9uzf6k,V2V with VMWare Converter Standalone - Network Card,2018-11-07 21:30:10,"I'm currently working on the task to migrate VMs off our old classic Cluster onto a brand-new VxRail. We have a requirement to move VMs one by one, so a straight-forward approach of re-attaching a datastore won't fit. So far, the best option seems to be an offline V2V conversion using VMWare Converter Standalone: Old vCenter as Source, new vCenter as Target, adjust Virtual Network/Disk configuration and run. First tests look very promising, with one caveat: The conversion process finishes successfully and the VM starts up just fine, but it seems the Network Adapter is being ""reset"", despite unchecking the ""Reconfigure destination VM"" option. The new Adapter doesn't have any of the previously configured static IP settings, and needs to be manually configured to the old values to bring the VM back onto the Network. While this is not a big issue, I'm curious if this might just be an option I've misconfigured in the conversion process?"
14.json,t3_9uzf6k,t1_e985vm8,2018-11-07 22:18:14,"Take a look VMWare Replication, I think this tool is more suitable for your needs. Also, why won’t attaching the datastore to the new VXRail cluster work?"
14.json,t3_9uzf6k,t1_e986cxi,2018-11-07 22:25:53,It sounds like you may have really old VM versions... did you move from VM version 4 by chance?
14.json,t3_9uzf6k,t1_e98ajal,2018-11-07 23:26:44,Every time I use the converter tool I have this issue. I don’t use it often enough anymore to worry about automating a fix.Since you are going from VMware to VMware you could (depending on versions of vcenter involved) do an advanced vmotion changing both compute and storage in one swing (per vm as per your requirement).Of course old hosts and new vxrail hosts need to be able to see each other either on management or vmotion interfaces. They don’t have to be on the same vcenter but it makes it easier if they are.  Oh and the hosts need to be the same brand of CPU or you’ll have to power the VM down in order to move it.This is my preferred method when adding new hosts and storage at the same time. Don’t need to connect new storage to old hosts or old storage to new hosts.
14.json,t3_9uzf6k,t1_e98awqe,2018-11-07 23:31:46,Thank you - Replication seems like a really good option I didn't consider until now. I'll definitely give it a try!
14.json,t3_9uzf6k,t1_e98be8e,2018-11-07 23:38:11,"I've seen the same behavior. My network adapter always shows up as blank and just have to re-attach. the guest os never lost ip. but like you, using converter wasn't frequent enough to worry. the fix takes around a min."
14.json,t3_9uzf6k,t1_e98p050,2018-11-08 02:36:10,"What version of vSphere is your ""classic"" cluster?  Assuming 6.0+, take a look at William Lam's (lamw07) cross vCenter vMotion tool.  No need to take an outage.VMware Fling"
14.json,t3_9uzf6k,t1_e98wz0f,2018-11-08 04:17:10,If your hosts share a vCenter server and vMotion network you can do a shared nothing migration through the usual vMotion. If you're using 6.5 you can link the vCenter's together.Used shared nothing to migrate 90 VM's without anyone in the business knowing about it. Best migration ever!
14.json,t3_9uzf6k,t1_e98yjhx,2018-11-08 04:36:06,"Thank you, this Fling was at the top of my list until I noticed it required Enterprise Plus Licensing. :("
15.json,t3_5vpogw,VxRail – One Year Later,2017-02-23 18:22:13,[]
16.json,t3_8y3hgz,Dell EMC VxRail,2018-07-12 05:10:55,"Good Afternoon Ladies and Gents, Long story short, we are looking at VxRails with VMWare. Currently on the traditional SAN/Cluster(Hyper-V) infrastructure. Small organization (~300 employees), most of our servers (~40) are virtual. VxRails E-Series would replace our current infrastructure and the SAN/Cluster would be put into DR until funds are available to move DR to the same solution. We also have some CAD workstations that are coming up on EOL, so I was looking at the V-Series as well to replace those and to host our upcoming VDI for Citrix. I am looking for Pros and Cons of this solution such as licensing gotchas, unforeseen requirements, and general notes on what to expect."
16.json,t3_8y3hgz,t1_e27vbpv,2018-07-12 05:40:04,"I’ve personally not had good luck on 6 vSAN installs and my 1 experience with VxRail.  It took Dell support over a week to get VxRail up and running, some kind of issue with their deployment script.A lot of people like hyperconverged for VDI but I’ve never really heard a good explanation why.  My $ is on Dell SC series (SAS if you’re sure 4 bodes is sufficient, but they now offer a 10Gb port in case you outgrow those 4)or for only 40 VMs you can buy a hell of a lot of Azure credits for the price of VxRail if you have fast WAN"
16.json,t3_8y3hgz,t1_e27vt4h,2018-07-12 05:46:44,"We currently have the SC series in place, it's relatively easy to manage. The scalability is great on it, we're getting ready to add two more drive arrays to it now.How long ago was your VxRail deployment?"
16.json,t3_8y3hgz,t1_e27xne3,2018-07-12 06:13:01,"The biggest debate with vxRail and other hyperconverged stuff is the cost (tends to be much more expensive and it's all $$ up front, whereas with traditional infrastructure you're typically not replacing storage and compute all at once all at the same time). That's always been my issue looking at HC stuff.Last time I was replacing 2 or 3 servers in my cluster plus looking at a new another SAN - traditional servers plus another nimble was like $75K, but the comparable vxRail or Nutanix setup was like double. Granted it was a bit more capable and slick, but it was tough to justify."
16.json,t3_8y3hgz,t1_e27yf1v,2018-07-12 06:24:29,"Haven't dealt with vxrail however I have worked with another hyper-converged solution. Instead of something easy to manage and cost effective, we ended up with a solution that doesn't need our needs and requires constant support. The vsan component breaks down all the time and causes a storage faiure. The customer support is awful and all activity (outside of basic operation), is password locked by support.As soon as we can life cycle then out we are going with a Dell sc series"
16.json,t3_8y3hgz,t1_e286omm,2018-07-12 08:35:13,I have heard from multiple sources including other sysadmins and our VAR that the VXRail project is painful and really unenjoyable to support.
16.json,t3_8y3hgz,t1_e28711y,2018-07-12 08:40:50,What HCI solution did you have? If you don’t mind me asking.
16.json,t3_8y3hgz,t1_e2873ab,2018-07-12 08:41:52,"VXRail is vanilla VSAN with a deployment orchestration layer, vmware data protection, and recoverpoint for VMs bolted on the top.Personally unless you plan on using the whole stack I don't think its worth it compared to vanilla VSAN on ReadyNodes(which i quite like).At times vxrail has also been 9 months behind in support to current stable of esxi as well so if you need urgent patches due to vsan stability/drivers you had to wait or run unsupported by Dell/EMC.Vxrail is purely a Frankenstein product that exists to compete in competetive tenders against Nutanix."
16.json,t3_8y3hgz,t1_e29s48g,2018-07-13 02:48:50,Take a closer look at HC3 from Scale Computing. It is made for the size and cost you are talking about
16.json,t3_8y3hgz,t1_e4moxua,2018-08-22 21:27:13,"I am also looking at vxrail, simplivity and ucs mini.  Is there a single point of failure in the vxrail as far as you know?"
17.json,t3_ahs9z0,Does VxRail actually add anything to vSAN?,2019-01-20 08:32:04,"Those of you that have deployed a VxRail in your environments, has VxRail added anything of substance? Currently troubleshooting some unrelated VM issues but I came to the conclusion that VxRail doesn't add anything to vSAN. It appears to be fancy dashboards with proactive monitoring from EMC if a host has an issue (which is nice but not sure if it's worth the premium) but aside from that, I don't see anything added to vSAN or increased performance. Am I wrong here? Or those you that have mature VxRail deployments, is there some feature I'm missing?"
17.json,t3_ahs9z0,t1_eehkpj3,2019-01-20 08:49:39,"In the beginning, because everyone has basically all but come to expect hypervisors to ""just work"" - regardless of hardware/firmware/drivers/etc - HCI was problematic - because storage is not something you can fuck around with, and expect great results. Remember Storage P1's are the best P1's.Shitty drivers, firmware, hardware, network cards, hba's, etc - can all have significant affects on storage throughput and latency. SAN's might still just be an x86 box with some software and a shiny badge on it - but most of the value in the SAN is in this validation and reliability package.VXRail was put together because Integrators, Admins and architects basically sucked at it (largely because everyone stopped caring about HCL's - because ""it just worked""). VXRail was/is a tested and validated combination of components that should lead to a reproducible result that functions under real world load with a high degree of availability.VMWare has helped here - by providing a very very broad and reasonably complete HCL. Other vendors have also stepped up to the plate via VSAN/S2D readynodes or XC Core. Again, all tested and validated hardware for Software Defined Storage platforms. This reduces the value proposition of VXRail - however they still have an advantage by virtue of Rail Manager - which handles the full stack update process (again to tested and validated configurations of a release certification matrix).You aren't getting increased performance when you buy rail. You are getting increased efficiency - because  you to don't have to cross reference every component in your HCI stack (Bios, Firmware, Hardware, ESXi build, VSAN version, etc) with eachother to find a combo that works - then update them all individually. Rail Manager does this.Now its true in vCenter 6.7u1 this functionality is moving to VUM - but it requires vendor support (Dell 14G which presents REST API access for iDrac's etc will assist here), and its still a big problem.imo - Rail is worth a premium over a ready node. But its not a crazy huge one as Dell etc have been selling in the past.Remember, your boss doesn't pay you to update servers or review release certification matrixes. They pay you to support and implement business applications. Hardware/software updates of infrastructure is a component of that - but its not the primary driver. Its a necessity borne of owning/managing your own hardware. This is a large component of the value proposition of the cloud. If i dont have to spend hours a week/month/whatever paying you to update shit - and the outages or decreased resiliency during that period - that is an efficiency gained."
17.json,t3_ahs9z0,t1_eehmssh,2019-01-20 09:10:55,"I agree here. Our first steps in to HCI ended in poor results and generally left us with a bad taste for it. We have the mentality that any device should do 1 thing and it should do it very well. When you start asking devices to host compute and storage at the same time, that invites trouble.One lesson we learned after our attempts at HCI was that having a 1GB internal backbone and 1GB interfaces to storage is not enough. So we started upgrade to 10GB interfaces where we could and decommissioned what we didnt need.After we deployed VxRail we didn't see a huge difference in compute or storage performance. At a minimum, we got performance comparable to our hybrid arrays (mix SSD/HDD) and under the best conditions we got performance comparable to our high performance arrays (all SSD).But you're right, I haven't had to think about driver upgrades. If anything EMC has reached out to me to schedule an upgrade to the VxRail so that's the proactive front. I do enjoy how the nodes will all phone home to EMC if they are having an issue and a ticket is auto-generated and assigned before I'm alerted to there actually being an issue.I think VMware should adopt Apple's hardware certification process. Where a given vendor (networking, processor, backplane, etc) has to have their hardware certified by VMware before it can be used in a virtual environment. The whole nebulous HCL invites room to have issues with drivers, firmware, etc."
17.json,t3_ahs9z0,t1_eehnsbc,2019-01-20 09:21:18,"Have Gen 1 (previously VSpex Blue) and VxRail Gen 2 nodes. Our opinion is the addition of VxRail to VMware and VSAN added negative value to the deployment. We’d be much happier with VMware and VSAN without VxRail running in hardware from the VMware HCL.Plus we’ve found VSAN performance, even on all flash nodes, to be utter crap for write intensive operations. So instead of VxRail replacing our legacy SAN we had to renew support on the SAN and buy a new SAN with tiered storage.When we looked at renewing support for the Gen 1 hardware for year 4, which came after the new SAN purchase, we decided we would come out ahead by sun setting VxRail and going with standard Dell 1Us for compute and using our new SAN or local host storage without a VSAN depending on application. Plus we’ll be dumping a lot of VMware license cost for XenServer to run our Citrix VDI workloads.Decommissioning of 5 VxRail appliances will be done by the end of Q1, with another 5 that will be decommissioned 18-24 months after that.For us, HCI didn’t bring the value, and EMC’s VxRail support had been a poor experience.For us, a good SAN, fiber channel, and standard Dell compute where we can replace the hardware every three years for less than years 4 and 5 of VxRail support is bringing the value.The current generation of VxRail is built on Dell hardware, which is definitely going to be much better compared to the crap EMC built Gen1 and Gen2 off of, but VxRail support still sucks so you may as well buy the same Dell chassis and blades, get yourself the needed switch infrastructure, and lean on VMware for hypervisor/VSAN support, Dell for the compute support, and Cisco for network support. Individually these companies have superior support to Dell/EMC’s VxRail support."
17.json,t3_ahs9z0,t1_eehp03t,2019-01-20 09:34:38,"I think a large component of this is that VSAN only just got RDMA support - and that when people have unique or demanding workloads, care isn't taken into just how much work the host has to do to serve the storage component of its workload.This is as much sizing as it is anything. But every Rail node i've ever seen came with cards that don't support RDMA.Also - if you need heavy writes - SSD choice (and now NVME/Optane/pMEM) will assist here."
17.json,t3_ahs9z0,t1_eehq9vk,2019-01-20 09:48:35,"It may or may not have been a mistake (so far it's worked out for us) but we went with all Dell for everything when we deployed the VxRail. Dell e460s (which if what I'm told is true, are rebranded PE R640s) with Dell S series 10Gb Fiber switches. All that feeds directly into our existing Cisco infrastructure.We've been lucky with EMC support. We absorbed a first generation VNXe and while it was under support, EMC was responsive and provided relatively good support. For the VxRail, it's been a similar experience. It could be our industry (state gov) that is the reason why we've been lucky"
17.json,t3_ahs9z0,t1_eeiffvo,2019-01-20 15:22:53,[deleted]
17.json,t3_ahs9z0,t1_eeilr85,2019-01-20 17:21:58,"Absolutely. We're a small team in a medium sized company, and the (off-business) manhours required on deploying and maintaining the various components of a traditional setup are a huge drain.As VxRail moves all of that individual planning, testing, upgrading, configuring... into a single package, the efficiency of maintaining our infrastructure is much increased, offsetting the higher initial procurement.As a disadvantage, it locks you in with DellEMC and their sometimes abysmal support. The theory is that with VxRail, you get to call a single support contact and avoid the vendor blame game since everything goes through them. In practice, the support for something that they advertise as ""mission critical workload ready"" isn't really up to what we'd like."
17.json,t3_ahs9z0,t1_eeonihi,2019-01-22 19:37:06,"I have 0 faith in VMware's HCL after being burnt by the Intel X710 multiple times. Its drivers are very problematic, and there were many months (~October 2017 - May 2018) when the card was being sold, on the VMware HCL, but no driver which didn't either result in PSODs or just randomly stopping traffic en masse. In May 2018 the patched version 1.7 came out, for ESXi 6.7 only.To this day, even the 6.5U2 Driver rollup installer includes a broken driver (1.5.X), which is still on the goddamn HCL even though it's widely known it's terribly broken since more than a bloody year (again, ~October 2017). Got burned by that again last week since i just stupidly assumed the 6.5U2 Driver rollup installer should include the latest drivers (the 6.7U1 one did include 1.7.11) and didn't bother checking it - not a day later one of the ports stopped working.https://www.vmware.com/resources/compatibility/detail.php?deviceCategory=io&productid=37976&deviceCategory=io&details=1&partner=46&releases=408&keyword=x710&page=1&display_interval=10&sortColumn=Partner&sortOrder=Aschttps://www.vmware.com/resources/compatibility/detail.php?deviceCategory=io&productid=37976&deviceCategory=io&details=1&partner=46&releases=427&keyword=x710&page=1&display_interval=10&sortColumn=Partner&sortOrder=Asc(proof that i40en 1.4, 1.5 is still on the HCL)"
17.json,t3_ahs9z0,t1_eokeus6,2019-05-24 07:10:06,You work for a VAR don't you...
17.json,t3_ahs9z0,t1_eokf39l,2019-05-24 07:12:44,"I've sold probably over 100 nodes of VxRAIL and customers are loving it. We have had some issues where support hasn't been to responsive but eventually have come through. I think just the superpackage update is worth it, who cares if this driver works with that one or if the BIOS is tuned correctly. Now with VxRAIL on VCF, game changer. Ping me if you want to see it in action."
18.json,t3_az3f8z,new vxRail deployment questions...,2019-03-09 21:19:43,"Ill try to make this as short as I can... Our company recently purchased vxRail in 2 location, one is to be US based and the other in Europe. Both were procured through a large reseller but we dealt with Dell EMC sales from the jump.  The sales process was a bit of a rough ride and the sales lead ended up leaving almost immediately after purchase (coincidence I guess). More to the point, we were always harping on the fact that the vxRail was to replace our current aging 'rack n stack' infrastructure. Our existing setup is not complex and very straight forward.  However, since it is in Production we told Dell team it was imperative to have a solid/flawless deployment which should emphasize the migration strategy to get us off the existing and onto the vxRails. Come to pass, after several calls with the new sales guy and tech, we are not happy - namely on the US side. Question: is it normal that their ProDeploy Install consists of nothing more than a remote webex screen share to install the vxRails?  Normally, I couldnt care less but to charge a customer $6k plus for a 'Pro' install and then to find it is done over a 3-4 hour webex is a bit outrageous, especially if you break down the cost to an hourly basis. We have yet to do the install to date, but the other looming issue is, they are seemingly reneging on their promise to assist with a migration strategy. In lieu of a true migration strategy, they will conduct a simple call to discuss examples on how to proceed with the migration. Side note: there was also a 'rack n stack' component line item where techs came on site to rack the stuff and get the iDrac working et. They left before forgetting to do specific tasks - like, setting authentication (username/passwords on switches). So, of course we have even less of a warm feeling about the next step , which is to install the actual vxRail. Now, before anyone jumps all over this to tell me I am being a baby and I should have gotten things in writing, or I should not expect migration strategy assistance.  The implementation in Europe based off of the the exact same quote /purchase and they received a tech on-site to perform the whole ball of wax  (rack n stack + install w/ assist for migration - which is ultimately what we expected and desire on US side as well. Simple environment or not, the intricacies and oddities which make up the vxRail components and having heard of disasters where things can go South quickly, if not installed or handled correctly has us worried.  We want a tech on site and expect one,especially for the cost involved here both for the install line items as well as the overall cost of the purchase itself.  We cannot really afford any down time and while we have a sense of how to conduct the migration, we do not think it's wise to do so without someone experienced with vxRail from Dell at the helm, or at least in the vicinity in case things go wonky, so they have direct line of sight to other Dell engineers.  However, despite all this, we are still being told that it is done remotely and that migration is totally separate basically. Is this a normal experience for all this to be done remotely and has anyone else had similar experiences?  I didnt think there should be such a disparity between how things w/in Dell would be handled depending on region.  Are we delusional in thinking they should at the very least be doing more than simply clicking through the setup wizard and "
18.json,t3_az3f8z,t1_ei526zx,2019-03-09 22:29:22,"Yes. Super common and they don’t have but a handful of installers anyway. Most just work from home at the behest of your assigned project manager.The dirty secret is that VXrail isn’t hard to install at all. It’s mostly wizard based and if you aren’t doing anything fancy (metro cluster, etc) you could probably knock it out in a day if you have an admin who knows basic VMware.Migration assistance is not included but you can try to get at least the vswitch setup included in the statement of work so you can be ready to start migrations soon after the cluster is up....IF that’s how you are going to migrate. There’s 8 or so different ways to do a migration (backup/restore, replication, vmotion) but you have to figure out what will work for your downtime requirements.Watch this video on migration."
18.json,t3_az3f8z,t1_ei53k7u,2019-03-09 22:50:30,"VMware employee here. For the migration aspect, what is the vCenter architecture being deployed. Will the VxRails end up on your existing vCenter, or on a vCenter in enhanced link mode with your existing vCenter? That makes migration the easiest if they are in the same location, just shared nothing vMotion over. Now your old and new vMotion IPs need to be able to talk to each other.If not in the same SSO domain there is a fling to do cross sso shared nothing vMotion. Still need vMotion IPs to be able to talk to each other.HCX does cross everything replication and vMotion, but the target needs to be licensed with NSX Ent +."
18.json,t3_az3f8z,t1_ei55ay4,2019-03-09 23:15:38,Thanks for your reply and insight. I will take a look at this video as you say. Glad to hear I am not just getting screwed and instead fwiw this is normal action from their side.
18.json,t3_az3f8z,t1_ei55rob,2019-03-09 23:21:58,"Hi, thanks for your reply.Existing architecture has vcsa 6.5, esxi hosts are 5.5 unfortunately. We will deploy new self-contained/dedicated vcenter for vxRail as were told was recommended setup and fully supported.For migration, we'd like to do it live vmotion if possible. Have discussed linking existing ToR switches to the new Dell ToR switches which were purchased via 10G trunk. Then, we were told we could check something similar to this as a best option to live migration:  https://labs.vmware.com/flings/cross-vcenter-workload-migration-utility . Pre-req was to g at least 1 of the existing hosts to 6.0U3 as required by this method.All vmotion &other vlans should be able to communicate, as we are simply re-using much of the same subnets for old/new environment.the only other alternative I was told was to have downtime and simply setup iSCSI connection via 10G to our current array & new vxRail environment and storage vmotion all our stuff off the array to vxRail."
18.json,t3_az3f8z,t1_ei5h0mj,2019-03-10 01:40:19,"I had a similar experience where we purchased, based on the same quote, in UK/EU and LATAM and both had the install option but they didn’t send an engineer for the EU install.I just finished our third VxRail deployment and actually insisted they do not send an engineer this time since I’m comfortable doing it myself. We still paid for it so it at least I had remote support available.For migration they couldn’t really help, but suggested vCenter Converter. Since this site only had like 50VMs. I thought that would be fine.vMotion would be preferred I guess if you have a big environment."
18.json,t3_az3f8z,t1_ei5lxdr,2019-03-10 02:39:08,"Thanks for your input, seems there are a lot of discrepancies w/ implementation et depending on where this gets implemented .  We were also given the Converter as an option but  cant do downtime unfortunately else would be just as easy. I need some migration prep not the whole migration soup to nuts obviously.   We just would feel more comfortable having what we anticipated, which was a tech on site to do all this given whats at stake and the costs already allocated.  To me doesnt seem too much to ask, but curious if you think should just proceed with remote option they are giving or if its a waste of effort to try and demand a tech onsite?"
18.json,t3_az3f8z,t1_ei75yjn,2019-03-10 17:53:15,We had a tech come onsite for install and switches configured remotely.  Actually node setup was done remotely and we were left with a blank Vcenter.  Migration was left to us and we ended up using Veeam to replicate.   Quick down time for final rep and a reboot. We only had 200 vms
18.json,t3_az3f8z,t1_eiecaj9,2019-03-13 09:16:54,"Thanks for your response. I am thinking of actually going the vCenter Converter route as opposed to what I posted above regarding the ""fling"" method. The reason is to use the fling method, I would need to disturb my current production environment a bit (upgrade 1 host to proper version , remove some existing cabling to be re-used) et. Therefore, I wanted to ask how well your migration went by simply using vCenter Converter ? Our environment is also not very large around 65 VMs total. Also, did you use 1G or 10G to migrate and did the actual transfer/copy  process take very long? Seems the only downtime is for spin up of VM once migrated to vxRail and VMware tools install, correct?  thanks!"
18.json,t3_az3f8z,t1_ej4xo2y,2019-03-23 02:22:47,"Vxrail is a joke. Your paying double if not triple the cost of the normal dell server just so it can say vxrail. The installers are just working off a script. If they hit a snag, they're lost. They don't really like to work with your environment, they want to do it their way and bitch if you want to deviate from that. And do some research on vSan, lots of horror stories about corruption and data lose. I fight with sales to just pitch sans and servers but they want to sell what makes them the most commission and disregard the advice of the engineers. Also they push them because Dell is pushing them to."
18.json,t3_az3f8z,t1_el0khoi,2019-04-16 20:56:53,"Ok, so while I can agree that the vxRail hardware is nothing more than a glorified PowerEdge server setup, I would think that it goes without saying, that you are not so much paying for the hardware as you are the software which drives the entire thing - that is where the so called 'magic' lies and where I would think the cost is as well.  I do agree it is overpriced, but to say it is a joke - well I cannot comment as I haven't used it enough yet.   Are there better products out there? Sure.  You are 150% correct about w/ your comment: 'they go off a script and if they hit a snag they are lost"".  Very true. This happened for us, cost us almost 1.5 weeks delay as they ran in circles.So far, approximately 2 months in and it has run very well.   For us, the real issues were not in the system itself (at least not yet) but in the Project Management & to some degree the Support side.   For those of you who have ever dealt with Cisco support...I'll say this, Dell Support ...is NOT Cisco support lol.  It is usually hit or miss with Dell Support and there are a lot of 'qwerks' with this system that Support themselves seem to be almost figuring out as they go along.  I think that is a result of 2 things - 1) the system is quite new to a lot of their staff and they are still getting ramped , and , 2) the recent merger/acquisition between Dell/EMC et that happened caused a lot of otherwise competent and knowledgeable staff to either be let go or be sacrificed for less competent 'greener' staff.In summary, if you are planning to purchase vxRail, I will say this, it is a beast and runs very fast/well - at least for our environment.  The cost is high, but so are all the alternatives in this space that we explored and for better or worse, we are satisfied thus far with the vxRail system.   Do NOT expect them to do ANYTHING but click through the xml script that they pre-bake to deploy your system. They will NOT do squat to help with any migration - I suppose unless you pay handsomely some extra coin as they blow this process out of proportion in so far as complexity is concerned. Make such a big deal out of this its not even funny - it's like we bought this system to replace our current system - why would you not think we would need to migrate?  Granted some environments are more complex and I get it, but they offer 0 help - at least in our case and were told they would help more when trying to get their sale initially - so just beware is all.To reiterate, (in case it wasn't already clear :) ) ...we did not appreciate anything and I mean anything to do with the deployment - it sucked and was driven by incompetent Project Management who know nothing about technology or the product itself and of course are pretty much just sales guys driven by %'s and #'s.   The deployment techs themselves are not bad people and to be fair,  all they want to do is help and get it done and could, if they were simply allowed to manage the deployment themselves.  However, once the so called ""Project Managers"", who have 0 tech expertise and dont know d!ck about the solution, start meddling in the deployment tasks, then $hit goes sideways and things do not get done and all we do is have meetings.   Left alone with the tech to run n gun, it was no issue, we bang it out boom and done.  Perhaps just our experience."
19.json,t3_c656vk,VSAN dedup and in guest dedup,2019-06-27 20:56:11,I wasn't able to find much on this when googling. We are deploying a VxRail all flash VSAN cluster and have dedup and compression enabled. We have some windows file servers that will be migrated onto the new deduped cluster. These windows server have in guest dedup enabled. Do I need to disable the in guest dedup and rehydrate the data before doing a storage vmotion to send the disks to the new VSAN deduped cluster?
19.json,t3_c656vk,t1_es683w6,2019-06-27 21:43:45,"You'd certainly get better performance numbers overall, and better dedupe and compression ratios out of vSAN if you aren't running dedupe in-guest."
19.json,t3_c656vk,t1_es6b5a1,2019-06-27 22:19:52,This is true. Dedupe in guest would make that data undedupable for the cluster and because dedupe of vxrail happens over the cluster that data Will be at a 2:1 instead of a 25:1 (for example)
19.json,t3_c656vk,t1_es6bu3c,2019-06-27 22:27:44,That is what I was thinking. I just wanted to be sure before I started moving stuff around.
19.json,t3_c656vk,t1_es6etur,2019-06-27 23:01:20,"You’re talking about file level dedup vs block level dedup. Host based file dedup will be invisible to vSAN. DDC on vSAN will be invisible to the host.The question should be about your performance overhead of having both enabled. Dedup ratios are over-rated. Overall space savings with multiple compaction technologies matters. Performance matters. If I have a 2:1 dedup, compression, thin, trim or unmap ratio on 1PB of data that is significant savings. If I have 10:1 on 400GB, it’s almost useless to me. EDIT: Always run in a test lab before running in prod."
19.json,t3_c656vk,t1_es6pcda,2019-06-28 00:53:21,Another benefit of leaving it uncompressed is better use of the vSAN cache for ‘hot’ data. You would not get as big an efficiency if you left the guest compression / dedupe enabled.
2.json,t3_9zlpsf,VxRail No Link Aggregation??,2018-11-23 14:05:43,"Can anyone explain why VxRail says that all LAGs aren't allowed in the deployment guide? I totally understand them not being allowed at initial deployment, but why would that remain a requirement beyond initial install. It seems like once the solution is deployed via vxrail manager that I could go modify the switches to use lacp, modify port groups, and continue about my day like I followed the guide. Can anyone think of any compelling reason why this won't work?"
2.json,t3_9zlpsf,t1_eaab1pv,2018-11-23 16:20:43,"I am pretty sure the issue comes during node replacements, Satadom/boss replacements where the node is RASRd.  It records a lot of the network config in the mystic DB and will most like give spurious errors when you try to add a node back in.That said I haven’t tried it but have experienced all sorts of fun trying to get nodes back in to a vxrail cluster."
2.json,t3_9zlpsf,t1_eaabbc6,2018-11-23 16:26:15,Why would you want to use a LAG?
2.json,t3_9zlpsf,t1_eaabe3d,2018-11-23 16:27:49,Why do you want to do LACP. First off for my reasoning to not do LACP from a host perspective is because LACP is controlled by the vmkernel. If anything were to go wrong that side you will drop comms to not just the management network but also the vms. I’d prefer the route based on physical load.Going back to the question. I have modified the policy successfully to my option without a hitch.Good luck there
2.json,t3_9zlpsf,t1_eaajqfs,2018-11-23 19:09:02,"It’s not implemented because it’s a mom and pop solution meant for stores that don’t use aggregation on their upstream switches.If you have vPC or MLAG on your access/TOR switches it’s obvious you would want to use LACP because it’s far superior to active/standby, but the assumption from Dell is that most people will have standalone switches."
2.json,t3_9zlpsf,t1_eaana5e,2018-11-23 20:37:45,"VxRail is supposed to be simple, both to implement and to run.Generally speaking, LAGs are not simple compared to host-managed policies like Route Based on Physical NIC Load.Can we set LAGs up? Sure. Is it worth it? Not in any use-case I've seen (VxRail or not).Ever tried troubleshooting network latency issues caused by 1 of 4 physical LAG member links being saturated?"
2.json,t3_9zlpsf,t1_eaargzw,2018-11-23 22:09:24,"The Mystic and Marvin databases of the VxRail Manager contain only information about the management relevant portgroups and the dvSwitch. That includes the name of the portgroup, the moRef ID, VLAN ID and some other VxRail related information. So only rudimentary information about the vMotion, vSAN, Marvin, ESXi Management, vCenter Network and additionally the VLAN-ID for NSX Networks are stored. The VxRail Manager therefore knows nothing about the VM networks. Basically you could change the Teaming policy for individual VM network portgroups without having an impact on the VxRail setup as such, RASR or cluster expansions.During a RASR or cluster expansion, the node only needs to know which vmkernel ports to configure and which dvSwitch to connect to. The VM networks are then inherited from the dvSwitch.LACP with the vm network portgroups works without problems, because I tested it myself several times.However, this is not supported by Dell EMC. For the reasons already mentioned. VxRails are a completely standardized solution and Dell EMC does not want to cover all possible setups, but concentrates on a standard setup that covers 90% of all customer needs."
2.json,t3_9zlpsf,t1_eaask0i,2018-11-23 22:29:55,
2.json,t3_9zlpsf,t1_eaasnyf,2018-11-23 22:31:59,"Not that I’m a huge fan of LAGs for management networks, but couldn’t you backup Host Profiles that configure a LAG, and a vDS backup to the database and restore from that?Edit Host profiles doesn’t support LACP."
2.json,t3_9zlpsf,t1_eaasqya,2018-11-23 22:33:34,If you use a 3 tier network design and have access switches rather than leaf switches others would call you a mom and pop shop.... networking design  turns into a religious war way too quickly.LACP isn’t always superior. 1+1 doesn’t always equal 2 (especially with a basic hash).  Stacked switches can fail as a stack (I’ve seen it) and for high uptime environments Many don’t trust LACP to anything that isn’t a chassis (or fancy ECMP fabric but layer 2 ECMP is falling out of favor).
2.json,t3_9zlpsf,t1_eaat3p0,2018-11-23 22:39:48,I’ve seen a fair Amount if support cases to broken vPCs and mismatches hashes. It’s a bit like jumbos. It’s either marginally helpful or it breaks everything.
2.json,t3_9zlpsf,t1_eaauuiw,2018-11-23 23:09:28,"Ah, but it isn't!"
2.json,t3_9zlpsf,t1_eaaxs84,2018-11-23 23:55:37,"So my take away from this is that they aren't supported and will only cause issue if I ever want to modify the cluster (add/remove/rejoin nodes).I have two TOR switches with 4 10GBE connections and wanted to run MCLAG with lacp. It just seems like a waste of available resources to run active/standby, especially when all the features are available in a standard deployment."
2.json,t3_9zlpsf,t1_eabdtjf,2018-11-24 03:24:51,"It’s an engineered solution. If you want to be mainstream with the majority of configs which is the value, follow the config guidelines. If your requirements truly dictate that you operate outside of the variable reducing guardrails of an engineered solution (in this case IMO, they don’t) then use ready nodes.Prove that you’ll use >10Gbps per host on average, otherwise the entire point of waste is shot no matter the teaming policy."
2.json,t3_9zlpsf,t1_eai8hop,2018-11-26 23:34:31,"It's not really a waste, each of your four 10Gb ports is being used for traffic.The main time LACP is useful is when you have a server that needs to provide more than 10Gb total spread across many clients.LACP was a hot feature when everyone ran gig on 4.1/5.0, then when it was released it was a little used feature since everyone went to 10Gb and didn't really need it."
20.json,t3_9oydgs,OSPF process restart - VDX6740t,2018-10-17 20:51:36,"Hello All, Background: We have a MPLS circuit at a partner in the Philippines, connecting to a Cisco 2911.  We receive the BGP routes on the 2911, and redistribute them into OSPF and sent to a VDX6740T switch.  That switch is in a logical chassis setup with a second VDX6740T.  Both 6740's are running NOS 7.0.2b.  The Cisco 2911 is running IOS 15.7(3)M3.  The Brocade logical cluster is being used as a core switch.  It was purchased for a vxrail implementation, and a decision was made (not by me - I just have to support their decisions) that since it can do routing, we were to make it the core switch as well.  Other details on these switches:  each switch only has a 24-port non-40GB license, with hardware maintenance only.  We placed them in a logical chassis;  replacing a Dell PowerConnect 6224 to handle the routing between the various vlans at the office, to both the internet connection and the MPLS connection.  A few months ago during the cutover to Extreme, I was able to convince someone in tech support to give us the 7.0.2 software; the patches are showing up under my account.  Also, these two Brocades are the only ones in use within the company - everywhere else is running Dell, Cisco, and HP for switching and routing. The problem: This works normally, except if, for some reason, the MPLS circuit goes down.  Once the circuit is back online, what happens is we stop receiving ospf routes from the 2911.  I have verified on the Cisco that it is receiving the BGP routes from the other sites in my company.  I have also done the following troubleshooting steps in the past to try to restart OSPF on the brocades: Question: Now, I was told in passing that the software we were using previously to this past weekend, 5.0.1d, had issues with OSPF.  We had a switch fail due to flash corruption during a reboot.  However, this gave me the opportunity to ugprade to 7.0.2b since we had to take an outage at the office to replace it.  When we got everything back up and in place, however, we tried running the ospf reset again.  once again, the processes would not restart.  Is there a better procedure to fix this, some troubleshooting steps, or is the solution always to get a maintenance window & reboot the 6740's?"
20.json,t3_9oydgs,t1_e8enlzb,2018-10-25 11:48:01,Buy another Cisco 2911 from eBay
21.json,t3_7d8py8,Is an RF3 (5 node) all flash 10TB VxRail better than an RF2 (3 node) Hybrid storage 4TB Nutanix?,2017-11-16 08:50:42,"As of now this is what I am faced with, both coming in at the same price $185k.  I have read a number of negative comments towards VxRail but for the price and everything Dell offers I feel I need to take a closer look or maybe just go vsan?.  I would like to use Hyper-V which I can do on Nutanix so I can user Hyper-V replica to another host, but I can also use vmware and recoverpoint. Otherwise, I may just go Nimble all flash and a cluster for my 10 VMs."
21.json,t3_7d8py8,t1_dpw5ejr,2017-11-16 11:56:07,"I manage something like 8 vxrail clusters.  Based on what you said, the vxrails are a much better deal.  5 nodes, all flash, for 185k?  Unless the processors and ram are really low end, that will be a much better cluster.   Again, assuming the 185k is for a total, turnkey solution for both, you are getting a lot more bang for you buck with the vxrail.  That said, I've played with nutanix before in a PoC, and it is a slick system.I think there might be something off about your pricing...I got some quotes for equivalent vxrail/nutanix clusters, and nutanix was a good bit cheaper for more hardware if I remember correctly.  If the nutanix uses hybrid storage, and only has 3 nodes, it should be significantly cheaper."
21.json,t3_7d8py8,t1_dpw6nos,2017-11-16 12:22:59,"You have 10 VMs. You could run that off of anything. You shouldn’t be looking at hyperconvered, there is no value for you there unless your growth is going to suddenly explode.For 10 VMs I wouldn’t look at anything VSAN. Even shared storage is a stretch.I would give serious consideration to just get a couple 2U servers loaded with disk and run Hyper-V replicas do you have a hot/cold failover scenario.Worry about things that scale up when you actually need to scale up. I’ve got a Dell 2U running about 40 VMs. It has a all SSD RAID10, and an all HDD RAID10. Happy as can be.Then there is my Dell blades with SAN storage, running all manner of critical VMs. Works great. Will be getting a bunch of 1U compute and new SANs in the future. Solid, reliable.Then there is VxRail. Biggest fucking pile of shit I’ve ever had to deal with in 20 years in IT.2Us with local storage with replication. NAS device to be a backup target for the backup product of your choice.If you want the new shiny thing that is hyperconverged you really need more than 10 VMs to justify it."
21.json,t3_7d8py8,t1_dpwjnui,2017-11-16 19:51:35,"thanks.  I tried replica and told the owners I could replicate the  VM every 30 seconds.  They were not too happy to hear that. With 3 locations relying on the app they wanted a solution with no loss of data.  So, while I realize it's not many VMs, they want something better than replica and are willing to pay for it to a limit.  That limit so far being $200k.  Could you tell me what kind of SAN you use?  I keep hearing Nimble and Pure.  Someone mentioned Tintri too."
21.json,t3_7d8py8,t1_dpwk2hn,2017-11-16 20:06:29,"Hi, not sure why the price is so high, not even using Dell hadware and things like UPS missing.  I will go back to them and push for something better or I may just do a cluster.  Can you share your experience with vxrail? any issues? happy?  thanksEach VX node is dual proc E5-2620 v4 8C 2.1GHZ (10 CPU) 80 cores 640GB RAM Each Nutanix node dual proc is E5-2643 v4 6C 3.4Ghz (6 CPU) 36 cores total 512GB RAM"
22.json,t3_86kywt,Horizon 7.4 unknown (missing) instant clones,2018-03-23 22:19:17,"Anyone having  issues where  new instant clones are going unknown (missing)? The new desktops are in vcenter but not available in horizon. Restarting the connection will make the available, but this keeps happening after a couple of hours. Horizon 7.4 VXrail 4.5 with external vcenter NSX"
22.json,t3_86kywt,t1_dx995h7,2018-04-13 02:37:29,"Hi! I get the same issue, I can see it on instant and also on likes clones. Looks for me like a bug in version 7.4. I’ve opened a SR. Do you have found a solution in the meantime? BR"
22.json,t3_86kywt,t1_dx9rdac,2018-04-13 07:20:03,I had an SR opened for about 2 months and about 3 weeks ago the told me they are aware of the issue and gave me debug version to install. The debug version solved my issue and now they are creating an H-patch version for me to use. I asked if the fix would be in the 7.5 release and tech couldn’t say for sure.
22.json,t3_86kywt,t1_e0tx5sa,2018-06-18 02:33:21,I observed this issue with 7.4 GA Easily way to solve it - reboot vcsa and connection server and it all shows VM available after comes upThis worked for me
23.json,t3_43m09o,'VxRail' looks like EMC's next-gen hyper-converged appliance,2016-02-01 09:05:57,[]
24.json,t3_475gzu,"EMC, VMware introduce hyper-converged VCE VxRail Appliance family",2016-02-23 14:57:01,[]
25.json,t3_4p9yju,VCE VxRail Appliances customer adoption exceeds expectations,2016-06-22 18:17:56,[]
26.json,t3_97lhgl,Clients who don't know what they want....,2018-08-16 03:58:20,"So i recently took on a client associated with the Department of Energy. Heart of America type client that the story is always the same. Bought out multiple times, went from a Mom & Pop to a Global Corporate Player in the span of 10 years. Like EVERY SINGLE CLIENT i take on, their environment is a complete and utter disaster. They have 12 remote sites, 2 of which are in Datacenters, if you can call a broom closet of a datacenter anything other than that. All of their ""sites"" are in various states of disrepair or caustic functionality. Essentially, each site is a shit hole. I am brought into the picture as a VMWare and Horizon View Subject Matter Expert. Recruiter is someone i worked with at a much larger agency who is now in a Boutique shop with 3 or 4 other guys. Tells me they are trying to ""Do IT better""... OK... But their rate is where i need it to be and he can do Corp2Corp. I walk the client through their network via Skype and decide that i need to do an onsite meet and greet with the client which is typical for me. I spent 3-4 days onsite, get to know the client, take a look at their infrastructure and go over plans. Those of you that know me know that essentially I am a workoholic. When I'm engaged, typically ill do 60, 70, 80 hour weeks if thats whats needed to get the job done. The client agrees that being that all of IT is new (Read like 2 weeks new) that they do not have the time or the expertise to do what needs to be done and they need someone aggressive. OK. I sign off on that. I agree to assess, plan, design, and implement a Infrastructure that's not only redundant but secure, and on their time table. They want everything done in 6 weeks after the assessment and design phase. 2 Weeks roll by and assessment is in, client signs off on time table and whats needed and we order 1.2 million in hardware and licensing for their 12 sites. VXRail for the DataCenters, Dell R730's for each of the satellite sites with Tegile San Storage. I start implementing. Now this is time consuming stuff. I have been flying around for two weeks now installing, migrating, and building each of these sites. I bill corp2corp. So the client doesn't see the first of the invoices till the beginning of August. This morning i get an email from said client. They freak the fuck out about the cost of implementation. Keep in mind, we went over in detail and Im exactly where i predicted it would be cost wise. Actually we are under since I saved them about $600k in hardware with a month end deal with Dell on the VXRail's and Servers. They discounted a shit ton. Client wants a meeting to discuss what the costs will be, on a daily basis and wants me to discount my hourly. Uhh No. Sorry I won't be doing that. Recruiter has my back but says that the job may get cancelled due to costs involved.... Essentially, they now want someone to do the work i do at half my hourly rate. I plan to tell them sorry. Given their timeline, you get what you pay for and what you asked for was an expert. Thats my rant for today."
26.json,t3_97lhgl,t1_e49658m,2018-08-16 04:51:39,Lots of I and not much we...  So you lack parallelism?  Maybe they picked up on that.
26.json,t3_97lhgl,t1_e496euq,2018-08-16 04:55:27,I wouldn't say that. Communication is overabundant. The client just doesn't know what they really need or want. They are a foreign owned entity working in the US Government workspace. So there are alot of challenges in language already with them and they weren't prepared for the cost once it hit reality. I justify just about everything in a proposal and almost always hit my mark. Its just one of those things when you are billing 300+ an hour the bills get quite high.
26.json,t3_97lhgl,t1_e4989u3,2018-08-16 05:22:02,Do you have a contract for the project? All these things should have been signed off on before hand.
26.json,t3_97lhgl,t1_e498b0l,2018-08-16 05:22:30,"Of course, and they have. this is just how some clients like to act."
26.json,t3_97lhgl,t1_e49i76n,2018-08-16 07:59:42,"Clients who don't know what they want are the best kind, because they can be gently guided into getting what they should have, without having their expectations violated.It's the ones who know exactly what they want that are hard to please. Even if you do what they ask, it will take a lot longer than they expected and be more expensive. That's assuming that what they want is a good idea, and straightforward to deliver.It sounds like you have a client that may not have perfect communication between all responsible parties. Most likely you need to get the persons who signed off on your original plan in a room with the persons who just contacted you, and very calmly go over things with a smile and many reassurances that things are going exactly as planned so far."
26.json,t3_97lhgl,t1_e49r3t1,2018-08-16 10:24:36,"Did you keep a good track of the billable hours?  They can go after you by asking for an itemization of them.  I've seen things get ugly quick, best to be prepared in case they ask (if they have not already..).  Represent the value of the work that has been done, so the business can justify the cost.It is also equally important to keep clear communication and updates on hours occurred, especially if it's greater than the weekly agreed upon amount, mainly so they aren't hit with a large bill during their payment cycle.  Not entirely sure about your situation, just speaking from past experience in contracting."
26.json,t3_97lhgl,t1_e49uxcx,2018-08-16 11:31:16,Been doing this for 11 years now. I itemize every single detail including real-time billing
26.json,t3_97lhgl,t1_e49x5xj,2018-08-16 12:15:43,"Yeah, my company likes the ""we"" bullshit too, but the thing is I like being accurate. If I do something, I did it. That means *I* get the consequences, good or bad. Why does taking ownership of something you're doing constitute a negative? That's not even being a contractor, where it literally is you, the individual, they signed on.So they decide to spend a million dollars on gear and they want some McDonald's wage schmuck installing it? They're trying to score a freebie if you ask me."
26.json,t3_97lhgl,t1_e4a0rev,2018-08-16 13:38:33,"I love these types that's why I project charge with about 30% more time than usual just in case.Prepay too. 750k to retro fit a dozen offices with new servers, firewalls and pcs if I charge hourly? Fine 950k flat rate everything included. For some reason clients eat it up.Explain to my techs we normally charge the client 500k including meals and lodging but the client was military manufacturing, so we have to double check aka slow it down and on top of that I didn't want to argue with them about all our techs  eatting at the local steak House (fine Sarah, for you, all you can eat tofu shack) at$150 a pop a night per tech at 3k for food a week.Made extra 400k to guarantee a payment and no complaints. I'm surprised more techs don't do it.Good Christmas last year.Charged 15k in labor to install a UBNT airfiber across a river to two manufacturing plants. Cost 750 for the electrician to mount it, 600 in labor for my techs and it was presetup a head of time for 1 hour of labor.Client had no problem prepaying a fixed install."
26.json,t3_97lhgl,t1_e4abnb4,2018-08-16 19:25:42,"This is likely intentional, some clients will do this."
26.json,t3_97lhgl,t1_e4ach6b,2018-08-16 19:46:22,Now you know why their infrastructure was poorly-maintained.'They never wanted to pay for it.
26.json,t3_97lhgl,t1_e4axd4z,2018-08-17 01:18:07,What was your path to doing what you do? Your work sounds very interesting.
26.json,t3_97lhgl,t1_e4dhr5c,2018-08-18 04:43:56,https://www.reddit.com/r/sysadmin/comments/8b1g30/the_consultants_handbook_to_success/
27.json,t3_9jmkvl,"Am I Getting Fucked Friday, September 28th 2018",2018-09-28 20:36:58,"Brought to you by the / All questions welcome, keep in mind that there are of course more pieces to this IT puzzle we can dig out of the box Required Info for accurate answers: As always, PMs welcome with your questions any time, not just Fridays. Warning: This thread is neither vetted, nor approved by the reddit administration or /"
27.json,t3_9jmkvl,t1_e6sl7f5,2018-09-28 21:04:30,Cisco 1108 with security license and high speed ipsec license. Plus the rack mounts. Or an equivalent low end Cisco 4000 series. Im seeing 70 bucks for just a simple 2 bent pieces of metal and some screws!
27.json,t3_9jmkvl,t1_e6sljxg,2018-09-28 21:10:41,I know you don't like dell but just thought I would check am I getting Fucked?3 hosts - R740•	2 – xeon gold 5118 2.3 Gig •	128 gigs of memory per host •	120 g raid ssd for OS •	10 gig network for network – copper 10GBase-T •	Duel power •	Windows server 2016 data center •	Remote management interface2 - s4112 network switches 10 gig 12 port duel power – storage network1 – Dell EMC Unity 350 F •	10 - 2 tb ssd drives raided 5 (8+1) •	10 gig for network – copper 10GBase-Tall hardware 5 year next day pro supportall in $122 K
27.json,t3_9jmkvl,t1_e6slvqf,2018-09-28 21:16:20,"Well, there are no part numbers...But it's not hugely hard to figure out here.Since it's an all-flash array the prices look pretty good but Unity does not dedupe from my understanding, it's always just 'coming soon'.There really is no reason to buy any all-flash storage that isn't either Nimble or NetApp (barring serious edge cases).It looks like all the margin and discount is tied up in the fact you're planning to buy a lie.I'd say fucked."
27.json,t3_9jmkvl,t1_e6slyll,2018-09-28 21:17:41,"Dude, 1108s and small Cisco routers aren't gonna get big discounts, you're gonna get 20-30% off List and you're gonna have to take it.Unless you plan to buy a couple hundred of them.Edit: Short answer - fucked."
27.json,t3_9jmkvl,t1_e6sn5sx,2018-09-28 21:37:18,"I manage a small domain hosted on GoDaddy.  I've got about 12 students, each on a sub domain with separate Wordpress installations.   Several times over the past few weeks, all the sites will become unavailable.  Looking at the backend, the metrics show low to normal usage during this time.  Calling GoDaddy, they tell me I'm maxing out the resources for my hosting plan (even though the usage graphs show no spikes), and I need to buy the next tier hosting plan for about double what I'm paying now.  In the past, this same plan has handled triple the number of students doing the same thing with no issue.  We've operated this class in this way for at least five years on the same hosting plan.  The students are doing very basic Wordpress stuff learning how to use the platform, adding and editing pages, adding photos, changing colors, adding a few plugins.  There is no outside traffic to their pages other than the students editing.FWIW:  Paying for 2cpu, 1gb ram hosting.  Charts show we never exceed 20% CPU Usage, memory usage, I/O usage.So, is it them or me."
27.json,t3_9jmkvl,t1_e6sn8kb,2018-09-28 21:38:31,"This is not the right thread for your question, but it might get noticed here so you don't have to delete it."
27.json,t3_9jmkvl,t1_e6snmyg,2018-09-28 21:44:50,I’d separate the server and storage costs otherwise you mix em together it’s hard to see where you got the deal and where you got fucked.
27.json,t3_9jmkvl,t1_e6snzdx,2018-09-28 21:50:18,"I am currently getting quotes on:Dell R740 - xeon gold 5118 cpu, 32GB Memory, 120 GB SSD, GB NetworkDell R740 -  - xeon gold 5118 cpu, 32GB Memory, 120 GB SSD, 15TB Raid 5, GB Network,Another option I am trying to figure out is instead of purchasing the R740 above, looking at Azure Backup for 15TB of data. Blog, just literally using it for an offsite file backup."
27.json,t3_9jmkvl,t1_e6so4r5,2018-09-28 21:52:35,"Do Azure, prices for data that sits there will only go down."
27.json,t3_9jmkvl,t1_e6so67k,2018-09-28 21:53:12,"Aruba Wireless Gear (EDU please!):Total: $22,550"
27.json,t3_9jmkvl,t1_e6sofrn,2018-09-28 21:57:23,"The Azure Backup is exactly what I'm looking for!I'm waiting on Veeam to have native Blob storage connectors, or whatever.  But might move on this soon and just have a VM server in Azure to connect to the Blob storage or something."
27.json,t3_9jmkvl,t1_e6sp37b,2018-09-28 22:07:15,"It's GoDaddy. You are 100% getting fucked. You can spend the time and money trying to prove that they are lying to you, or you can tell them to fuck off and go somewhere else. If you can manage the domain yourself your possibilities are fucking endless.Alternatively, tell GoDaddy that you're a customer of $soandso years and that unless they cut you a discount on the next tier, you'll be moving to $competitor, who offers $pricing.They'll probably laugh and tell you to go fuck yourself, because that's GoDaddy's relationship with their customers, but you could get lucky and get the discount and save yourself a migration."
27.json,t3_9jmkvl,t1_e6sp65n,2018-09-28 22:08:28,"Looks like you're getting ~41% off list which, for education, seems pretty good."
27.json,t3_9jmkvl,t1_e6spkle,2018-09-28 22:14:31,Interested in pricing on the s4112 as well.
27.json,t3_9jmkvl,t1_e6sppbv,2018-09-28 22:16:30,Aloha VARs! Can anyone give me an idea if we're getting F'd on some VMware pricing? Our partner is being awfully suspicious with how they're quoting this out. Let me know and I'll post the details of what we're trying to get.Thanks!
27.json,t3_9jmkvl,t1_e6spr14,2018-09-28 22:17:13,"Yeah, make sure to have part numbers and all the pieces asked for up top."
27.json,t3_9jmkvl,t1_e6sr1bi,2018-09-28 22:36:06,I am hoping someone has a ball park figure for Forward Networks Enterprise:3 Users 1000 deviceshttps://www.forwardnetworks.com/enterprise/Trying to figure out if its worth selling to management.
27.json,t3_9jmkvl,t1_e6sr7iw,2018-09-28 22:38:35,Never heard of it.Very doubtful you'll see any ballpark here.
27.json,t3_9jmkvl,t1_e6srgkt,2018-09-28 22:42:14,"This isn't pricing related, but a 15TB Raid 5 array is just about guaranteed to get you fucked in the future on data loss..  Nobody should do raid5. At a minimum Raid 6"
27.json,t3_9jmkvl,t1_e6st6rz,2018-09-28 23:06:31,"Looking at Tegile expansions 2 HE-25 (1.5tb ssd 26tb hdd) 16k each for around 32k total 14 1month premier proactive support (parts, 4hr onsite etc) 1.75k 2 professional services onsite 6.25kall said and done about 40k"
27.json,t3_9jmkvl,t1_e6sttjf,2018-09-28 23:15:25,SquizzOC batter up!
27.json,t3_9jmkvl,t1_e6suf1t,2018-09-28 23:23:42,"I spoke with Tegile the other day. Told me they were doing a promotion where they double your expansion slots for free right now. Not sure if you have to buy a base unit for that to apply, though.In our quote, they say the MSRP for an HE-25 (same as you, 1.5 TB SSD, 26 TB HDD) is $25k, but they're pricing it at just over $11k. Our ""standard onsite professional svc"" is MSRP $3k, actually charging us $2.5k. I think you're getting fucked."
27.json,t3_9jmkvl,t1_e6svhys,2018-09-28 23:38:38,In response to PM found this for a 10 location retail chain in CA:
27.json,t3_9jmkvl,t1_e6swbii,2018-09-28 23:49:45,"It looks high on cost. I'd ask your VAR to do better, they aren't negotiating hard enough on your behalf."
27.json,t3_9jmkvl,t1_e6sxero,2018-09-29 00:04:26,"We are going to be revisiting our SPLA licensing in the next couple months, and just curious if our current vendor is killing us or not.VLA SQL SERVER SAL L/SA ALL LANGUAGESMfgPartNum : 228-05018Qty: 724Unit Price: $14.26Total: $10,324.24VLA WINDOWS SERVER STD PER PROCESSOR LIC/SA ALL LANGUAGESMfgPartNum : P73-04837Qty: 54Unit Price: $16.17Total: $873.18"
27.json,t3_9jmkvl,t1_e6sxqt5,2018-09-29 00:08:59,"Can't really comment on SPLA licensing, but I believe you are in for a 10% increase in price as of the 1st."
27.json,t3_9jmkvl,t1_e6t0v8v,2018-09-29 00:51:36,Get hosting from OVH. its $3.50 a month for a simple VPS.
27.json,t3_9jmkvl,t1_e6tafr3,2018-09-29 03:00:42,"Looking to pull trigger on Tegile - just want to check am I getting screwed.4800 INTELLIGENT FLASH ARRAY PERP DUAL CONTROLLERS 4 XEON = $103,000HE-50 HYBRID FLASH EXP SHELF UPG 6TB SSD & 52TB HDD IN 3U = $22,000DUAL PORT 10GBPS ETHERNET SFP+ CTLR NIC = $2,700INTELLICARE 1YR NON-RETURN SVCS DRIVE OPTION FOR HE-50INTELLICARE 1YR STD SUP FOR SVCS HE-50 NBD 24X7 CALL SUPINTELLICARE 1YR NON-RETURN SVCS DRIVE OPTION FOR T4800INTELLICARE 1YR STD SUP FOR SVCS T4800 NBD 24X7 CALL SUPSupport total = $9,500STANDARD ONSITE PROFESSIONAL SVCS SVC = $2,800Total = $140,000Pure was also considered, but out of my budget.  I did not look at Nimble."
27.json,t3_9jmkvl,t1_e6tamw4,2018-09-29 03:04:01,Give me one week to get real competition from Nimble. You will not be disappointed.PM me if you have the time.
27.json,t3_9jmkvl,t1_e6tcqru,2018-09-29 03:33:14,i got a question on your line 1. if someone asked for solution for single Hybrid-Cloud and Multi-Cloud security solution what would you suggest?
27.json,t3_9jmkvl,t1_e6tdqe8,2018-09-29 03:47:07,Is this VMware only or tied to hardware too?
27.json,t3_9jmkvl,t1_e6tdy8x,2018-09-29 03:50:11,"If you are not already in a cloud provider's ecosystem, and just need raw offsite file storage, Backblaze has the cheapest offering. $.005 / GB / month storage, $.01/GB outbound bandwidth. ~1/3 of S3, Azure, GCP, etc."
27.json,t3_9jmkvl,t1_e6th2uo,2018-09-29 04:35:48,"No such thing, security comes in layers and not marketing speak."
27.json,t3_9jmkvl,t1_e6tj2gk,2018-09-29 05:06:08,"Question on this Dell EMC SCv3020 quote18 2.4 10K SAS drives, two 4-port 16GB fiber cards, everything else is base, with 3 years ProSupport Plus 7X24X4 support. This seems a bit high at $32K ... it's more than when I configure it on Dell's website, but includes installation. We have a 4020 and an old XioTech currently connected to our VM farm over fiber channel."
27.json,t3_9jmkvl,t1_e6tni86,2018-09-29 06:15:49,"Dell VxRail adding disks:qty 3 Install VxRail Node Disk Group Upgrade $11,505.00qty 3 VxRail 14G CACHE SSD 800GB 10WPD 2.5 $6,956.25qty 15 VxRail 14G CAPACITY SATA SSD 3.84TB $58,762.50don't need anyone onsite, but remote assist, systems in Reno, NV"
27.json,t3_9jmkvl,t1_e6tqhk9,2018-09-29 07:06:47,"Likely this is your biggest line item in the quote.  If you're willing to purchase those from a separate vendor, you could probably get drives and caddies for significantly cheaperOne of my favorite conversations with a Dell rep:Me: ""The drive caddies are listed on your site for $69.00, but that same part number is on Amazon (new-unused) for $10.00.   Is there a difference?""The rep:  ""They are exactly the same, but ours our freshly manufactured...""So... if your only goal is that ""freshly manufactured smell..."""
27.json,t3_9jmkvl,t1_e6vxbed,2018-09-30 09:58:22,"You don't say what you're paying but if you can budget around $20/mo you can get a reseller plan from a number of place (stay away from any EIG or ECG subsidies, PM if you want recs in the US, been doing research recently) and not only give them experience with the WordPress but also the cPanel account they will likely need to use when they deal with clients.There are some less expensive reseller accounts too if space requirement is small.  I think down to around $8 or $10/moVPS could be a few bucks cheaper but seems more headache than worth if this is recurring students semester after semester."
28.json,t3_bch22e,Question while in support limbo about system persistence.,2019-04-13 02:31:50,We have put in a VXRAIL system esxi/vsan..the works. Unfortunately I can't put in any tickets for it because vmware wants me to go through my VAR and so far they haven't got back to me. So figure while i wait maybe someone here knows what might be going on. We have a few 2016 machines that are used mostly for RDP which all of a sudden went non persistent. The only thing that really changed was enabling hot plug on ram and cpu but other then that nothing. The drive is set to dependent but after a reboot reverts back to a state from last week. There are no snapshots at all so not sure what is going on. *UPDATE* for anyone else who ends up having this problem. Ended up figuring it out myself. looks like there are citrix drivers from the VDA agent that will cause the system to not be able to write to the c:\ drive which also cause system updates to fail and revert every time the system rebooted.  I ended up having to restore from backup because there was no way to uninstall the VDA agent without booting to full windows and there was no way to  keep from reverting because the drivers were still being called to. even if they were removed it wouldn't unload them so the second it rebooted it would just revert back... once the backup was restore I was able to uninstall the VDA agent run system updates then install the agent again.
28.json,t3_bch22e,t1_ekr2p8h,2019-04-13 05:33:19,"Yuck, who's your VAR?  That's lame!Why are you using hot plug on CPU?  That shuts off vNUMA."
28.json,t3_bch22e,t1_ekr5gq1,2019-04-13 06:03:11,[deleted]
28.json,t3_bch22e,t1_ekramq7,2019-04-13 07:01:51,It's mostly because the servers arent complete and somehow got put in production and i had no idea what the load would be like once 40 people were on it.
28.json,t3_bch22e,t1_ekrappj,2019-04-13 07:02:51,That's what i ended up having to do. Unfortunately wasn't much help 4 hours on the phone with no answer to why it's doing what it's doing.
29.json,t3_b3wmle,VxRail backup,2019-03-22 05:58:19,"Hi, We are interested in VxRail HCI.  Does anyone know if it includes backup software? Thanks"
29.json,t3_b3wmle,t1_ej2sd0s,2019-03-22 06:49:26,"It runs ESX, and almost everything supports backing up ESX. Check out Veeam."
29.json,t3_b3wmle,t1_ej2u3m5,2019-03-22 07:11:12,Thanks for your reply.  Most of HCI come with build-in backup software.  Do you know if VxRail comes with backup app without additional $.
29.json,t3_b3wmle,t1_ej32xve,2019-03-22 09:07:23,It doesn’t come with any backup software for the VMs.
29.json,t3_b3wmle,t1_ej362n1,2019-03-22 09:48:47,Your options will be Avamar/DD and recoverpoint for vms. It's a add on.
29.json,t3_b3wmle,t1_ej36qc7,2019-03-22 09:57:24,"The data protection software included consists of: Active/active stretched clusters, RecoverPoint for VMs (starter license), and vSphere replication. Obviously this is mainly marketing and isn't granular backup and restores but RP4VM and vSphere Replication do replicate and provide point-in-time recovery of VMs. Of course you're going to need another vSphere cluster to replicate to with these options.There are enhanced data protection options like DPS for VMware along with Data Domain VE. These are licensed software bundles that are separate costs. DPS for VMware is going to include Avamar VE, Networker, and RP4VM. Avamar and Networker will get you the granular backups. You'll just need a separate storage unit to keep those.You could just include an iDPA appliance with your purchase and be done with it."
29.json,t3_b3wmle,t1_ej39up7,2019-03-22 10:39:34,Most of the recommendations are for a non HCI solution for backups. Look for HCI vendors for backup too. They exist. Seems sort of backwards to have a more complex solution for backup than the production solution itself.
29.json,t3_b3wmle,t1_ej43gs1,2019-03-22 20:57:15,"Data Domain VE with replication to AWS or Azure? Not free, but it's ""included"" as a plugin in the store. We haven't seen a Rail in a little while though, could have changed."
29.json,t3_b3wmle,t1_ejhmg82,2019-03-27 19:48:11,"Depending on your size and budget you could look at the EMC IPDA 4400, HyperConverged Backup to go with you HCI :)"
29.json,t3_b3wmle,t1_emsb19g,2019-05-08 07:59:37,https://www.dellemc.com/en-us/data-protection/integrated-data-protection-appliance.htm
3.json,t3_brdg2l,REST APIs for VxRail?,2019-05-22 01:44:37,Does anyone have any documentation or examples for the REST APIs for VxRail?
3.json,t3_brdg2l,t1_eocoqe2,2019-05-22 01:58:04,"I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit: If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info / ^Contact)"
3.json,t3_brdg2l,t1_eodooqe,2019-05-22 06:25:20,The VxRail API guide is on the Dell EMC support site (registration required) here: https://support.emc.com/docu91468_VxRail_Appliance_4.5.x_and_4.7.x_API_Guide.pdf
3.json,t3_brdg2l,t1_eofgvvx,2019-05-22 22:08:34,THANK YOU!
3.json,t3_brdg2l,t1_eq247ca,2019-06-05 17:43:28,"Hi,I have some examples of how to use the VxRail Public REST API's on this Git repo:https://github.com/nfonseca/vxrail-apiYou will find a collection of json bodies to build the request and also an experimental Python script that uses the VxRail API's to perform several tasks in one or in several clusters.The Python Script is not officially supported by DELL EMC. It was developed just to provide an example of how to integrate the VxRail APIs with Python.Hope this helps you on your Digital Transformation journey using the best of the Automation features that VxRail offers."
3.json,t3_brdg2l,t1_eq2qc3k,2019-06-05 21:42:09,"Thank you very, very much!!"
30.json,t3_bfgbkx,Server 2016 guest not detecting drive as thin provisioned,2019-04-21 03:53:30,"Hello, I have a newly configured VxRail all flash appliance running ESXi 6.7 build 13004448.  I have deployed a new server 2016 guest from a template with hardware version v14 and latest vmware tools installed.  The VM as well as the template have the default vsan storage policy applied with object space reservation set to 0 - Thin. When I look in the windows guest under optimize drives, I see all the drives registering as Hard Disk Drive and not Thin Provisioned.  I have this same configuration running on other non-vxrail storage minus the storage policy and the guest detects disk as thin provisioned without issue. Is there a setting I'm missing or a gotcha with running this configuration on vsan?  I followed the article  Thanks for the help!"
30.json,t3_bfgbkx,t1_eldfmsp,2019-04-21 04:03:37,I must admit Ive never seen Windows know anything about the VMDKs being thin or fully provisioned. Where would i see this information in the guest?
30.json,t3_bfgbkx,t1_eldh7cj,2019-04-21 04:20:53,"If you open disk defrag or now called defrag and optimize, it will indicate the drive type as either Hard Disk Drive, SSD, or Thin Provisioned.  The type determines what optimizations get performed on the drive (defrag vs. trim / unmap)."
30.json,t3_bfgbkx,t1_ele0wrn,2019-04-21 08:15:05,I am curious about this as well. Old-school Xen had this and I figured it was Microsoft wanting you to use Hyper-V in order for the guest to be thin allocated aware.
30.json,t3_bfgbkx,t1_ele3lwb,2019-04-21 08:49:16,I don't know about Windows recognizing the disk as thin provisioned.  But what I do know is of your disk is thin provisioned and your datastore is vmfs 6 it will automatically take care of the unmap features and properly shrink when you delete data
30.json,t3_bfgbkx,t1_ele6zpi,2019-04-21 09:34:15,"I'm a few months out of a job from running a datacenter and we only used vcenter 6.0, but I don't think Windows guest is ever aware of it has a thin provisioned disk or not. Unmapping needs to be run at the data store level. I think you are seeing the Windows version of a thin disk, and nothing to do with your thin provisioned VMHD.However, I did find this article explaining it a bit. The relevant text:""Starting with Windows Server 2012 (R2 I believe) and on VM hardware version 11 with a PVscsi controller, Windows should be able to pass TRIM commands to the ESX upon file deletions which should shrink the VMDK if thin provisioned.  In ESX 6.0, this will not work with CBT enabled by will work in 6.5.  There is a known limitation where depending on the NTFS allocation size, the UNMAP commands might be mis-aligned causing UNMAP commands to be discarded in ESX but that is also fixed in 6.5 and in your case when deleting 8TB of files, should still reclaim more than what you are getting now.   Since you are looking to re-claim at the VMDK level and not the array level, running sdelete might not benefit you here.    Once you verified the prerequisites above, start by running the defrag / optimize task in Windows to make sure your virtual disk is being detected as Thin-Provisioned.""In my 6.0 datacenter, I had a periodic TRIM command run on certain disks within the guest Windows VMs, then run UNMAP against the datastores from the ESXi hosts(s).Edit: I was just reading your other comments. Are you running TRIM on your windows disks already? This will write to all empty sectors of the VMHD. It's all 0s, but it counts as a writes to the VMware VMHD (I think), and gets set aside as written. Running UNMAP from the ESXi host against the datastore will recover the space."
30.json,t3_bfgbkx,t1_elecy8c,2019-04-21 11:03:51,"Are there any other Windows VMs running on this host who are also on the same datastore as the Server 2016 VM? Do they show that they are thin provisioned?If not, some all flash appliances may not report to ESXi that the LUN is a flash disk and thus the host thinks it’s an HDD. You can check this by viewing the Storage Devices under the configuration of the Host server. It will either say Flash or HDD next to the LUN. You can also manually change this to Flash but it’s recommended that you do a storage vMotion on any VMs on that datastore first, so that the datastore is empty when you make the change.You’re going to find a lot of older articles and VMware admins who say defragmentation is unnecessary on VMs. However, I don’t find this to be the case anymore if you’re using Thin Provisioned disks and VMFS 6. Defragmentation is helpful with the space reclamation feature in VMFS 6. Just keep an eye on your IOPS.Edit: ...and don’t over provision your datastore!"
30.json,t3_bfgbkx,t1_elf2lta,2019-04-21 20:49:19,"Is the underlying datastore formatted as VMFS 5, by any chance? AFAIK, in order for guest-issued unmap support to work it needs to be VMFS 6."
30.json,t3_bfgbkx,t1_elf9g3b,2019-04-21 22:19:58,"Thanks.  This is a vsan datastore in this case so no vmfs5 or 6.I would only need to run Optimize-Drive (defrag) once to do an initial re-trim of unused blocks.  After that, windows does this automatically."
30.json,t3_bfgbkx,t1_elf9nx8,2019-04-21 22:22:29,Underlying datastore is vsan so no VMFS.  VMDK needs to be thin provisioned for guestOS unmap to work properly as well as HW v11 or higher AFAIK.  I am meeting all those requirements and this works successfully on VMFS5/6 datastores already like you mentioned.  I just can't seem to get it to work on vsan which is making me think I am overlooking something simple.
30.json,t3_bfgbkx,t1_elfbcbe,2019-04-21 22:41:34,"Zeroing is one option to reclaim space if you are using thing thick on thin which is similar to my scenario and is something I might have to look into.  This article more or less explains the behavior I am trying to accomplish:  https://www.codyhosterman.com/2017/03/in-guest-unmap-fix-in-esxi-6-5-patch-1/Instead of having to manually zero, windows can automatically issue unmap commands to reclaim space within the VMDK and since vsan now supports native unmap, the shrinking of the vmdk should reclaim space on the datastore as well.My issue is windows is not detecting the vmdk as thin so it won't issue unmap commands and automatic space reclaim is not happening.This might be something I need to ask vmware."
30.json,t3_bfgbkx,t1_elfdtap,2019-04-21 23:10:11,"According to these docs, it seems like unmap support is not enabled by default. Have you enabled this yet, and then power cycled the VM? (A reboot is not sufficient)"
30.json,t3_bfgbkx,t1_elfh47d,2019-04-21 23:47:25,Ah interesting...  I think that might be it.  I was not aware it wasn't enabled by default.  I will check that out and see if it's enabled or disabled.  Interesting it wouldn't be enabled by default as it is with VMFS6.  Maybe something they will change in a future release.
31.json,t3_bqk01f,Distributed Switch black holing VMs after upgrade,2019-05-20 02:09:11,"Completed a VxRail upgrade from 4.5.231 to 4.7.111.  Everything looks fine but had reports of VMs dropping off the network.  Reached a crescendo and they are all in the recently upgraded cluster.  Looks like the dvswitch upgrade of VxRail has partially completed with hosts out of sync and the ""upgrade in progress"" banner still showing.  Colleague worked with dell support to get the out of sync hosts back in but the upgrade is stil in progress.  Am also still seeing vms drop off the network when vmotioned - but not all of them.  Anyone know anyways to troubleshoot a ""failed"" upgrade of a dvswitch?  I have tried powercli to set the version again which completes but the error stays.  also anyone seen this behaviour on 6.7u1 and know of any workarounds?  Am hoping we dont have to move everything to standard switches and recreate the distributed switch"
31.json,t3_bqk01f,t1_eo5gpxd,2019-05-20 04:13:56,"I don't really understand what could happen by VxRail upgrade to your vDS. But out of sync issue is a thing in 6.7u1 , I personally encountered similar problem (hosts being out of sync) I ended up creating new vDS and migrating half of my NICs to new vDS, then migrated my VMs and vmkernels to new configured vDS and then moved rest of my NICs to new vDSI know it is a painful procedure, but nothing helped me to resolve my problem and it took me 2 days to do this! (I wish I did it at the beginning before I waste too much time)"
32.json,t3_b8je1z,"Looking at HCI, what do you use?",2019-04-02 21:42:56,"We have been shopping around for a few months now for new hardware. Our VAR has been pushing HCI with VXRail pretty hard. We got a quote for a 6 node cluster with 30ish TB of flash storage for north of 600K (5 years of license and support included). That is a huge pill to swallow, so we started looking at other vendors like Nutanix, Maxta, and Simplivity. Our biggest push for this is the fact that we use Equallogic storage arrays and they will be EOL and out of support soon. Plus our cluster is mainly R820's for compute which will be out of support soon too. I can deal with the 820s since the hardware will be easy to maintain ourselves, but the Equallogics are a whole different beast, requiring Equallogic specific drives and backplanes. Do you use HCI? What is your vendor? Is HCI even the way to go, or maybe stick with a standard cluster with separate storage and compute?"
32.json,t3_b8je1z,t1_ejy3kft,2019-04-02 21:51:48,Went ended up with nutanix and won’t be looking back anytime soon
32.json,t3_b8je1z,t1_ejy46ek,2019-04-02 21:59:57,"How much storage do you really need, and does it really need to be all-flash? We're trying to push as much stuff to the cloud and reduce our on-prem footprint. Nutanix is on my radar (oddly it's Lenovo pushing it locally), Cisco have HX which might be worth looking at. I do think HCI is worth looking at, since it means you don't have to be as much of a SAN admin, and can just use normal server hardware. VXRail is too fixed IMHO."
32.json,t3_b8je1z,t1_ejy4ual,2019-04-02 22:08:41,"We did Starwind and it all works great. Had someone help me with setup and it was a breeze. Pricing was great especially when I compared to the big players like Nutanix.More importantly, at least for me, was support and it was great. Was really easy to get in touch with them and got help fast."
32.json,t3_b8je1z,t1_ejy4z7v,2019-04-02 22:10:28,"We currently have 51TB used in our cluster on hybrid SSD and 10K RPM Equallogics. No dedup or compression enabled at the SAN level. We don't need super high iops, usually peaking at around 4K W during heavy load. The Equallogics, at least in our setup, require little to no babysitting. Set them up and go.What do you mean by that?"
32.json,t3_b8je1z,t1_ejy4zzp,2019-04-02 22:10:45,"Who else were you looking at? What were your reasons for choosing Nutanix over the others? Did you go with Acropolis for the hypervisor, or VMWare/Hyper-V. We would prefer to stay with VMWare since we have a bunch of ROBO licensed servers at our school locations. Would be nice to have a single management point for all of them."
32.json,t3_b8je1z,t1_ejy503a,2019-04-02 22:10:47,[deleted]
32.json,t3_b8je1z,t1_ejyaig3,2019-04-02 23:18:32,We use Windows Server HCI (https://www.microsoft.com/en-us/cloud-platform/software-defined-datacenter).
32.json,t3_b8je1z,t1_ejyawuz,2019-04-02 23:23:16,"We just refreshed this past summer and went with Nutanix. Zero complaints so far! We did stick with VMware, but I have thought about going to AHV in the future."
32.json,t3_b8je1z,t1_ejydxhu,2019-04-02 23:58:39,"Eh we just evaluated it. For us the premium was way to high for what it is.   Just did more blades and iscsi sans.  I just don't see the real benefit for paying that kinda price. Large VDI maybe, general workload idk.  It largely just seems like a buzzword that people want to hop on. There's nothing wrong with traditional server/iscsi.   Sure it's not sexy, but it's modular and it works. It's easy to troubleshoot and maintain on our own.  If nutanix has a problem it's their Blackbox your relying on and troubleshooting. Not just a straightup host w/ some iscsi targets.VXrail seems like an overpriced vsan. There's also some pretty bad reviews around here about it.Nutanix is a leader but was expensive af. Sure they discounted the first few nodes heavily. But once their in would they just nail us on new nodes?Hyperflex is a sick joke."
32.json,t3_b8je1z,t1_ejye6g4,2019-04-03 00:01:32,How's this working out? Hyperv is great now. Storage spaces scare me a bit. Idk why though.
32.json,t3_b8je1z,t1_ejyft4z,2019-04-03 00:20:35,"We haven't moved on it yet, but we have a Nutanix quote from our primary VAR that's 4 node, 100TB (spinning drives). It's less than half of the price you were quoted ($231k). The quote includes 5 years of support/maintenance and Nutanix admin training for two of us.I can't vouch for Nutanix as we've never used them, but we're certainly considering it. Our VAR is also working with Dell and Lenovo on a Storage Spaces Direct solution as we are 100% Hyper-V here already.  We'll be comparing the two solutions."
32.json,t3_b8je1z,t1_ejyfydz,2019-04-03 00:22:17,Who else did you look at? Why did you choose Nutanix?
32.json,t3_b8je1z,t1_ejyg5i7,2019-04-03 00:24:31,"We are 100% VMWare and would prefer to stay that way. We have 25 or so school sites, each with their own local esx server licensed with ROBO.EDIT:Is that Nutanix quote for their Acropolis hypervisor, or for VMWare/Hyper-V?"
32.json,t3_b8je1z,t1_ejygpme,2019-04-03 00:30:50,Who do you use for your iscsi SAN?
32.json,t3_b8je1z,t1_ejyhmpy,2019-04-03 00:41:15,"We are in a similar situation. I have received proposals from Lenovo, Dell, Scale, and 2 VARs.  Options include vSAN, Nutanix, and traditional Computer + SAN. We have 4 HP Servers and a NetApp we are replacing for the same reason you are replacing your Equallogic . We also need about 20TB of storage. We are pushing as much to the cloud as we can and I think 20TB will get us through the next 5 years. We do not have VDI or plan to implement it.vSAN quotes averaged about $120k. For 3 Ready NodesNutanix quotes averaged about $120k. For 3 Ready NodesBefore you get a quote for Nutanix, make sure you get it from your prefered hardware vendor first. Nutanix seems to give the best deal to the first group who asks.Scale was a few thousand less for 3 nodesTraditional Compute and Storage averaged about $125k for 4 hosts, 20TB of SAN, and vCenter licensing.Windows HCI is not on the list because we have tried it in the past, and it was too much work to get up and running. Not impossible, just with everything on our plates, just more cost effective to pay for support when we need help.I am stuck at the decision of which direction to go. With the prices being so close:One option is to go with vSAN because it is what we all know. We wanted to go with it in the past, but it was just too expensive. But, it does not have dedup and compression in the bundle we are buying. I think that is an important feature to help us stay within our 20TB budget.That brings us to Scale and Nutanix. What do you think?"
32.json,t3_b8je1z,t1_ejyojol,2019-04-03 01:57:05,"Looks like we are in the same boat. It's a tough choice. On one hand I'm a nerd and would love to play with something new like Nutanix/Scale/VSAN. On the other hand I don't want to spend a ton of money cause something is new and cool and end up having it not fit what we need, or have it work but hate the way it goes about different things. This is pretty much going to run our datacenter. It HAS to be right.I like the VXRail system. I am familiar with Dell, VMWare, and it includes their Recovery Point system for DR within our network, similar to VMWare SRM. Our VAR has always been great with us in the past as well. They know our environment almost as well as we do, and have pointed us towards options in the past that would not have been in their (the VAR) best interest, but have ultimately been the best choice for us. I realize that is what they should do, but these days it's not what often happens.I'm also trying to think of how whatever we get will fit into our current monitoring and backup infrastructure. Trying to stay VMWare as well since our school sites each have a single esx server running ROBO licensing and I would prefer to stick with one management console for the two of them. Also our backup system supports VMWare, Hyper-V, and bare metal installs. If we get something like Scale we would need to install the agent on all machines since it wouldn't be supported with agentless backup, and we still would need to keep VMWare for the schools, or moved their servers back to the datacenter. Also deciding if HCI is even the best fit. Maybe just stick to separate compute and storage. But then we end up in a similar situation of having to find a new SAN."
32.json,t3_b8je1z,t1_ejyp7pp,2019-04-03 02:04:12,"Not K12, but manufacturing here.  We went with Nutanix and have been incredibly happy.  It just works.  It's so simple and intuitive.  Great monitoring, easy to configure, and the performance has been spectacular.  Obviously, 10Gb networking is a must, but it's worth it.  We're using VMware as our hypervisor.Tbh, we didn't evaluate that many other HCI solutions.  We priced out some alternative solutions like the traditional SAN route and some Dell blades with isci storage but nothing was as appealing as the Nutanix offer.Since we implemented it, Nutanix has been great in regards to service/support and we've added some additional nodes without a significant sticker shock.  I would do go with Nutanix again."
33.json,t3_aymwkq,What about the IDPA Backup tool,2019-03-08 14:05:22,How is the result of IDPA bakcup in the market ..I saw most of the Customer are moving from Legacy backup tool or majorly from Avamar-DD to IDPA bakcup solution.
33.json,t3_aymwkq,t1_ei2ukfx,2019-03-09 00:35:43,"They are moving away from legacy, yes, but IDPA is a legacy backup product.  Fundamentally, it's Avamar, a very ""legacy"" backup product.  It uses Datadomain as a storage target, something people have been doing for over a decade.The only thing ""new"" about IDPA is the packaging.  Essentially, you're buying a reference architecture, that included Datadomain and Avamar.The IDPA 4400 is an interesting play for SMB users.  Everything is nicely packaged into a 2U R740xd server, and comes with all the licensing you need.  The catch is, there is a single instance of ESXi hiding in that box.  Updating the hypervisor will be a very interesting ask when it needs to happen.  The messaging Iv'e gotten from Dell is that support will likely be the ones to do those updates, at least in the near-term.The larger IDPA appliances are literally a half-rack or more of gear, and require, generally speaking, a Dell ProDeploy SKU to be included in the quote to handle getting the system online.Rubrik and Cohesity are more ""next-gen"" than IDPA, imo.  They are designed to support, primarily, HCI workloads and offer a very similar consumption model as HCI when you need to scale them.  That said, they're expensive for what they are.Commvault is interesting here, in that they still have a deployment model that allows you to leverage commodity hardware, or re-purpose your existing production storage as ""media agents"" bring the cost per TB down a lot for backups.Pure is getting into the data protection game too, but that product is too new to have an opinion.On a whole, Dell is just looking for ways to repackage existing IP into new ""products"" and ramming down the throats of their existing customers, which is a lot like how EMC operated.  There isn't much innovation in VXRail or IDPA, but that doesn't make them bad products, so long as you understand what the real value proposition is.So, I say all that, and I just did an IDPA 4400 build this morning for a customer looking at a 5 node VXRail solution.  It's a good fit, and the package is big enough that Dell will discount it somewhat aggressively, so it'll be a very good value-for-money solution."
33.json,t3_aymwkq,t1_ei4fkz5,2019-03-09 13:23:13,Great Explanation!! I believe Cohesity and rubrik this is trying to capture the market in backup industry and now they are coming with the Software define storage and backup so that we can use them with any Hardware which is certified from them.. and I guess they are coming with cisco hp etc ..
33.json,t3_aymwkq,t1_ei9vyrt,2019-03-11 21:22:03,Packaging is key...we're working through a review of the 4400 right now. It's actually become progressively better pretty quickly with tighter integration on the unit itself.
34.json,t3_51949n,vxRail: anyone have any first hand experience?,2016-09-05 20:08:24,"We've got a full paid and supported ScaleIO system that has been nothing but a source of pain for over a year now. Our hardware is very close to the spec vxRack configuration, but no end of trouble and Emc seem to be without an idea. Evaluating other options from them to migrate to, looking at an all-flash traditional San option, or vxRail. Basically, same setup as we have now, but with VSAN instead of ScaleIO. Are we gluttons for punishment for even considering hyper-converged again, or have we been facing a one off configuration mess up/failure. I guess what I'm asking is: how has vxRail been for others? VSAN seems fairly stable these days, but on the surface so does ScaleIO."
34.json,t3_51949n,t1_d7a92lb,2016-09-05 22:07:58,Well... what are you intending to use this setup for?
34.json,t3_51949n,t1_d7abk0i,2016-09-05 23:22:25,"Any chance you can share some details about your ScaleIO problems, we're about to get in on this with 28 nodes and I wouldn't mind questioning EMC about it.As for vxRail it's the evolution of their old evo rail but in theory it sounds fine.  That said, vSAN still has problems if you've been following vmware .  Depending on the scale you want, you might want to consider the vxRack SDDC version which is vSAN instead of the vxRack flex which is scaleio."
34.json,t3_51949n,t1_d7addst,2016-09-06 00:11:17,"Disclaimer, VMware employee, working in the VSAN group.1st, not sure what your ScaleIO issue is, but there are some pretty good folks over there that can likely help with your issue. Feel free to PM me with your contact info & I'll reach out to those that I know.2nd, VSAN is very different from all the other HCI/SDS platforms out there today. While no software is perfect, once on supported hardware, with supported firmware/drivers, VSAN is very solid. I didn't work for VMware until after VSAN 6.0 was released, and many/most/ifnotall of the issues with VSAN 5.5 have been resolved. If you want to get familiar with VSAN, try the Hands On Lab: http://labs.hol.vmware.com/ (look for HOL-SDC-1608)3rd, VxRail is not just VSAN, but other software on top of VSAN, on EMC/VCE hardware. VxRail isn't available on any other hardware, and must be bought as an appliance. Here's an interactive demo: http://interactivedemos.emc.com/vxrail35/ (to see the difference between VSAN/VxRail)Again, if I can be of help, feel free to PM me. Jase"
34.json,t3_51949n,t1_d7ao93d,2016-09-06 04:50:22,"+1, please OP, share your ScaleIO story!"
34.json,t3_51949n,t1_d7aqcvt,2016-09-06 05:46:06,"We were looking to do a POC of ScaleIO but I've heard mixed things. Then we came across Starwinds VSAN and so far in our lab, things have been going well.I'd reach out to the community for help, otherwise give SW VSAN a try. The licensing is very affordable too and support seems very helpful even though we haven't made the purchase yet."
34.json,t3_51949n,t1_d7ar3go,2016-09-06 06:05:57,Couple comments...
34.json,t3_51949n,t1_d7arxw8,2016-09-06 06:28:51,"All of the issues with VSAN that I've seen in the past year on VMware stem from non-supported hardware being used (There's a built in check for this in the health UI), or a case related to the driver/firmware issue with the perc730 that was fixed early this year).VxRAIL does carry over some code from EVO:RAIL but my understand is it is 80% net new code at this point. Hundreds of addition pre-configuration has been automated, they've extended preflight checks quite a bit, and they've included some day 2 stuff (CloudArray, RP4VM, onboard phone home stuff for support etc).As far as scale, VxRAIL 3.5, now supports multiple clusters managed by the same vCenter (so in theory you could put 1000 of them behind a vCenter if you felt like it). Your clusters will max out at 64 hosts, but given that that is a lot of VM's in a management/failure domain already this typically isn't an issue.  The benefits of VxRACK SDDC have more to do with lifecycle management components and turnkey deployment of vRO and NSX.Also in the interests of confusing us all, the VMware product name was  changed from EVO:SDDC to VMware Cloud Foundation at some point (Although I think EMC is sticking with VxRACK SDDC for now).My biggest concern is the memory or CPU may be anemic if these ScaleIO nodes were meant for storage only."
34.json,t3_51949n,t1_d7ascn8,2016-09-06 06:40:08,This is a great question. ScaleIO's strengths and weakness's are kinda the opposite of VMware Virtual SAN.  You typically don't see both pitched for the same use case...
34.json,t3_51949n,t1_d7b009w,2016-09-06 10:08:05,"Primary usage is a SQL Server. We've got about 5Tb of data split between 2 SQL servers. One is archival, and is about 3TB of that data, the other is active usage, fairly heavy IO and where our main pain point is.The rest of our system is made up of file servers, web servers, etc, running on windows. We're not really seeing any major performance issues there, but that's mainly because they're not heavy IO operations."
34.json,t3_51949n,t1_d7b0fs2,2016-09-06 10:19:51,"I've posted about it here before, and that post is actually part of what got EMC support to finally respond to us. We bought and deployed Scale IO about a year ago. We had a VNXe prior, which we'd hit a performance cap by maxing out its 1gbps network interfaces with iSCSI, so wanted something new.Our parent company chose and supplied the ScaleIO solution, we had no input to the choices, but I do love the idea of the hyper-converged setup.We ended up with 4x Dell R730xd's, with 4 10GbE connections per server to a pair of Brocade VDX switches. each server has dual Xeon E5-2667 v3 @ 3.20GHz, 512GB ram each and the following disk setup: Raid Controller: H730 Mini 17x 600GB 15krpm SAS disks 7x 1.6TB SSD SAS disksAcross the cluster, this gives us ~76TB raw capacity, or about 35Tb usable. We're using about 20TB at this stage.From the get-go we had problems. Performance has always been shocking, slower then the VNXe, latency miles out. But worse, reliability issues. Not long after installing, we had problems where the entire storage system would fail out. Rebooting all 4 hosts was the only way to recover. Thsi occurred every couple of days.We eventually traced this to a failing 15k rpm disk. One single disk took down the entire system, kind of the opposite of what ScaleIO is supposed to do.Turns out this was because the ScaleIO ""controller VMs (SDS/SVM)"" were on local disks, attached to the same raid controller as the storage disks, this was why. (failing disk spammed SCSI bus resets, controller shit the bed, ScaleIO couldn't talk to the SVM/SDS properly, and thus the whole cluster went down)We also had massive problems whenever a rebuild/re-balance occurred of the system dropping to almost unusuable state (VMWare seeing disk timeout failures, IO re-tries, etc.)This was diagnosed to being the same problem, the H730 apparently gets flooded with iOPS, and the SDS times out. Doesn't seem what you'd expect, but Dell ended up supplying us 2x 300GB 15k rpm disks, the entire back-plane mounting kit for 2 more disks, and a second H730 PCI card for each of the servers. At no cost.After installing this, the rebuild/re-balance issue is mostly gone. there is still a minor impact during those operation, but its not crippling like it was. We haven't had a disk failure yet, but I suspect that may be immune to that now, but can't really trust that for sure.In the last week, we've had one node decide to just offline itself from ScaleIO for about 5 minutes. This kicks off a rebuild/re-balance, but in these cases, the system lags out again (doesn't do it during planned rebuild operations) -- It's actually symptomatically very much like the original failing disk issue, but so far no disks report any issues.I'm not saying EMC aren't working on it, we've just had enough. We've basically demanded that EMC supply us with an alternative storage system until we can trust ScaleIO again, so we can move out production workloads there. They're deciding between an All Flash SAN, or to give us 4x vxRail as a solution. The SAN has the traditional ""single point of failure"" downside, but vxRail is another hyperconverged solution, so we're a bit gun-shy.."
34.json,t3_51949n,t1_d7b0l09,2016-09-06 10:23:38,"our current hardware would be replaced if we went to vxRail. We have 4x Dell R730xd's with H730's for ScaleIO. EMC are looking to give us a temporary solution to move production to while they resolve the ScaleIO issue, with a lead to crediting the ScaleIO cost against the temporary solution if they deem it cannot be salvaged.I'm not privy to the purchase details, but I know it was bought as a package via Dell, to include the 4x R730's, ScaleIO & VMWare licencing.I'm guessing that part of the deal EMC will do if they trade this in would be to convert the licencing. We'd keep our current 6 licences, and EMC would add the VSAN support, cost-wise, and the rest of the deal hinges around the hardware side then.How it balances out at a dollars level isn't my concern.I've posted a detailed summary of the ScaleIO issues above."
34.json,t3_51949n,t1_d7b0md1,2016-09-06 10:24:37,"I've posted a summary above about our issues. I love the concept of ScaleIO, and hyper-converged in general, so I'd love to be able to stay with that. I guess i'm just concerned about having major issues on one platform, are we better to jump back to a traditional SAN model, or try again."
34.json,t3_51949n,t1_d7b0t6k,2016-09-06 10:29:42,"I'll PM you details in a second. We are working quite closely with EMC,  up to almost daily phone calls at this point, but after nearly a month of this close-contact regular followups, lots of log gathering and random setting poking, we're just not getting anywhere. It's not that they're not trying, it's just not working.They're going to give us a temporary storage system to put our production workload on until they can fix ScaleIO. If they deem they can't fix ScaleIO on our system, then we'll be doing a refund/credit trade deal to the new solution.It will either be a traditional All Flash SAN, or a vxRail solution.It's good to hear that VSAN is a pretty solid product, i'd have to say it's probably got a much bigger installed-base than ScaleIO, and vxRail means 100% compatible hardware (even though our systems currently are almost exactly the vxRack hardware configurations)So VSAN on our hardware isn't going to be a choice, as this has to be a deal structured with EMC directly. It will come down to EMC Unity vs vxRail.vxRail would cost us more to do teh changeover, since we'd be getting 4 whole nodes of it, but it woudl mean the existing R730's we've got could be re-purposed to our DR site, albeit with no shared storage. I guess we could look at VSAN for the DR site, but for now, that's not in focus."
34.json,t3_51949n,t1_d7b4h5z,2016-09-06 12:15:15,"Ouch.  Those drives and HBA should all be certified for VSAN, and no, they shouldn't be putting out performance numbers that high."
34.json,t3_51949n,t1_d7b518a,2016-09-06 12:34:44,"I'm glad to hear my feelings verified :) The problem part is that EMC also agrees that none of this is normal, but everything checks out ok.Worst part? if you push the storage system with a benchmark, it performs beautifully! 100,000+ iOps, 600Mbps sustained and low latency in that test. but then in general operation, its woeful. It's astoundingly annoying!"
34.json,t3_51949n,t1_d7bpiyj,2016-09-07 01:20:21,"Dear VIDGuide, I am the founder of ScaleIO and the business' VP & GM at EMC.I would like to make sure that our Engineering team is directly involved and solves any issues you may have. Can you please send me your tel. number or email, so that we can ensure that your problem gets resolved post haste?ScaleIO has been running successfully in production on thousands of servers and many PB's at many of the world's largest enterprise datacenters, as well as at a large number of small and medium enterprises, since early 2012...Let's understand what issues you are dealing with, and help you solve those ASAP. Feel free to contact me at your convenience. Best regards, Boaz"
34.json,t3_51949n,t1_d7dj8cd,2016-09-08 08:07:42,"thanks for the message. I will PM you back shortly with the SR numbers. i can confirm engineering is involved, and there is activity, there just doesn't seem to  be much in the way of progress or outcomes. We've had ScaleIO for over a year at this point, with multiple failures and unreliability issues, on top of the ongoing performance issues, which we had the original SR opened for back in November last year. Re-issued (only after posting on Reddit and getting EMC's attention) got a new SR opened a few months back (May I think from memory), and only in the last few weeks have we gotten 'daily phone calls' and 'level 4 engineering' involvement after threatening to basically demand a refund and go to a competitors product.Right now the action plan is to do an upgrade (from 1.32.3 to 1.32.6) and hope for the best, basically. It's not exactly filling us with confidence that anyone has any idea of where to start looking."
34.json,t3_51949n,t1_d7f4npe,2016-09-09 10:32:47,"Our insiders told scaleio will go and VMware VSAN / Microsoft s2d are here to stay. Dell is consolidating storage stuff, and will get rid of an aging/ overlapped product lines. Equal logic/ compellent/ scaleio .. That's what I was told, no references so use with a grain of salt ;)P.S. We didn't have any success with scaleio ever, and tend to replace it for our customers with the other similar SDS options (Ceph?) where we can."
34.json,t3_51949n,t1_d7fdfv1,2016-09-09 15:21:29,fucken A!
34.json,t3_51949n,t1_d7fei2n,2016-09-09 16:17:45,"We have been using HP VSA a long period of time until we’ve got time bombed while purchasing further licenses for new hardware. Since the performance was not so good either we decided to switch to something else.Our IT department has tried to create a Ceph-based storage array. Nice technology indeed, but you need to have a literally very experienced person to properly maintain and support those things.So we are using software from Starwind quite a while already and are satisfied for what is does for the money. Recently we’ve got a couple of DELL ready-nodes from these guys. They call it hyper-converged appliances. Very happy so far.*typo"
34.json,t3_51949n,t1_d7m9fkt,2016-09-14 19:43:08,If you want to know what real users think it's worth reading the user reviews on IT Central Station. You can see VSAN user reviews here: https://www.itcentralstation.com/products/vmware-virtual-san Users who read reviews for VSAN most often compared it to HPE StoreVirtual. Comparison page is here: https://www.itcentralstation.com/products/comparisons/hpe-storevirtual_vs_vmware-virtual-san (you'll need to register to read more than one review.) I hope it is helpful.
35.json,t3_9bjuic,Data center refresh,2018-08-30 21:58:08,Currently we are in the planning stages of moving from a traditional 3-tier solution based on Dell Equalloqic SANS.  We are looking at both VXrail and Nutanix.  Anyone have any experience with these solutions or have made the jump to Nutanix?
35.json,t3_9bjuic,t1_e53hmgg,2018-08-30 22:11:31,"We're looking at both currently too. If you're looking for something a little smaller. Check out StorMagic. We've just implemented it in one of our datacentres and it's also great. Stupidly simple licensing model, and you can drop as low as a 2 node cluster."
35.json,t3_9bjuic,t1_e53i4sl,2018-08-30 22:19:17,"I'd throw Cisco Hyperflex in there as a solution worth consideration, but VXrail and Nutanix are both solid!"
35.json,t3_9bjuic,t1_e53mpt9,2018-08-30 23:25:13,Do an honest cost comparison. The 3-tier traditional option is still more cost effective in many cases.
35.json,t3_9bjuic,t1_e53piad,2018-08-31 00:03:38,"I've been going through this process for the last month and a half myself.We're most likely going with VxRAILS, but I finally heard back from HP, so I'm giving them a week and a half to put together a Simplivity solution and quote, so we'll see.Nutanix was described to me by their own sales person as ""needing some more work""."
35.json,t3_9bjuic,t1_e53s8rp,2018-08-31 00:40:32,"We are same. R730+EQL.Looked at 3 different Dell/EMC solutions, VXrail came out most expensive, then VSAN ready nodes, then a like for like using SC5020 instead of EQL which was the cheapest.Not really 100% convinced on any of em.... still looking!"
35.json,t3_9bjuic,t1_e53v02p,2018-08-31 01:16:22,We where orginally presented a 3-tier solution which came in a bit cheaper but we like the idea of cutting down on admin consoles and simplying the environment.
35.json,t3_9bjuic,t1_e53xwwd,2018-08-31 01:55:05,"Either of these are great. If you are looking at simplifying management this seems to go well only if you are all in on the stack you pick. If your side by side or only use HCI for one workload you still have multiple consoles, etc.Some solutions will integrate with existing vCenters or management but you may code lock all your other gear as you have to wait for the HCI to be validated on new code. A positive is the HCI vendor validates all the code/drivers but it takes more time which holds up your other equipment if they are integrated."
35.json,t3_9bjuic,t1_e540rbi,2018-08-31 02:33:50,"I've been waiting to hear back from Cisco for a month now. I'm a signature away from dropping a quarter million on a refresh project.Dell has been involved each step of the way, I finally heard back from HP this week and they now have until Wednesday of next week to get me numbers, and a month in with every other day attempts at contact, haven't heard dick from Cisco other than ""We'll get back to you"".Edited: Some grammar (don't reddit while meeting)"
35.json,t3_9bjuic,t1_e5491td,2018-08-31 04:28:33,FWiW I just did a refresh to VXrail this summer and it was smooth as hell.
35.json,t3_9bjuic,t1_e54ibnv,2018-08-31 07:23:25,Datrium?
35.json,t3_9bjuic,t1_e55okeg,2018-08-31 22:29:15,"Made the jump to Nutanix about 15 Months ago, and haven't looked back.It's costly, but the lack of overhead is worth it. Support is pretty spot on, too."
35.json,t3_9bjuic,t1_e55omw0,2018-08-31 22:30:17,"You are right with this from a CAPEX perspective, but it also costs money to pay people to maintain all of that infrastructure. HyperConverged really helps lower OPEX."
35.json,t3_9bjuic,t1_e5g6thq,2018-09-06 02:38:09,I'm a Nutanix focused VAR if you need guidance on additional pieces like network/backups. Lots of integrated solutions out there. We also provide services/training if you don't have a Nutanix VAR yet and want to give us a look.Happy to help!
35.json,t3_9bjuic,t1_eeyisbo,2019-01-26 04:26:18,"we are very similar. We ended up with Dell VxRails. We are not a high performance heavy SQL/ OLTP type shop so VxRail was good for our mix purpose environment.The big whoa moment was when we did an upgrade. The VxRail certifies an upgrade bundle and in a A single click and it upgraded the firmware, bios, nic drivers, idrac software, vmware software and even vcenter all without a hiccup all within a few hours.This use to take us months of planning and looking at compatibility matrixes to make sure every piece was supported. Nothing worst then esx host software and then everything dies or PSOD's because a nic driver or some firmware wasn't supported."
36.json,t3_9eik24,HCI with Fibre Channel only - possible?,2018-09-10 09:49:20,"Hi all, I've been thrown onto a vSAN HCI project that was fully scoped before I arrived.  I apologise for the possibly stupid question in advance - I've only ever used FC in the context of storage.  Of course, this was a pure sales exercise until I came along - no technical review of the solution as scoped. So I've racked a 4-node HCI cluster, and realised that each node is equipped with 2x optical SFPs - and a switch filled with a handful of optical SFPs too.  Not a copper port in sight apart from the iDRAC port. I know that IPoFC is a thing - but I cannot find any reference to this being supported with ESXi.  This thread does not fill me with confidence:  Any input appreciated!"
36.json,t3_9eik24,t1_e5p88in,2018-09-10 09:56:57,Are you sure those aren’t SFP+ ports attached to a NIC?
36.json,t3_9eik24,t1_e5p8lnl,2018-09-10 10:02:45,"You're likely seeing a 10Gbps switch. HCI relies on network interconnect for cluster storage. FC won't do TCP/IP. Get a consultant. vSAN is difficult to configure and be reliable, especially if it's your first time. No offense, but it sounds like you're in way over your head."
36.json,t3_9eik24,t1_e5p90tv,2018-09-10 10:09:38,"...this is why I apologised for the stupid question :)Yes - it probably is.  This suddenly all makes sense and I'm a bit ashamed that I didn't realise this!We're replacing their core switches too and there's still not a single copper SFP in sight though, so it's still sales going in half-arsed :)"
36.json,t3_9eik24,t1_e5p9muh,2018-09-10 10:19:58,"I'm in over my head, but I wouldn't say ""way"".  It's a VxRail solution so fortunately it's a bit more out-of-the-box than stock standard vSAN.Not helped by the way sales ship a bunch of nodes and then say ""figure out how this all goes together"".  But it's the way things are usually done... and it's how we learn :)"
36.json,t3_9eik24,t1_e5pd9hc,2018-09-10 11:20:38,"We just rolled out a 4 node VXRail HCI solution.Dell Pro Deploy did the base configuration for ESXi, VXRail MGR, and vCenter.  Cost like $10KNothing overly complicated but definitely needs to be done by a VXRail certified deploy expert.  After watching I would feel comfortable deploying again.Requirements they gave us were simple network port configurations, but must be iSCI and not isolated storage switches, must be routed to the rest of the network as the VXRail uses 4 NICS and they tag traffic instead of dedicating traffic to a NIC."
36.json,t3_9eik24,t1_e5pjz8z,2018-09-10 13:44:30,You are mixing up fiber channel which is a protocol and fiber cable which is a medium. If the switch and servers have SFP+ ports you can use DAC cables instead which are cheaper but limited on distance (10m). If they truly have optics you will have to use fiber cable.
36.json,t3_9eik24,t1_e5q60ev,2018-09-10 23:10:08,"It's fairly easy with EasyInstall to boot strap things, and Start to finish the config. https://www.youtube.com/watch?v=5Qc6BYlet3IAs this is VxRAIL I'd call Dell-EMC and have them do the install, but if you have a vSAN cluster and need some help with an install there are quite a few options."
37.json,t3_9jj41n,AMD FirePro™ S7150 XD Support on vCenter/ESXi 6.5,2018-09-28 10:25:54,"So, I had a strange one this week.  The client is using AMD FirePro S7150's on XD.  Hypervisor is vCenter/ESXi 6.5.  Does anyone know if AMD FirePro's are supported with vCenter/ESXi 6.5 and Citrix?  The whitepapers are Citrix + XS or vCenter/ESXi 6.5 + Horizon View.  I can't find a definitive answer. VDIs are: 4 vCPUs, 8 GB memory, 1/8th of a vGPU, and 50 GB of free disk.  Hardware is new Dell VXrail. Long story short, the client's app isn't calling the GPU and the integrator offered to allow the client to return the cards... the client can save ~100k, shouldn't I be the good guy?!  In a VDI sessions, the Citrix Graphics, the Desktop Windows Manager, and the Client Server Runtime Processes are eating up 80% of the GPU when the mouse is moved.  The client wants to know why. In summary, the two questions are: Are AMD FirePro S7150's supported on vCenter/ESXi 6.5 with XA/XD 7.15 LTSR? Why are the Citrix Graphics, the Desktop Windows Manager, and the Client Server Runtime Processes eating up all of my GPU time?"
37.json,t3_9jj41n,t1_e6sp923,2018-09-28 22:09:41,"https://www.amd.com/en-us/solutions/professional/virtualization/citrixhttps://www.amd.com/en/support/professional-graphics/firepro/firepro-s-series/firepro-s7150-active-coolingLooks like it from their documentation 7.15 + 6.5 should be supported.Their deployment guide is devoid of Citrix references though....https://drivers.amd.com/relnotes/amd_mxgpu_deploymentguide_vmware.pdfFound some other stuff (sure you've seen the same)https://support.citrix.com/article/CTX220663And found this 2 year old thread I posted in, obviously things have changed since then -https://www.reddit.com/r/Citrix/comments/51ls90/mxgpu_amd_firepro_s7150_citrix_support/"
37.json,t3_9jj41n,t1_e6tbyxa,2018-09-29 03:22:20,"Thanks, I found all of those sources except (surprisingly) the CTX220663 article.  I ended up using RDAnalyzer, which worked out.My problem here is that I can't find any white papers for Citrix on ESXi.  I know both XS and ESXi are supported hypervisors, but if this one ends up blowing up, I don't want either Citrix, AMD, or VMware saying a competitors platform is not supported. Ultimately, I think the cards are going to be sent back because they aren't helping the appliation out."
38.json,t3_b1fppa,Dell VxRail nvme cache,2019-03-15 22:37:45,"We are looking at a few vxrail options (g560) but our guys are pretty hands on.  Looking at solve desktop, why is there no customer replacement procedure for the nvme cache disk?  I would reall rather avoid calling dell out when one of these things go."
38.json,t3_b1fppa,t1_eilk32s,2019-03-16 00:36:53,I would go with VSAN Ready Nodes instead of VxRail if you want to be more hands on with the system.
39.json,t3_ahsatq,Does VxRail actually add anything to vSAN?,2019-01-20 08:34:39,"Those of you that have deployed a VxRail in your environments, has VxRail added anything of substance? Currently troubleshooting some unrelated VM issues but I came to the conclusion that VxRail doesn't add anything to vSAN. It appears to be fancy dashboards with proactive monitoring from EMC if a host has an issue (which is nice but not sure if it's worth the premium) but aside from that, I don't see anything added to vSAN or increased performance. Am I wrong here? Or those you that have mature VxRail deployments, is there some feature I'm missing?"
39.json,t3_ahsatq,t1_eehmnj3,2019-01-20 09:09:26,"EMC doesn't add anything to your datacenter.Edit: who ever downvoted me, I propose a challenge. Name an EMC product that you think I can't find a better, cheaper alternative for."
39.json,t3_ahsatq,t1_eehyz9v,2019-01-20 11:32:35,"It automates the process of updating both your ESXi installation and your hardware drivers/firmware. It's important to not use Update Manager with VxRail, to keep yourself on a supported configuration.Theoretically, it's a one-click upgrade for your entire cluster. In practice, it's not that consistent, and takes about an hour and a half per node. And you can't just leave it to it's own, because it fails enough to where you can't just trust it.The setup process is pretty slick, and it automates the entire deployment and VSAN setup.I don't think it's worth the price premium, but it's a good fit for VDI, or ROBO installations, or anything really where you're not putting a ton of load that's heavily transactional. Don't run SQL on it and you won't have a bad time."
39.json,t3_ahsatq,t1_eei2rvx,2019-01-20 12:19:46,"Your question does it add anything to vSan. Answer is No. It’s a vSan delivery appliance. What it does is give you a bunch of automation to help with “day 1” and “day 2” tasks. Allows a system admin to focus more on the business need as opposed to working to keep the systems lights on.1 - Automated Cluster configuration, including mgmt cluster, - PSC, vCenter, vDS , vSan and TORs.2 - Life Cycle Mgmt.  updating everything from Disk level firmware to vCenter and mgmt Stack3 - These code versions and the order in which they are patched are pre tested and pre validated to insure compatibility.4 - Automated cluster scale out/up. Ie cluster/disk add5 - Single support for hardware and software stack5 - Only appliance certified  for VMware VVD, latest version of cloud builder has VxRail Specific6 - Integration with VMware VCFIt can take time to update nodes depending on much data in your vSan data store. As during upgrades data is moved off each node and distributed across the rest of the cluster. To ensure compliance with the vSan SBPM policy."
39.json,t3_ahsatq,t1_eei2xqh,2019-01-20 12:21:43,Re SQL. Once sized correctly it’s not a problem. Know a lot of institutions running a lot of SQL workload and they are more than happy with performed.
39.json,t3_ahsatq,t1_eeiad9o,2019-01-20 13:57:47,"As others have said, it's just a packaging and automation and support structured.Technically you can get proactive support/phone home on Dell servers (ProSupport Plus?). vSAN itself has a phone home system to VMware (Support Insight).What VM issues are you having? Anyway the community can help :)"
39.json,t3_ahsatq,t1_eeidzt0,2019-01-20 14:54:59,"Please, just buy Ready Nodes..."
39.json,t3_ahsatq,t1_eeiqr34,2019-01-20 18:38:52,"That's not the point. Of course, there is always a product that is cheaper. And of course you will find a customer for every product in the world who has problems with it and is dissatisfied. But is Dell EMC therefore worse than other manufacturers? No."
39.json,t3_ahsatq,t1_eejesra,2019-01-20 23:36:16,"VSAN is very easy to get wrong; you have to adhere strictly to the HCL and have just the right firmware/drivers. VXRail makes that  bit easier since all of their code ""works"" and is supported. Note they are every bit as vulnerable to issues as any other vendor to driver/firmware/software bugs, they just make code/parts easier to pick out.As a VSAN customer with VXRails we have had lots of ""uh oh another ""data-consistency"" VSAN bug lets patch"" the last year especially, and have had 2 critical Intel driver bugs we have needed to patch for.Also don't expect ""special support"", you aren't going to get VCDX-grade VXRAIL support engineers, you will get hardware that is easier for VMware or EMC to troubleshoot, but thats about it. The VMware support is still the ""luck of the draw"" support techs.I think the product is ok for a product using VSAN; but just like every other tech product don't expect your results to match what the sales guys tout."
39.json,t3_ahsatq,t1_eejku6t,2019-01-21 00:34:31,"As someone that worked for a VMware/EMC/Cisco partner and installed vxRail, please...don't.  I called them vxFail.  Installation is crap, support is worse.  Just don't do it.   Just buy vSAN-ready nodes from your preferred vendor.That said, there are a couple of use-cases where hyper-converged really makes sense.  I just can't recommend vxFail as that hyper-converged solution."
39.json,t3_ahsatq,t1_eejlcan,2019-01-21 00:39:08,"The VSAN controller driver firmware checks are built into the product. VUM is integrated to patch the controllers now.  VxRail automates non-VSAN related firmware and bios which is nice, but note that can also be done with OpenManage (which has VSAN baselines)."
39.json,t3_ahsatq,t1_eejue00,2019-01-21 01:56:49,I can’t agree more with everybody else in this post. You’re much better off buying vSAN ready nodes or exploring HPE SimpliVity/ Nutanix than you are buying VxRail
39.json,t3_ahsatq,t1_eekkix3,2019-01-21 05:57:18,Same story from me.  Previous role at a VAR that liked to push VxFail regardless of its abilities.  Don't do it.
39.json,t3_ahsatq,t1_es45vx7,2019-06-27 03:36:02,"We have VxRail out our Main DC We've experienced poor support, limitations (supported configurations), slow moving bureaucracy.VMware and VSAN is already great it seems like VxRail is for a slow moving hands off approach that you can't touch or configure without asking. If you can build your own clusters use VSAN Ready Nodes."
4.json,t3_asf9bx,Anyone have a Nexus 9K base config for a new VXRail install?,2019-02-20 04:52:44,"I'm going through their switch guide, but want to make sure I am not missing anything critical (like system MTU, etc.). If anyone has a base/sanitized config, I'd appreciate it. I haven't been involved in a VXR deployment before, and need to turn around the switch install and config in a day, basically (a few days from now). I'm trying to go in with everything but the site-specific info already laid out, so I can just blow it in and concentrate on the physical stuff. Thanks for any assistance anyone can lend."
4.json,t3_asf9bx,t1_egtypks,2019-02-20 05:39:36,"I've done my fair share of VXRail deployments with various switches.  It's not really that difficult you just have to know what to expect.  You get one shot to build the cluster or you're going to spend the next two hours resetting all the nodes if it goes south and trying again. Pray to the gods and block off a couple days if you have to join it to an existing vCenter.You just need a VLAN for Management, VSAN, vMotion, and one or more for production VMs.  I HIGHLY recommend setting the management VLAN as native on the trunks you send to the nodes.  Each node will require two links before it will let you build the cluster.  VSAN doesn't use multicast from VXRail version 4.5+, but make sure that MLD snooping is enabled for the management vlan.  The hosts will discover each other over the management VLAN.  If you set the management VLAN as native tell the server guys to set it as VLAN 0 in the VXRail deployment if it asks for a Management VLAN number."
4.json,t3_asf9bx,t1_egukrru,2019-02-20 10:13:26,Is this Dell's answer to Cisco UCS/Hyperflex? Sounds like a nightmare.
40.json,t3_74agx8,VSAN/w Dell servers vs VXRail vs ExtremeIO - VDI Solution,2017-10-05 03:20:10,"I am also submitting this in VMware. We have a VDI solution being worked for 1000 windows 10 desktops (That will grow year after year), 2 Vcpu and 3Gb memory.   We will be utilizing linked clones and App volumes. It will most likely be an all flash solution, at least with vsan and vxrail and will only support our VDI environment. My question is on the hardware/software combo that will support.  First i want to say that i would only like discussion in regards to the above subject.  We will be going with one of the above solutions and want to get input and personal experience with them.  I personally would like to go with vsan/dell server configuration, dell is also wanting to purpose to other solutions one with VXRail and another with ExtremeIO.  They will be staying with in our budget. My concern with ExtremeIO its your standard server fiber to san solution and once you reach capacity you have to buy a whole new shelf.  To be honest, I really don't have much experience with the vxrail, and extremeIO so any input would be great. Another reason i like the vsan solution is that to add 100-125 more desktops i know i need one server and vmware licensing and im good to go. EDIT:  also from a vSAN configuration and connectivity we will be connecting all hosts via 10GB, to two 10GB switches and will connect those 10GB Switches to our Core Switches.  Should make cable management clean. Hopefully there is enough info there i will do my best to give any additional info."
40.json,t3_74agx8,t1_dnwqomx,2017-10-05 03:24:22,"I use the XIO for VDI and love it.  Deduping is around 6:1 and its crazy fast.  However, I don't have any experience with the VXRail.With the XIO, you'll still need to buy hosts, where the VXRail is all in one, right?"
40.json,t3_74agx8,t1_dnwr3el,2017-10-05 03:31:29,"So VxRAIL is basically just vSAN shipped as an appliance with a wrapper on it to look like a 'ground-up' HCI solution like Nutanix on Simplivity.You do realize that you're only entertaining 2 solutions from a single company, right?No matter, all of the above will do VDI very well."
40.json,t3_74agx8,t1_dnwrl2a,2017-10-05 03:39:58,"I wouldn’t wish VxRail on my worst enemy. Just cross it off your list and save yourself the stress and forever souring managment on HCI. If you actually want to scale your HCI system get one from a vendor  like Nutanix.Maybe, just maybe, the 3rd generation Dell hardware is better than previous generations. The current VxRail code release however offers little improvement. You’d be much better off building a VMware 6.5 VSAN stack without VxRail piece. VxRail adds a significant negative value to a VMware VSAN deployment.My source for this is dealing with 8 VxRail appliances (that’s 32 ESXi hosts) for the past 15 months and the experience of my coworkers before I joined the team."
40.json,t3_74agx8,t1_dnwrsyi,2017-10-05 03:43:40,"This is good feedback, and validates what i have been hearing, i have yet to see much of the positive responses. Thank you!"
40.json,t3_74agx8,t1_dnwrvfr,2017-10-05 03:44:53,"Yes i do realize this, reasons i don't want to get into a discussion about here.  :-)Thank you for the input!"
40.json,t3_74agx8,t1_dnwrxko,2017-10-05 03:45:54,"Yes, VXRails is the all in one.  I am assuming you have this connected via 16Gb fiber?"
40.json,t3_74agx8,t1_dnwuhtg,2017-10-05 04:30:14,"Any chance you can expand on your issues with vxrail?  Dell/Emc is pushing this hard as their preferred hci solution , with really attractive discounts etc. so my cto is really excited about it, but I haven't found much from end users to say if its decent either way.Nutanix looks really good to me but there isn't enough usage in my industry to take a gamble on it unless vxrail is a nightmare."
40.json,t3_74agx8,t1_dnwum7e,2017-10-05 04:32:22,"I have good experience with VxRail, but not with the 4 nodes 2u chassis, I worked almost exclusively with the GPU enabled nodes. VxRail 4.5, which will leverage Dell 14th generation of PowerEdge server is supposed to be the bee's knees.VSAN is getting better with each release. VxRail is a solid solution that sits on top of the VMware ecosystem and can leverage everything (VSAN, NSX, etc.). Horizon is built to leverage VSAN also.Xtreme-IO is a beast. A real beast. I have deployed some large Horizon farm on XIO. Can't go wrong there, obviously. But, compute + storage fabric + storage will probably end up more costly then VSAN ready Dell nodes or VxRail.To be honest, all these solutions are good. VxRail or VSAN ready nodes are probably more flexible. VxRail will probably be the easiest to maintain, as all firmwares and software updates are handled by the VxRail manager."
40.json,t3_74agx8,t1_dnwvg19,2017-10-05 04:46:45,"I said the exact same thing recently. We have 28 nodes. When it works it works,  but the benefits are no where close to the horrors of dealing with all the crap they tie in. Oh, if something happens to the vxrail manager, the only option is to redeploy the entire appliance. ONE vm can make the house of cards crumble."
40.json,t3_74agx8,t1_dnx1cdu,2017-10-05 06:37:32,Thank you for this info!
40.json,t3_74agx8,t1_dnx9v7f,2017-10-05 09:29:44,"Disclaimer - I work for Dell EMC. These are my own opinions, I'm not the official company spokesperson.Are you doing full clone desktops or linked clone desktops? If full clone desktops XtremIO is a powerful solution because of the deduplication engine. Dedupe is VSAN happens on a disk group level, XtremIO has a global dedupe table across all bricks in the cluster. This can drive fantastic dedupe with full clone desktops. Depending on the storage profile you could get 1000 to 3500 desktops on an X1 brick, I haven't benchmarked an X2 brick yet. We just wrapped up 20k desktop design with a customer that has gone really well.With linked clones dedupe isn't quite as important and all of the solutions (VSAN vs XtremIO) are on even footing.VxRail 4.5 Is shipping currently so with either VSAN solution the end result is going to be a VSAN 6.6 powered cluster. The difference if that VxRail includes an automation engine developed with Vmware wrapped around the node. So stand up, expansion, life cycle management, and call home are handled by VxRail manager.If you choose to build it yourself with PowerEdge servers I strongly encourage you to leverage VSAN Ready Nodes. VSAN HCL alignment is critical for success, ready nodes keep you in that swimlane.The most popular VxRail node is the E series  (1u,1node). We use it for VDI frequently, along with the V series node (2u,1node) that is designed for graphics intensive VDI (has slots for and enough power for GPUs).If you have any VxRail questions feel free to reach out."
40.json,t3_74agx8,t1_dnxacu0,2017-10-05 09:39:23,"Ugh, so the first 2 generations (vSPEX Blue, and VxRAIL) were technically not Dell hardware (They were quanta) made on contract for EMC.."
40.json,t3_74agx8,t1_dnxapgl,2017-10-05 09:46:36,"I'm not a fan of fault domains with 20K users in it. I was working on a VDI design for a customer recently where we did cloud pod, and split up the users on each side into multiple shared nothing between clusters to prevent any spill over from something going haywire. We've reached a point on density where we need to stop asking ""If we can"" but rather ""if we should"".For test Dev? Sure. 20K VM's on a brick is great (I've used XIO for Build automation testing and it's great at crazy dense duplicate scale). For anything prod, I like to consider what would happen if 10% of the impacted users tried calling the help desk at once. Do we have enough PRI trunks for that?Throw in the fact that vSAN licensing is free with Horizon Advanced, and justifying buying ANYTHING honestly gets hard."
40.json,t3_74agx8,t1_dnxath1,2017-10-05 09:48:57,Simplivity isn't really ground up (They don't own their own hypervisor).
40.json,t3_74agx8,t1_dnxaveu,2017-10-05 09:50:01,"My concern with ExtremeIO its your standard server fiber to san solution and once you reach capacity you have to buy a whole new shelf. To be honest, I really don't have much experience with the vxrail, and extremeIO so any input would be great.Didn't You have to buy bricks in pairs, or powers of two or something?"
40.json,t3_74agx8,t1_dnxd7me,2017-10-05 10:38:59,"With 1st generation XtremIO hardware you could have a half brick, full brick, two bricks, four bricks, six bricks, eight bricks. Scale-Out architecture, adding X-Bricks adds both capacity and performance.For VDI I most often saw half bricks, full bricks, and two brick clusters. Keeping things within a manageable fault domain.The new generation, XtremIO X2, support one to four bricks in a cluster. Each brick is two storage controllers, and one SSD shelf. XtremIO X2 provides flexibility of both scale up and scale out. You can scale up the amount of storage by adding only disk to the brick or bricks in the cluster, or you can scale out the performance of the array by adding additional bricks to the cluster."
40.json,t3_74agx8,t1_dnxdfbv,2017-10-05 10:43:28,"You can buy just one X-Brick, or even a half Brick."
40.json,t3_74agx8,t1_dtf15ct,2018-01-30 00:36:08,"If you were to go down the VSAN route on VSAN ready nodes, what toolset would you use for any automation etc? Does vRealise have this built in?"
41.json,t3_4dwmve,VCE VxRail: Drive specs? Usable capacity?,2016-04-08 22:31:15,"Can anyone tell me what the drive specs are for a VCE VxRail system?  Data Sheet only shows RAW capacity which is of very little use. Also, for the hybrid models it doesn't tell the quantity, size or type of HDDs. SATA drives or SAS drives? How many drives per node? What's the RAW to usable capacity? They're being very vague about this. They force you to engage a sales rep to learn more. I don't need a VCE sales rep hounding me when all I want is a simple answer to a simple question."
41.json,t3_4dwmve,t1_d1v2tjl,2016-04-09 00:23:38,"Thats because usable capacity is up to your choices on FTT.  Remember, its VSAN, so you can choose on a per VM basis how many copies of a given VMDK to have - 1 to 4(?).  That directly impacts your useable space, so it can't be spec'd clearly.Check this post (especially the tables in the middle): http://virtualgeek.typepad.com/virtual_geek/2016/02/vxrail-an-incredible-product-at-an-incredible-time.htmlAlways SAS drives.  6 drives per node.More good info here: http://www.dell.com/us/business/p/vmware-vxrail/pd"
41.json,t3_4dwmve,t1_d1vff82,2016-04-09 05:00:20,"Let me give you a word of advice, definitely look in to the all flash. I know that you're thinking the cost might be too much, but for the third time in the last few months I have found that the cost is almost negligible. A few grand difference. Especially when you get the added benefits of the All Flash VSAN and what it provides to you.Do try to go with at least 5-6 Nodes if you can, 6 preferable to do the RAID 6 and FTT=2 setting. That would put you at 2 chassis. http://www.yellow-bricks.com/2016/03/01/vsan-6-2-going-forward-ftt2-new-default/"
41.json,t3_4dwmve,t1_d1zo7p9,2016-04-12 19:49:03,RAID 5/6 with a 2-3x dedupe and compression factor is cheaper than hybrid RAID 1 typically from the numbers we've run.
42.json,t3_74altj,VSAN/w Dell servers vs VXRail vs ExtremeIO - VDI Solution,2017-10-05 03:40:01,"I also submitted this in Sysadmin. We have a VDI solution being worked for 1000 windows 10 desktops (That will grow year after year), 2 Vcpu and 3Gb memory.   We will be utilizing linked clones and App volumes. It will most likely be an all flash solution, at least with vsan and vxrail and will only support our VDI environment. My question is on the hardware/software combo that will support.  First i want to say that i would only like discussion in regards to the above subject.  We will be going with one of the above solutions and want to get input and personal experience with them.  I personally would like to go with vsan/dell server configuration, dell is also wanting to purpose to other solutions one with VXRail and another with ExtremeIO.  They will be staying with in our budget. My concern with ExtremeIO its your standard server fiber to san solution and once you reach capacity you have to buy a whole new shelf.  To be honest, I really don't have much experience with the vxrail, and extremeIO so any input would be great. Another reason i like the vsan solution is that to add 100-125 more desktops i know i need one server and vmware licensing and I'm good to go. Hopefully there is enough info there i will do my best to give any additional info."
42.json,t3_74altj,t1_dnwtenn,2017-10-05 04:11:19,"vxrail is basically vsan ready nodes and everything is licensed ect.  Dell/EMC has a little sheet that asks you all kinds of questions and will spit out a suggested vxrail config.  There are some nice things to it, like they have an ""easy button thing""  basically if you build your config with this in mind when a new version comes out you can go to the vxrail management interface and give it the files and say upgrade and it will do all the work for you. (upgrade your vcenter, esxi hosts, vsan  ect ect.)  When we looked at it, it was more expensive than building it your self so.....  I guess it just depends what you are looking for operational wise.If you are going to look at arrays i would suggest you look at a Pure Array.  We own both XIO and Pure, if we had to pick only one i would go Pure.  If you tell the two vendors they are up against each other they prices start getting real competitive."
42.json,t3_74altj,t1_dnx1et2,2017-10-05 06:38:54,"Yea, I have some experience with Pure, and I really liked it!"
42.json,t3_74altj,t1_dnx1t2v,2017-10-05 06:46:40,"VxRAIL includes an OEM license for Enterprise vSAN, you still need to bring your own vSphere ESXI license (although Horizon has that and can cover it).It also includes a included OEM vCenter (or you can use an external like you would be entitled to with horizon) and an OEM license for LogInsight (for use with the vCenter and hosts).I'm not sure why you would pay for Pure, or XIO given that vSAN Advanced licensing is included with Horizon Advanced and up. I don't think they can beat ""the cost of a few drives in a server"".  Deploying Horizon on anything that isn't vSAN just seems really weird at scale. A local hospital is doing a 20K+ user roll out right now and it's going well I hear."
42.json,t3_74altj,t1_dnx951o,2017-10-05 09:14:56,"Do not buy FX2 or Intel X710 NICs. Those two plus vSAN 6.2 create the PSOD Hold Trinity. FX2 is just a mini-blade chassis with extra crap to cause problems and you never know if they're going to change their minds on the platform; they already had to reduce the number of DIMM slots on the FC640 from the FC630 because of the larger heatsink for Skylake. The Intel X710 NIC is another big monkey wrench with vSAN and we're still waiting for Intel to get their driver shit together.One of the big things GSS has said for the past six months and every VMware person at VMworld this year told us is we need to upgrade to vSphere 6.5 and vSAN 6.6. This is their goto solution for all of our vSAN headaches and claim will bring about stability and mass quantities of rainbows and unicorns. Just speaking of vSphere 6.0 at VMworld had them cringing or hiding their face; we never got a straight answer why 6.0 was borderline taboo.Overall vSAN is a great idea and can be cheaper than regular SAN or the other HCI solutions (Nutanix is very proud of their software $$$) even when including licensing. But vSAN needs you to be on the ball. You have to stay on top of firmware and drivers for everything. When it works, it works great and you get stupid IOps with if it's all flash storage that may just finally shut-up the DBAs. When something is wonky, you get PSODs. Not just one PSOD in a blue moon, but a bunch and sometimes more than one Host will PSOD at a time. You need to do the Health Check almost daily. Don't be surprised if there are alerts about VASA providers being gone because the vsanvpd stopped working on most of the Hosts and can only be started via SSH with no PowerCLI options.They want vSAN to succeed. They have plans to include better checks on firmware and drivers so you don't have to check the HCL daily or end up with another PSOD. They plan on reducing the need to look at Health Check daily. They also want to move away from one relying on the Ruby vSphere Console (RVC), which one has to SSH into the VCSA to get to, for more detailed vSAN health checking that should have been in the web client in the first place.My recommendation is to avoid FX2 and Intel X710 NICs, run vSAN 6.6, and get them to lend you hardware for POC. Do an in-depth POC and thrash the shit out of it with the kinds of workloads you plan on doing. HCIBench isn't going to tell you a complete story when POCing."
42.json,t3_74altj,t1_dnyaw51,2017-10-06 01:39:20,"The FX2 with Intel v3 had a purple screen bug (BIOS update fixed it, had something to do with PCI-Express checksums). That was a known/resolved issue within a month of the V3's coming out.The Intel X710's have had multicast retransmit, and LSO/TSO offload bugs for years (this impacts regular storage traffic too BTW, not just vSAN).  A script to disable LSO/TSO has floated around for a very long time to mitigate this but I've been told by Dell's largest FX2/OEM Customer that this was fixed in the newest Intel driver/firmware release earlier this year. (feel free to test it out).Here's the script https://github.com/jasemccarty/Vsan-Settings/blob/master/Vsan-SetTsoLro.ps1Let me try. As far as 6.0, it's not that bad most of the common bugs were fixed by patch 5 (I just saw an express patch go out today), but significant improvements to how rebalances, and rebuilds work went into vSAN 6.6. If you want to know more watch the vSAN day in the life of the I/O deep dive, and skip the boring part with John at the front for Pete at the end. he gets into the meat of smarter rebuilds being based on component, not object. This can mean on an object with 4 component stripes or chunks 75% less data involved in a rebuild if 1 component is damaged. It's a huge improvement in low-level performance and balance. LSOM will also chunk a component into smaller chunks during a rebalance. This combined with other LSOM placement tweaks leads to a lot lower backend overhead on clusters.For VDI, I'd go spin up a Linked Clone/Instant Clone (Whichever you plan on doing) pool and have users log in. I would POC ACTUALLY running VDI on it. The FX2 isn't so bad with the above-mentioned patches and tweaks (although the Qlogic NIC's work just fine too). I know large airlines and chemical plant control systems who use FX2 and vSAN and are more than happy with the above tweaks. I've deployed VDI on vSAN and the performance was amazing, and it survived failures like a champ. VMware internally uses it (it's 10x lower latency than our previous hybrid array platform from what I've seen as a user), and massive banks, hospitals and other customers use it for VDI.They want vSAN to succeed. They have plans to include better checks on firmware and drivers so you don't have to check the HCL daily or end up with another PSOD. They plan on reducing the need to look at Health Check daily. They also want to move away from one relying on the Ruby vSphere Console (RVC), which one has to SSH into the VCSA to get to, for more detailed vSAN health checking that should have been in the web client in the first place.The vSAN performance service gets rid of the need for 99% of the RVC and observer. (Observer is deprecated technically as the new VMware hosted performance analytics takes over GSS and engineerings need for it).The driver/firmware integration first piece being integrated into VUM was nice, but Watch Christian Dickman and Junchi's VMworld presentation if you want to know where things are going. Christian did this session last year and everything they showed off shipped by this year so we'll see if he can do it agian.BTW, the NIC issues symptom you mention (retransmits, and packet loss) will show up in the vSAN performance service (6.6.1, and possible 6.6  tracks packet loss). We are looking at adding a Health Alarm for this. I was testing it against phone home performance telemetry last week. The idea is we test new health checks in our ""cloud"" of phone home data first (that GSS has access too) and as we can refine out good KB's, root causes, and get rid of false positives roll it out to customers in the form of a phone home recommendation. A lot of vSAN issues, are fundamentally vSphere issues at the end of the day (If your NIC's don't work, or host clocks are all off, even without vSAN you are going to have a BAD day). If you look at where VMware is going with the product improvements, and support and phone home improvements it's about improving the entire platform to become more resilient and easy to support.A quick podcast I'd recommend you listen to. Grab episode 57, and skip to ~26 minutes in, the Christian Dickman interview. https://itunes.apple.com/us/podcast/virtually-speaking-podcast/id1084076135?mt=2 (Yes this is also on the android play store too)."
42.json,t3_74altj,t1_dnyu39n,2017-10-06 07:25:27,"For the number of users, you’re probably best going with VSAN ReadyNodes and Horizon Enterprise over vxRAIL unless you can get Dell to decouple the OEM license costs from the hardware.  You’ll probably want Horizon Enterprise to get instant clones as well as UEM & AppVolumes to make environment management & physical overhead less, and Horizon Enterprise already comes with vCenter, vSphere Ent+ for Desktops, and VSAN Advanced in the bundle.  With that many users, assuming you haven’t bought the licensing yet, you should easily qualify for deep discounts with VMware via an Enterprise License Agreement too, making it even less advantageous to get into vxRAIL’s bundling.You could also potentially look into adding in NSX and going with a Cloud Foundation stack added to Horizon Enterprise - the SDDC Manager from Cloud Foundation provides similar lifecycle functionality of the vxRAIL manager, and the bonus would be having NSX for robust granular security throughout your end user networks."
43.json,t3_cbxei9,Upgraded VXRail from 6.5 to 6.7 . Now Horizon 7.7 isn't playing nice,2019-07-12 00:05:22,"Trying to manage\learn a new VXrail 4 node Cluster that I inherited. I upgraded VXRail from 6.5 to 6.7. no issues there, everything went great. But now our consultant for VDI is saying that provisioning for instant clones isn't working. Horizon wasn't updated and is on version 7.7. Any advice of a course of action? Sorry. Meant to reply. My lack of answers to your replies showed me that I needed to open a ticket with VMware. I will update the post with a solution when I get it. Thank you. Update. So the VXrail updates went fine (it's actually a really nice process, moves, updates and reboots everything in the correct order) but the original setup team built two connection servers for redundancy. It was reporting that both servers were running 7.7 but when looked at individually one was 7.4 and the other was 7.7. The 7.4 one wasn't compatible after the vxrail update to 6.7. So that server was updated and all is good."
43.json,t3_cbxei9,t1_etj37qa,2019-07-12 01:15:09,Hard to say without more details on what isn't working on the horizon side.
43.json,t3_cbxei9,t1_etj4gzc,2019-07-12 01:27:06,Need logs and errors out of Horizon
43.json,t3_cbxei9,t1_etj7hqe,2019-07-12 01:54:59,"Get the logs from the connection server(s), probably programdata\vmware\vdm and check / post here. Suggest if you have a Horizon consultant he needs to give you more info than “it’s broke” tho"
43.json,t3_cbxei9,t1_etjchlc,2019-07-12 02:45:31,Does the provisioning error say that it's (missing) ?
43.json,t3_cbxei9,t1_etjdf9e,2019-07-12 02:54:56,https://docs.vmware.com/en/VMware-Horizon-7/7.6/horizon-upgrades/GUID-1A13F6F4-F9B6-4B3E-97AA-B2077E76EC4E.html?hWord=N4IghgNiBcIG4GcAOALApgJzQAgGwDoB2EAXyACheck this
43.json,t3_cbxei9,t1_etjdxmn,2019-07-12 02:59:54,"Did you upgrade vCenter as well? I have seen provisioning issues with instant clones when the VCSA isn't the same version as the hosts.Connection server logs will be your best bet, however."
43.json,t3_cbxei9,t1_etkmd02,2019-07-12 11:24:19,the horizon vCenter instance probably isnt happy.  Upgrade it.
43.json,t3_cbxei9,t1_etndp3e,2019-07-13 10:15:06,What version of the VXRail is installed? With VXRail you can't apply vmware updates as before. It has to be all done in the VXRail manager.
44.json,t3_5sxpbb,test,2017-02-09 11:42:32,"With Jadwiga released and the promise of multiple leaders for one same civilizations, I’m excited to hear what else could be coming next. As for me, I’ll try and reduce my nominees to only those I have a sort of idea how they’d work, but I would mostly like to see:   France Napoleon Bonaparte Bricks of Triumph UA: Whenever a military unit you own wins a battle, a small percentage of production is given to a currently building wonder.** Napoleon should have a bonus centered on military and wonder building to both fit his legacy and France’s Grand Tour ability. I think this kind of bonus would be reminiscent of Napoleons decision to build the Arc de Triomphe after his victory at Austerlitz. You could even add the Arc itself as a wonder.   England Elizabeth I Swashbuckling Epics UA: Whenever a city is conquered with a naval unit within 5 tiles, that city earns a Great writer. As the monarch who legitimized privateers before anyone else (or at least the first to do so with said name) she should have a bonus that makes it so she outranks Vicky when it comes to Sea Dogs and navies. And what better than a culture one, to go alongside British Museum, and one that reflects her love for plays at that.   Richard I The Third Crusade UA: Whenever you declare a holy war against an enemy, other civilizations may choose to join you. For every civilization that joins you in the holy war, the warmonger penalty of both you and the civilizations that joined is reduced by 25% (does not include capturing cities). Whenever you capture a city which does not follow your religion, that city gets a relic as well as a free slot in it’s the city center. Richard Lionheart managed to make many join his cause to take back Jerusalem, and his UA makes you capable of the same. Of course, capturing cities still gives you warmonger penalties, so that if you keep calling form more holy wars, at some point someone is going to call you out on it. However, your “allies” are still affected by warmonger penalties from captured cities as well, so if they are following your religion or you simply don’t like them, you can use their cruel acts on the crusades as an excuse to later on go to war with them (or maybe just because they captured the city you wanted during it). The relic bonus also boosts your cultural victory and religion, fitting both this leader and his nation’s UA.   Japan Ieyasu Tokugawa Sakoku UA: You may not open borders for any civilization. International trade routes provide bonus science for your cities. Gain the Ninja Unique Unit when you get the Feudalism civic. Any enemy religious units in your territory lose health per turn Sakoku was a policy which dictated that no one could come in or out of Japan except for some traders in some ports. At first I thought this should make Culture Victory impossible, like with Mbemba and Religion, but if Gandhi can nuke Vicky then, hell, why not? The Sakoku was initiated because of fear of religious conversion, hence the enemy religious units loosing health. Essentially, Tokuwaga does not have a lot of bonuses for winning the game, but for preventing the enemy from winning trough culture or Religion. Ninja UU: Can only be obtained by upgrading a scout. It can’t be upgraded into ranger. Invisible to all enemy units unless adjacent to them, except when in hard terrain. When it pillages a tile, the tile’s owner can see the ninja until it leaves their territory. When attacking an enemy unit from invisibility, it deals 100 damage, minus the target’s strength, and turns visible until it leaves line of sight. All Ninja units are lost when getting the Industrial age Melee Strenght: 10 Movement: 3 Maintenance: 1 Gold Oh come on, ninjas in civ would be awesome and you know it. They were assassins, hence their damage. It might sound like a lot, but once you hit someone, you are exposed, so you have to choose your targets, better if they are secluded. Also, you can never kill a unit in one hit due the damage total, even if it’s one weaker than the ninja. They were spies, so you can plant them in an enemy city and observe them. They were saboteurs, so you can infiltrate a city and pillage an important tile. I don’t know how much I’d charge to upgrade the scouts though.   Carthage I won’t give any numbers here as I think that needs play testing beforehand Mercenary Army UA: Military units purchased with gold cost extra gold maintenance per turn. You may purchase military units with gold for less. It’s well known that Carthage preferred hiring mercenaries to training soldiers, but this brought problems sometimes as they could find themselves in occasion not having money to pay them with. This bonus puts you in a situation where you shouldn’t get greedy, but can amass a nice army through gold. Cothon UD: Replaces the harbor. Domestic trade routes from this city get bonus production, while international trade routes get bonus gold from this city. Enemy naval units which start their turn in the same tile are damaged. Carthage was characterized by its economic power, so that’s what their UB should provide. During the Roman invasion it was said that the Harbors had many inventions, supposedly created by Archimedes (we are talking cranes that lifted and dropped ships and mirrors that made sunbeams to turn enemy vessels on fire bruh!) UU: (I want to say War Elephants, but since Gandhi’s already reduces the strength of all adjacent unit, the fear factor that was present in Civ 5’s elephants is kind of lost. So yeah, either that or a ship, or something.) Hannibal Barca Oath Driven UA: Military units may pass through mountains. Units receive 50 HP damage if they start their turn on Mountains. Whenever an enemy unit enters one of your unit’s zone of control, your unit automatically attacks it. If your unit wins, the enemy unit must pull back to the previous tile. So the first part is very similar to Civ 5’s Dido’s UA, while the second part I think is a nice way of capturing the way Hannibal defeated the Romans many times: by surrounding them and preventing them from escaping.   Now, seeing as this is a huge post and possibly by this point you’ve ceased to read what I’ve written, I’m just going to put my country (because of course I would). Again, like with Hannibal, numbers require playtesting. Argentina Immigrated Workforce UA: Your cities obtain a percentage of your international tourists in population. During the 19th to 20th Century, Argentina invited many Europeans to come to the country to take control of the land in the inner provinces, in an attempt to invite settlers into developing this areas. The plan backfired, however, as most of the people who came decided to stay in the city and work there. Coventillo UD: Replaces the neighborhood. Bonus Production when adjacent to an Industry Zone. Provides Tourism. The immigrants who came resided in this cramped buildings that would be close to the industrial zones for quick access to work places. In the present day, they are still used as residences, though not with the same conditions of population, and are considered a visiting spot of the capital city, Buenos Aires. Granadero UU: Replaces the musket man. Bonus Strength when fighting an enemy you have declared Liberation War or Protectorate War. Jose de San Martin Generalísimo UA: Military Units recive 1 bonus movement points when starting on hills or adjacent to mountains. When Military units you or your allies own die, all adjacent units you or your allies own are healed. Founder of the Granaderos regiment, he took inititative to liberate Chile and Peru by crossing the Andes with his troops. It was a similar campaign to that of Hannibal, so sadly I could not give him the same bonus, mostly because Hannibal is better known for said feat and did so considerably earlier (and with elephants…). In a very important battle, San Lorenzo, he was wounded and trapped beneath his horse. He was almost killed had it not been by Juan Bautista Cabral, who died saving him. The bonus also affects allies since San Martin was allied to Bolivar, who is also considered one of the liberators of Latin America, and whose help was invaluable.   So, thanks for reading my pool of text. I realize it’s a lot, but I encourage you guys to write your own ideas and wishes for the game."
44.json,t3_5sxpbb,t1_ddkppai,2017-02-10 23:00:32,"VAR Implementation SE here. I guarantee that the 7000 VSAN implementations are not all in production. VMware has been pushing VSAN hard since it was released, for good reason it's an emerging market and they have the hypervisor space dominated. They need to show growth somewhere and NSX/VSAN are where they are pushing. If you bought the Uber Platinum Horizon Suite in the last few years, you are considered a VSAN customer. If you didn't look over your ELA closely, they threw it in as a zero dollar line item for a few sockets, you are a VSAN customer.If your SE said he hasn't seen anyone set FTT=2 then no disrespect to him I would question his experience and research into the underlying VSAN technology. There has been constant debate and blog posts about FTT=1 vs FTT=2. If you ask me, the amount of variables that could go wrong during a standard host patching or vib install operation is just too high for me to be comfortable with FTT=1.Soure: http://www.yellow-bricks.com/2016/03/01/vsan-6-2-going-forward-ftt2-new-default/Source: http://www.vclouds.nl/watch-out-with-vsan-ftt-1-and-maintenance-windows/As for VxRail specifically, I just installed one last week and it blew up on the launch pad. The initial install went fine, albeit a lot longer than they market, it took the majority of the day with planning, DNS entries, racking, cabling, updating and initializing. After we got it up and running it was just performing horrible, we called VCE so the customer could see how his support experience was. And that's where it blew up IMO. The customer was working with VCE directly trying to get the issue resolved. We installed the VxRail on 1/26, on 2/3 VCE had officially punted blaming the network 100%. We came back in to help troubleshoot the network, we went through basic troubleshooting of moving cables to narrow down what port/cable it could be. The solution was that it was Port 1 on the NIC in Host 1. NOT the network... Now it's taken a few more days to get the host RMA approved with it scheduled to be replaced today on 2/10. Now after reading this story of their environment being down 1 host for over 3 weeks, and knowing how FTT=1 vs FTT=2 works, are you comfortable with FTT=1?More on VxRail. Do you like using VUM to stay up to date on patches? VxRail doesn't support it and doesn't plan on it. You are supposed to do all your updates via VxRail from VCE. Also the Day 2 operations inside VxRail are disappointedly all pulled via vCenter. There is no IPMI magic, it just matches vCenter objects to VxRail objects and presents them in a very pretty ECS (Eye Candy System). You will see alerts in vCenter before you will see alerts in VxRail.I know the Dell/EMC/VCE evangelists will say how great VxRail is but our job is to separate the signal from the noise. VxRail isn't perfect but it's not terrible either. You just need to do your homework and weigh your risks vs rewards vs costs. An all flash V-node is one of the great use cases for it. Properly configured VDI with stateless desktops can withstand host failures a lot better than traditional general purpose or high performance.For anyone looking at a properly sized VxRail/VSAN infrastructure. I would challenge you to do your due diligence and compare it to a traditional 10Gbps iSCSI/NFS infrastructure with an All Flash Array and stand alone hosts. You will need the 10Gbps ethernet either way. You will get better data reduction on an AFA vs VSAN because VSAN is only on a per disk group."
45.json,t3_9xpl4v,Considering VxRail...,2018-11-17 03:58:40,"I'm helping a small shop that has 70 or so VMs on six old Dell R810s, all connected to an old VNXe SAN with around 50TB or so of 7.2K drives. Just general purpose apps, no heavy hitter database or transactional stuff. Considering a VxRail solution to get them into the current century. What does the groupmind think?"
45.json,t3_9xpl4v,t1_e9u8z6m,2018-11-17 05:08:05,[deleted]
45.json,t3_9xpl4v,t1_e9ucmko,2018-11-17 05:54:48,"VxRail is great to deploy vSAN in a curated relatively low risk way. The down side that to do VxRail  properly to give you the redundancy and protection (and ditto again you you are applying an update and are updating a rail) you need quite a few rails which pretty much prices it out of the market for small installs where something conventional conventional like rack servers with a small storage array would do the same thing with much less risk and alot cheaper price point.Remember. You can stand to lose almost everything, apart from.the storage!"
45.json,t3_9xpl4v,t1_e9ucmzq,2018-11-17 05:54:56,[deleted]
45.json,t3_9xpl4v,t1_e9uey2p,2018-11-17 06:24:22,"Run a LiveOprics report. I suspect 4-5 (maybe even single socket) nodes would work, driving down costs, power, footprint and maybe open up budget for a small DR cluster."
45.json,t3_9xpl4v,t1_e9uj3n8,2018-11-17 07:18:50,We have a 4 node and 3 node G series cluster. We are just about to buy a new 5 node E series cluster. Generally it works well.We did have a drive with read errors which caused havoc in vSAN but Vmware tech support were quick to diagnose that and we are replacing the faulty drive.The support from Vmware and Dell EMC has been really great with the VxRail.We have 75 VMs on the 4 node G Series cluster. I don't think 50TB usable space is possible with hybrid on G or E series on 6 nodes. You would have to go all flash.
45.json,t3_9xpl4v,t1_e9vpgyo,2018-11-17 21:34:37,"Max Usable Capacities per platform for a 4 node cluster configured for HA (N+1) Using FTT=1.Hybrid:S570 - 12x4TB = 64.4TBP/V 570 - 20x2.4TB = 63.9TBE560 - 8x2.4TB = 25.1TBG560 - 5x2.4TB = 15.3TBFlash:P/V 57F - 20x3.84TB = 103.6TBE560F - 8x3.84TB = 40.8TBG56F - 5x3.84TB = 25.1TBRe the S nodes, at this max config they would need 1600GB Cache Drives due to the Cache to RAW Ratio of 3.6% using 800GB drives."
45.json,t3_9xpl4v,t1_efjrfju,2019-02-02 05:13:44,I had a 4 node V Series cluster.  I ran around 100 VMs on it with plenty of room for growth.
45.json,t3_9xpl4v,t1_egdjxlh,2019-02-13 19:59:37,"I may be a bit late to the show, but we have a 4-node E series VxRail cluster (which we decided to buy after doing some calculations and finding out that Azure is not only complete garbage but also expensive garbage) and holy shit does that thing just work great.I've read quite a lot of complaints in here about VxRail, but so far our experience with it was smooth.Since I have a shit ton of other stuff to do and projects to work on than play with vSAN, firmware upgrades and other things, VxRail is great in many aspects, some of which are:1.) Redundancy. Obviously, the cluster is robust, everything is doubled, backed up. The VxRail orchestration takes care of basically everything. We have critical systems on our cluster and their down-time is not an option. I sleep the emerald-slumber-calm-sleep when it is running on VxRail.2.) Power. This is mainly of course due to our configuration - 4x Xeon Gold 6138, 512GB RAM, 32TB of all-flash. Everything fucking flies on that thing. 30+ VMs with some being heavy-load databases, some are compute-heavy app servers, reverse proxies and webservers with big e-shops, RDS servers, you name it. The average cluster load sits about 10% for CPUs and 25% for RAM. When we did the initial heavy testing, the vSAN pulled off +370K IOPS and that was way before its limit.3.) Management. VxRail Manager + vSphere + Enterprise iDRACs. Whatever you want to know about your cluster, you know it. Whatever you need to configure, you can. The options are endless.4.) Support, updates and their functionality. We've had some available updates of ESXi and BOSS cards. One e-mail to EMC. Everything went perfectly. Due to the architecture itself, the update of anything goes as follows. VMs on updated node traverse with zero down-time to another node. Updated node shuts down, updates are applied, checked, node is booted up. VMs traverse back and another node update begins. During the update, whole cluster is running like nothing is happening.5.) DRS - the load balancing of the cluster is seamless. The nodes are used evenly with VMs being distributed across the cluster to spread the load.The VxRail is also not great in other things :-) Some of which are:1.) Price. It is seriously costly. However, we were able to get some nice discounts from Dell, with bigger clusters almost automatically getting bigger discount per node. VMWare licenses are expensive, the hardware is top-notch, therefore expensive. Yeah, it is not for people working on small budget2.) False positives. As mentioned a few times here, you will find your vSphere spewing some false positives. Since they are identified as false positives by EMC, I could not care less...To summarize it:If you're the kind of guy who likes to play with his Mikrotik, uses SSH for everything, works on a tighter budget and your job is only to maintain the servers, then VxRail is not for you. I am convinced that everything I have mentioned above can be done on cheaper systems without the automated orchestration of VxRail tools. VxRail is expensive, to some degree it is overbuilt and not for everyone.HOWEVER.I do not have the time to spend half a year studying and creating the same functionality with third-party tools from 7 different brands, half of them with no support, and spend an hour every day to maintain it.Therefore, I have VxRail and I am happy with it."
46.json,t3_ceyd8f,Hyperconverged system vs classic SAN+Server for medium-sized company?,2019-07-19 05:14:43,"Hello guys, I have the honor to replace the whole server infrastructure for my company. Right now we have 4 old tower servers(up to 15years old). I want some rack sized new ones and maybe even Zen2 processors. What we have: 2 DC´s + 1 Exchange + 1 telefon system + 1 ERP + 1 Ubuntu for Intranet + 1 AdminServer + 1 small terminal server The fileserver and backup server is a Synology rs1619xs+(HA) which I don´t want to change because I love the software that synology provides. Everything is backed up to two additional small Synology NAS and and to Amazon(don´t worry). What we will get soon: 1 DMS server + 1 MS SQL server + 1 Insiders OCR server + new ERP system + 1 warehouse management system + possible management systems for our new building How many users do we have right now: 56 but I believe by the end of 2021 it will be at least 70. We are going to move into a new building(2 server rooms in seperate buildings). For the future we want to have HA for all of our other servers too(right now only Synology). I did talk to some IT system houses. Some told me I should go for a Hyperconverged system like Nutanix. Others told I should get myself a classic SAN like the HPE msa 2052 or HPE Nimble. So guys, what would you suggest we?"
46.json,t3_ceyd8f,t1_eu5uap1,2019-07-19 05:31:51,I'm sold on 3 Dell servers and a hybrid or AF Nimble.  I prefer VMware but HyperV has caught up.  Rock solid performance and not a second of downtime the last 7 years.We currently use Veeam to replicate critical systems every 3 hours.
46.json,t3_ceyd8f,t1_eu5w6sq,2019-07-19 05:52:17,"For something that small, I suspect you'll find hyperconverged costs way more money than buying a pair of hosts and a small SAN."
46.json,t3_ceyd8f,t1_eu5yab5,2019-07-19 06:15:41,"Dell is beating down our door to replace our FX2/SAN deployment with VxRail. They've yet to give an actual reason why other than, ""that's where everyone is going.""Whatever you decide, don't feel pressured."
46.json,t3_ceyd8f,t1_eu61q1m,2019-07-19 06:50:32,"I was in a similar spot last year and we went with a Unity SAN, with blades. The cost of HC is extreme when it comes to data storage. It is great for stable sized work load (VDI) but falls flat with any expansion needs.There is also a reason Nutanix is losing market share after getting out of the hardware game. If you are going HC I'd go VXRail."
46.json,t3_ceyd8f,t1_eu6mjsv,2019-07-19 10:35:48,"We were evaluating updating all of our Hyper-V FoC clusters with HPE hardware (DL380s and 3PARs with Peer Persistence/Replication), coming from HPE hardware, too. Some sites had MSAs, some were standalone, etc. Its a PITA to manage.Yesterday we learned about HPE Simplivity and Nutanix. We are now interested as it seems we can spend less money with that. We'll see what they size it, but in paper, we could replace all of our clusters with that and do DR / Backups in the same box."
46.json,t3_ceyd8f,t1_eu70haj,2019-07-19 13:27:56,"First, be very aware of Microsoft licensing per core. Second, if you can, consider replacing exchange with O365. you're a small shop and geographic redundancy doesn't sound like something you're able to achieve.That all said, get the hardware requirements for all of your software, current and future. We are in the process of implementing our first ERP system, and it wants 70 CPU cores and 240 GB of RAM. Storage IOPS is another metric you need he's Des capacity. We are a 330 user company for reference.In 2 years my SAN will need replacement. I'll be weighing HCI myself. The single dashboard is appealing. But I'm not sold yet. I don't have crazy growth and need to scale. If you can predict a 5 year trend, then see how the numbers shake out and go from there."
46.json,t3_ceyd8f,t1_eu8w9si,2019-07-20 03:55:45,This.  hyperconverged is expensive and probably doesn't provide much benefit to you.
46.json,t3_ceyd8f,t1_euch3wo,2019-07-21 10:08:54,I’d look at Nutanix. Cost is always a critical factor but service and support was important for us. Nutanix has net promoter scores over 90 for 5 years. That pushed us to move that route. Scale up and scale down as needed especially as they have moved to subscription model. Our last vendor we had issues and while cheaper we spent more time resolving.
47.json,t3_5urqz7,Dell EMC Announces it Will Launch Hybrid Cloud on VxRail Appliances,2017-02-18 17:34:13,[]
48.json,t3_9cxxk6,New Dell EMC VxRail Sub,2018-09-05 01:41:36,"Greetings! Our organization just switched from a traditional VMware stack to HCI (VxRail). I found that there was not a VxRail sub, so I set one up. I'm trying to get the sub off the ground and would appreciate anyone running or thinking about running VxRail contributing. vxrail Thanks"
48.json,t3_9cxxk6,t1_e5e1oqj,2018-09-05 02:33:30,That subs been around a while.
48.json,t3_9cxxk6,t1_e5e1tk6,2018-09-05 02:35:29,"Yes, and didn’t get much traffic... Hence my request to mods to post here to get more interest."
48.json,t3_9cxxk6,t1_e5emf20,2018-09-05 07:51:34,Thanks for the heads up.  I have been working with vxrail clusters for about a year now and it would be great to have an active subreddit for the subject.  Just subscribed and commented on a couple posts!  Maybe I can talk the other 2 guys in my shop into joining as well haha.
48.json,t3_9cxxk6,t1_e5ems84,2018-09-05 07:57:43,"emc2 is for all EMC based products, lots of employees around too"
48.json,t3_9cxxk6,t1_e5enp9e,2018-09-05 08:13:28,I'd really appreciate that!
48.json,t3_9cxxk6,t1_e5epjbw,2018-09-05 08:44:39,"We just stood one up for DR, I’ll be involved."
48.json,t3_9cxxk6,t1_e5er1n0,2018-09-05 09:10:04,"Just curious, what's your overall opinion on the VxRail so far?  We've deployed 6 around the world, so far.  I'll save my own thoughts on it for now.  ;)"
48.json,t3_9cxxk6,t1_e5es37k,2018-09-05 09:27:17,Vxrail has horrible reviews all over sysadmin. I crossed them off the list just from the reviews. That and Dell is horribly confused with its storage. They push several different product lines and will bad mouth each other.
48.json,t3_9cxxk6,t1_e5eyrbq,2018-09-05 11:21:44,EWWWWWWWWWWWWW.
48.json,t3_9cxxk6,t1_e5p0tam,2018-09-10 07:59:09,"FWIW if all you have is a hammer all problems look like a nail. The use case for Isilon (massive scale, high throughput NAS) doesn't really overlap with a VMAX (FICON support, 3x way replication 99.99999% uptime)."
49.json,t3_bhk9nx,From Converged Vsphere/Blade/Netapp to Hyperconverged Nutanix/AHV,2019-04-26 17:16:05,"Management did want to get rid of high Vmware license costs. Decision made to switch to Nutanix with AHV a year ago. My summary and personal opinion: After  all if you use Nutanix/AHV expect to hit more bugs than when running  Vmware with traditional environments. It's simply not that much of a  mature product as traditional converged architectures/environments which have been on markets  since 15-20years. Nutanix with AHV has great potential - but it's not yet there. And if Nutanix is not there yet i wonder were the minor marekt players like Netapp HCI or Cisco Hyperflex are, probably nowhere...ok there you might run it anyway with Vmware Vsphere/Hyper-V"
49.json,t3_bhk9nx,t1_eltibr7,2019-04-26 17:35:35,Thanks for your feedback. What is the size of your cluster/number of  VMs?
49.json,t3_bhk9nx,t1_eltiktg,2019-04-26 17:41:58,It was (luckily...) not a migration project. We run 700+ VMs on the old converged Vmware Infrastructure on different clusters - biggest one has 15 Nodes. The new Nutanix/AHV stuff is for new VM's and is currently simply one block with the minimium of three Nodes and will now grow as needed.
49.json,t3_bhk9nx,t1_eltju85,2019-04-26 18:12:06,"Besides VxRails and ""normal"" servers with vSphere we also have a NetApp HCI and a Cisco Hyperflex cluster. We got them for free and they only serve as lab and test platforms.Because ESXi runs on it, it's not really different from traditional servers without HCI. They just have a few additional HCI-like features. With Cisco Hyperflex it is the storage layer that is managed by UCS Manager and works similar to vSAN and with NetApp HCI it is the integration with Solidfire that makes the management and provisioning of datastores easier. But in the end, you have datastores that work just like normal SAN datastores, the network is deployed over NSX, and otherwise it's a normal vSphere environment that is managed just like ESXi hosts without HCI. I don't think you can really compare these two solutions with real HCI solutions like Nutanix or VxRails. It's more like HCI Light. :DVxRails on the other hand are very similar to Nutanix with their own Management Layer, Easy Install, Scale and Replacement, Automatic Upgrades of the whole Firmware, Driver and ESXi, etc.. Basically like Nutanix with vSphere. Since Dell EMC belongs to VMware the integration is a bit better than Nutanix. And there are also HCI licenses, so it is a bit cheaper than the normal VMware licenses. But it is still an expensive solution compared with traditional setups."
49.json,t3_bhk9nx,t1_eltjyiq,2019-04-26 18:14:57,"Used Nutanix on VMware a few contracts ago, I wasn’t a fan to be honest. It did the job though I guess."
49.json,t3_bhk9nx,t1_eltlwf8,2019-04-26 18:57:38,I worked for a VAR that was responsible for installing vxRail.  My advice.   Just don't.  We called them vxFails for a reason.  It's a horrible product with even worse support.
49.json,t3_bhk9nx,t1_eltm3xm,2019-04-26 19:01:50,"I have a problem with hyperconverged.  It looks good at the beginning, but if you add a node for more resources, you end up having to spend extra money to make the next one match the old ones.With tradition, you add CPU/Mem or Disk, not both at the same time."
49.json,t3_bhk9nx,t1_eltmqyf,2019-04-26 19:14:08,What do you mean by this? Nutanix at least definitely supports hardware generation mixing.
49.json,t3_bhk9nx,t1_eltrt6v,2019-04-26 20:54:23,I have 4 clusters of Nutanix.  2 AHV and 2 Vmware.  They both have pros/cons but overall its the Nutanix that shines through.
49.json,t3_bhk9nx,t1_eltszk3,2019-04-26 21:10:51,"I manage an environment with 36 nutanix nodes.  30 of them are running esxi with zero esxi-related issues the past 3 years.  Care to elaborate on this 'gut feeling?'AHV still not having memory dedupe is the deal breaker for us, we want to abandon vmware but it's just not reasonable yet."
49.json,t3_bhk9nx,t1_eltt40t,2019-04-26 21:12:22,"We use HX in our branch office deployments and it's a pretty nice offering imho. I know UCS and I know VMware so it's a pretty good system for my current knowledgebase.The underlying StorFS system is an NFS based storage system. I liken it more to the old LeftHand SAN/HPE StoreVirtual system except it seems to be much more modern and thought out.What I DON'T like about HX is that everything is interdependent. If you want to upgrade/patch the underlying UCS system because of a vulnerability that is newly identified, you may not be able to because the latest version of UCS that fixes the problem might not be supported by the HX data platform. Same thing with vCenter patching and ESXi host patching. You have to make sure everything is cross compatible or otherwise your stuck. Recently, it's been a lot better as Cisco has stepped up their release cycles for HX, but it's still a pain in the ass mostly because the standard HX packages use C-Series servers. B-Series (blades) usually only take 15-30 minutes to firmware patch. C-Series servers can take anywhere between 30 minutes and 2 hours (seriously). My HX installations are typically 3 C220 MSX4s (M4s) so I've had patch cycles run 6+ hours to get the entire thing updated:That's not 6 hours of downtime mind you (thanks DRS), but it's still a weekend/off hours undertaking to minimize impact to production."
49.json,t3_bhk9nx,t1_eltxrlv,2019-04-26 22:02:36,"The gut feeling is for hyper-v with nutanix. No experience myself - only read about problems. Talk from Nutanix consultant: For Nutanix AHV integration is priority, then Esx and then Hyper-v.  Imo because AHV is most integrated in Prism/AOS you lose some advantages if you use ESX or Hyper-V. There are some things you simply cant use with ESX/Hyper-v (SSP etc.) or generate additional management work"
49.json,t3_bhk9nx,t1_elu5etf,2019-04-26 23:18:05,"Running 2 Nutanix clusters setup as a Metro-Cluster with ESX for our VDI environment.It's been a great solution, very very easy to manage & provided you size correctly performs very well.Unfortunately we're looking to switch to another platform purely on the cost basis... It is very pricey (not so much purchase, but annual maintenance burns) and it's hard to make an argument supporting the cost vs benefit for us to continue."
49.json,t3_bhk9nx,t1_elu8sz6,2019-04-26 23:51:25,"I was the lead engineer for my organization, as we did a large Nutanix implementation, transitioning from HP Blades, and while I agree with some of these points, there are others which are seriously lacking context.  I'm not going to go point by point, but here are some observations from my ~3 years working with the product-As for only running AHV......uh, that is totally off base from my experience. I've run Nutanix both as a a base for ESXi and running AHV. AHV excels at exactly one thing currently, which is not having a direct licensing cost. This is more than made up for by the premium you pay Nutanix for every other aspect of the product. If you buy the Nutanix hardware, you're going to pay ~50-100% more per unit of actual usable compute, especially once you take into account the CVM resource usage.  vSphere runs fine on Nutanix, but requires some automation to make certain normal VMware process work as expected (Automated patching of hosts for instance)AHV is just fancy KVM, and that shows from time to time. Want to change what VLAN a VM is connected to? Delete the VM's NIC and add a new one. Then go configure it. Yuck.As for settings, if you're trying to use Prism Central (or Element) to replace entire vCenter workflows, you're gonna have a bad time. Nutanix is built around automation, and while their API's can be a little inconsistent at times, the new v3 stuff is great and kept me from having to touch the GUI more often than not.Nutanix can be super quick to setup, but you need to understand what you're doing. Its designed to be a simple product from that aspect, but it has some specific needs from the  environment, especially on the network side. A good example is if you're having issues getting the nodes to detect each other - IPv6 Network Neighbor Discovery needs to function.As for Nutanix being a different philosophy, I completely agree with you. The ""Nutanix Bible"" which is slightly out of date does an amazing job of covering the differences and the similarities. Nutanix is also definitely still light on features, especially on the AHV side, but also for the other hypervisors. Having to automate around the CVM limitations is the one at springs to mind for me.As for DR....this is where I both agree and disagree with you. Is setting up a quick aysnc DR plan for a few VMs easy? Sure. Especially if you're on AHV. If you want to setup an enterprise DR solution to failover ~1500 VMs to a remote site with a different IP layout? You're gonna need to do a lot of automation on your own to make that even remotely functional.In the end, every technology has use cases where it excels, and others where it doesn't. If you need a simple, robust, converged solution, and cost isn't the primary driver, Nutanix is a great choice. If you're running tons of VDI instances via Citrix, Nutanix+AHV is hard to beat. On the other hand, if you have a few thousand VMs and you need them to run general compute workloads, or if you've containerized your services and want to go further down the private cloud route, Nutanix may not be the best or most cost effective answer.*Also the Nutanix DSF is still pretty far ahead of VSAN in my opinion. But thats a topic in and of itself :)"
49.json,t3_bhk9nx,t1_elu90xj,2019-04-26 23:53:20,"AHV for VDI, and VMware for everything else? Thats what we ended up going with, since we were using Citrix for VDI and there isn't much point to paying VMware to license ESXi when you're not using *anything* other than hosting thin clients."
49.json,t3_bhk9nx,t1_elufrde,2019-04-27 00:59:09,So NetApp isn't really HCI in the traditional sense.   It is compute blades and Solidfire blades.    You can scale compute and storage independently which is a plus vs the other solutions IMO.
49.json,t3_bhk9nx,t1_elui2yw,2019-04-27 01:20:24,"A really interesting read, comments and allThank you all for sharing your experiences !"
49.json,t3_bhk9nx,t1_elunran,2019-04-27 02:13:00,"I am a (very) focused Nutanix VAR. Happy to take a 2nd look at maintenance costs, shouldn't be getting burned."
49.json,t3_bhk9nx,t1_elv50yd,2019-04-27 05:01:21,"I agree that there are some features missing (vm to vm anti affinity is available but not supported!) There is so much available if you are willing to use the cli.  I have changed the plan without deleting the nic.  https://next.nutanix.com/server-virtualization-27/changing-vlan-of-vm-17763. Not saying it is without it's downsides, we need a small VMware cluster for hosting our shoretel environment since every other hypervisor is unsupported.  But overall it has worked well for us.  I'm not discounting your experience though"
49.json,t3_bhk9nx,t1_elv6dek,2019-04-27 05:15:19,"A lot of the additional features Nutanix  loves to talk about are specific to AHV and not ESXi. Especially when you start talking about anything around their Enterprise Cloud OS stuff (Flow, Calm, Xi, etc.) which is competing with vRealize, NSX, etc.If you're going to run ESXi on HCI, why wouldn't you just do it on VxRail where you aren't going to be gated from features?"
49.json,t3_bhk9nx,t1_elv6ujo,2019-04-27 05:20:11,"Look at something like VxFlex OS (was ScaleIO). It has storage and compute decoupled, so you can truly expand them independently. You can, also, have multiple compute clusters sharing the same storage pool, which you can't really do with other HCI offerings. Way more flexible."
49.json,t3_bhk9nx,t1_elyfu5k,2019-04-28 10:44:31,"You can add compute only nodes to vSAN. As far as growing storage asymmetrically I see people go single socket, or leave 1/2 the drive bays empty and add them as they go."
49.json,t3_bhk9nx,t1_em1ww1y,2019-04-29 20:17:41,I am just getting back to this post. This is pretty informative and definitely highlights some things about Nutanix I wasn't aware of and I doubt a marketing/sales/sales engineering team will tell me about.
5.json,t3_c72qxz,"Worldwide Converged Systems Market Posts Strong Results in the First Quarter of 2019 with 19.3% Year-Over-Year Revenue Growth, According to IDC",2019-06-30 02:24:58,[]
5.json,t3_c72qxz,t1_escm116,2019-06-30 02:27:43,HPE ~= Cisco ~= LenovoJust one question: How can it be true?
5.json,t3_c72qxz,t1_escr4it,2019-06-30 03:31:35,This is a comically optimistic way to show growth in hyperconverged systems by throwing things like flash stack into that category when in reality they aren't remotely converged. Gotta love the hype though it's right up there with astroturfing for cloud by the industry.
5.json,t3_c72qxz,t1_escukjl,2019-06-30 04:14:44,"Because their statistics show it. Statistics are like art, it can show anything you want."
5.json,t3_c72qxz,t1_esdgwyz,2019-06-30 09:11:38,"Agreed..If anything, hyperconverged is dying.Those of us who've been around the block a few times realize the value of having and maintaining an open systems approach to our gear. The lock-in nature of hyperconverged infrastructure is a step in the opposite direction from this philosophy, with no tangible benefits to the customer. Only perceived ones that don't pan out.Anyone who's had to suffer through a 2+ week long RCM upgrade on a Vblock can attest to this. What was promised as a turnkey solution for growth rapidly becomes a series of artificial constraints that hamper growth, rather than encourage it.A Vblock is, for all intents and purposes, closed system, not an open system; a rack full of gear which can't leverage outside resources by virtue of its support contract is logically no different than a rack full of gear which can't leverage outside resources by virtue of hardware incompatibility.Not once in the past 3 years have any of us said, ""Boy, I'm glad this hardware stack is Hyperconverged.""... What we have said, frequently, is ""Boy, I wish we could take advantage of resources outside the Vblock""."
5.json,t3_c72qxz,t1_esdtnr7,2019-06-30 12:18:48,"Nothing I’m seeing is indicated HCI is dying. Open systems architecture requires armies of staff to take care of it. It might scale better, but man power is more often than not the choke point for orgs. HCI is rapidly becoming the default platform for almost all of the mid-market and small enterprise.Dell is doing some serious “manifest destiny” shit with VXRail. It’s a true “good enough” product and they’re riding that the bank.  Nutanix has a much better chance in the upper end of the market because they actually have real IP in their products and a dev cadence that can react to customer demands.Public cloud fatigue is real. Lots of dev heavy orgs are looking at HCI (really nutanix with AHV, or something with ESXi if they’re mid-ELA contract) to bring their dev workloads in house since the costs on public cloud are so variable with Dev workloads.But, even with those headwinds I’m still betting on HCI for the long term. Talent is, seemingly, ever harder to come by and so many orgs want the ease of mgmt because of it. If you’re not on a coast, hiring devops engineers is really tough.RE: Cisco, HPE and Lenovo - I 100% believe that. Hyper flex only gets sold when it’s thrown in for free (and Cisco can’t figure out how to price their servers anymore), HPE/Simplivity is not priced well enough against VXRail to close deals on its own. Then customer ask why would you run a VMware dependent product from an OEM that doesn’t’t own VMWare, when I can by VXRail directly from VMWare (so to speak).Lenovo? I thought they only existed to sell servers to run Nutanix on and to companies that still haven’t figured out they’re not actually IBM anymore.No one is buying this stuff because of business value, they’re all doing it because they can’t deal with the overhead of 3-tier and the various industry rags and experts tell them all the cool kids are on HCI.  HCI is far from dead."
5.json,t3_c72qxz,t1_esobg5q,2019-07-03 22:28:08,"HPE has the benefit of vSAN and Simplivity - they latter of which we're hearing they're pricing very aggressively - that said, hard to imagine Dell EMC isn't selling more. Lenovo has 7-8 stacks they offer in the ThinkAgile family, but they're still a small player in the enterprise, though that's changing (look at the server tracker). Cisco, who knows...their HyperFlex (Springpath) thing isn't special and even if they get credit for Flexpod (not sure if NetApp or Cisco gets that in this tracker), that's not a giant number."
50.json,t3_a6pwyb,Dell EMC VxRail P570F Review,2018-12-16 23:39:29,[]
50.json,t3_a6pwyb,t1_ebx4h33,2018-12-17 01:34:50,Would be more interesting if performance and price was compared to an identically configured 740xd Ready Node.
50.json,t3_a6pwyb,t1_ebx5j10,2018-12-17 01:47:06,They seem to believe they're still competitive...
50.json,t3_a6pwyb,t1_ebxa2qn,2018-12-17 02:39:51,"Price is what it is, but what makes you think performance would be any different given an identically configured node?"
50.json,t3_a6pwyb,t1_ebxgmgl,2018-12-17 03:54:14,How easy was initial install and implementation though? I’ve heard horror stories
50.json,t3_a6pwyb,t1_ebxijmw,2018-12-17 04:14:59,"Performance would be the same, the additional rail setup and system management goodies aren't overhead. That said, the appetite to send us ready nodes is pretty low across the board. Dell EMC wants to sell VxRail of course and HPE won't even talk Ready Nodes unless you ask really hard ;) The closest we get usually are systems from Supermicro, like in the Optane vSAN review we did recently."
50.json,t3_a6pwyb,t1_ebxims6,2018-12-17 04:15:55,Both times we've looked at this it's been pretty easy. Normally it appears people get hung up on network settings.
50.json,t3_a6pwyb,t1_ebxrw3p,2018-12-17 06:01:51,"We are looking to get four of these to replace our current UCS / VNX5200 setup.Our alternate solution would be to get a Unity, but I'd like something a bit more streamlined than the UCS for compute."
50.json,t3_a6pwyb,t1_ebxy51h,2018-12-17 07:23:26,"We looking into this now. It's between this and a power max, or possibly a Nutanix.Interested to see when the pricing ends up. I like the simplified management for vSAN."
50.json,t3_a6pwyb,t1_eby05ag,2018-12-17 07:47:45,"I just deployed a similar model and they are borderline stupid and ultra fragile out of the box deployed. The install process wasn't awful, but you have to touch every host if you aren't doing the default vlan / ip addresses.  I had to tweak a lot to get everything right post install. They don't support LAGs. The alarms are really dumb as they simply show up as vx##### alarm and basically force you to contact.  Once you get past all of the nuances, it's not awful."
50.json,t3_a6pwyb,t1_eby6um3,2018-12-17 09:12:22,Looking to replace our Dell FX2 chassis and 3 Nutanix nodes with these
50.json,t3_a6pwyb,t1_eby9x1a,2018-12-17 09:57:16,"We're still on the Nutanix ""not friends"" list."
50.json,t3_a6pwyb,t1_ebybdpq,2018-12-17 10:18:42,"Running our first cluster for a few months now and  it has been very positive. 4 node installation went fine, no issues, but there is some networking configuration to get it right. Ordering a few more modes this year.  Retirement of my SAN is on the roadmap ;-)"
50.json,t3_a6pwyb,t1_ebybkw3,2018-12-17 10:21:42,storagereview you guys should try and do a review on datrium gear.
50.json,t3_a6pwyb,t1_ebyfai8,2018-12-17 11:16:57,Last time we checked on pricing it was about 2x cost for the VxRail. This was R730 series servers. Also they had some known issues which led me to go with vSAN ready nodes and vSAN.
50.json,t3_a6pwyb,t1_ebyfraf,2018-12-17 11:23:44,Does the 1 click upgrade actually work now? And does the patch cycle actually keep up with VMware?ps I’ve worked with VxRail since the quanta days....
50.json,t3_a6pwyb,t1_ebytb8e,2018-12-17 15:52:53,Aahhh the goold old 2U4N days. Quite the excitement that was.
50.json,t3_a6pwyb,t1_ebyudzw,2018-12-17 16:24:12,We have 2 clusters. 1x 4 node. 1x 10 node. First time the install was very painful. Took one week. But this was due to a couple of things.And that was it really.The second cluster was much easier and much quicker as the ground work was done. Time sync issues occurred again. But I do think this is due to our vCenter setup.Secondly idrac had to be reset on one of the nodes as upgrade was failing. One click upgrade eyes rollingAfter those niggles were sorted it worked like a charm.The first cluster had to be reloaded as we were moving workloads from one cluster to another and wanted to seperate vCenters. This reload went super smooth. After RASR all the nodes then boss card and idrac firmware. Deploy and upgrade took a day. Super quick and all one click.Performance is fantastic. Boot storm of 700vdi caused latency spikes of less than 2 milliseconds.Yeah it's expensive and you get locked in to the product. But upgrades and deployment are fairly easy. For us it's working well.
50.json,t3_a6pwyb,t1_ebyue3x,2018-12-17 16:24:17,[deleted]
50.json,t3_a6pwyb,t1_ebzag0c,2018-12-17 22:58:44,We did this about a year ago -https://www.storagereview.com/datrium_dvx_with_flash_endtoend_review
50.json,t3_a6pwyb,t1_ebzji3q,2018-12-18 00:56:53,vSAN's got a Time drift alarm to catch this happening early and cluster quick starts now directs you to setup NTP (Time.Vmware.com works BTW!)
51.json,t3_57gz3j,vsan ou infra classique ?,2016-10-14 23:39:26,"Salut ! aujourd'hui j'ai une infra virtualisée classique: 2 esx, une baie. je dois changer tout ça.. on me propose soit: 2esx, une baie ou 2 esx avec vsan j'avoue ne pas arriver a me decider ... un avis ?"
51.json,t3_57gz3j,t1_d8ryla6,2016-10-15 00:56:45,"VSAN sans hésiter pour moi. Ton use case est parfait : petite infra, une seule baie. Ton nouveau cluster tiens dans un 2U et coutera sans doute moins cher (à chiffrer tout de même).Après tu as aussi la soluce hyperconvergé Nuta Express ou VxRAIL (mais VxRAIL risque d'être un peu cher je pense)"
51.json,t3_57gz3j,t1_d8s8na3,2016-10-15 04:24:16,en bonus pour ceux qui ne connaissent pas les VSAN https://www.youtube.com/watch?v=pAFPP98XEtk
51.json,t3_57gz3j,t1_d8scna6,2016-10-15 05:56:10,"Je te conseil vsan également. Si tu as besoin de grossir, l'ajout d'un noeud t'apportera du compute, de la RAM et du stockage. De plus, plus tu ajoute de noeuds, plus ton stockage est performant en terme d'iops.Sinon, pour seulement 2 noeuds Nutanix est une bonne solution aussi. Je crois que c'est gratuit jusqu'à 3 noeuds. Par contre attention au surcoût engendré par la CVM."
51.json,t3_57gz3j,t1_d8swlto,2016-10-15 17:35:46,Tout dépend des besoins ... Mais effectivement les architectures hyper convergées on la côte j'inclue aussi VSAN... Qui est de l’hyper convergé comme VxRail typiquement sauf que le matos est à part c'est pas de l'intégré. Pour info nous démarrons un projet stockage dans notre boite voici le besoin :La solution doit répondre aux besoins suivants :Les éditeurs/constructeurs que l'on rencontre :
51.json,t3_57gz3j,t1_d8sxabc,2016-10-15 18:20:00,j'aimerai bien avoir ton retour sur Simplivity qui était dans la course face a Nutanix chez nous
52.json,t3_9hnuqq,Sanity check on vSphere Replication Traffic isolation (VxRail),2018-09-21 17:24:53,"Hello vExperts, our new VxRail PROD & DR Clusters are freshly deployed and running beautifully. Now, we're looking to deploy vSphere Replication (and later SRM). That fairly simple task has an added complexity: There is a requirement to use a non-routed site to site link for replication traffic, instead of the default Network. I've already deployed the VRAs, and confirmed I can link both sites together. In this default setup, the Replication traffic flows over the Production Network and then the firm-wide VPN to reach the remote site. So the next step would be to separate that traffic over the site to site link. That link is a direct connection between sites over a Layer 2 VLAN, i.e. no routing. VMWare documentation has some good information on isolating the replication traffic ( Here's the setup I have in mind. It's pretty much straight out of the documentation: - Create the replication VLAN as virtual Port Group on the DSwitch - On each host, create VMKernel Adapter for Replication Taffic (Source) or NFC Traffic (Destination) and assign static IP addresses - since this is on the non-routed link I can assign any IP from one of the private Networks (172.16 etc) - Create Adapter on each VRA and assign the Replication Port Group, configure them to be assigned to replication traffic - (Uncertain if this is needed when we use the direct link) Create the static routes on the hosts Does this make sense? Or am I missing something when this specific Network topology is used? Cheers!"
53.json,t3_97oi8g,"Horizon 7.5.1 + M10 vGPU + Instant Clone, Customization Operation Timed Out",2018-08-16 10:38:45,"Hey all, working on a new build for a customer.  VXrail V570f hosts (latest code), with Tesla M10's and Horizon 7.5.1 with instant clones, AppVolumes 2.14 and UEM 9.4 (Windows 10 2016 LTSB) I have no problems deploying pools with all EUC agents including Appvol and UEM.  however as soon as I add the grid profile to the parent VM and deploy a new pool, i run into issues.   Majority of the VM's in the pool error and logs say ""Provisioning error occurred for machine.... customization operation timed out"", followed by attempting automatic error recovery.  I'm not sure what's going on, and why it appears to be limited to the NVIDIA enabled VM's.  (NVIDIA Driver was the latest when I downloaded it last thursday - 391.81 and the host VIB matches the driver in the guest) Any ideas?"
53.json,t3_97oi8g,t1_e4a0wte,2018-08-16 13:42:25,When you installed the latest Nvidia Drivers did you reinstall the tools and agent correctly?
53.json,t3_97oi8g,t1_e4ac908,2018-08-16 19:40:46,I'm not sure what you're getting at.  This is a brand new Win 10 image.I installed the components in the following order:ToolsView AgentView direct-connect agentUEM AgentAppVolumes AgentGRID drivers
53.json,t3_97oi8g,t1_e4artuv,2018-08-17 00:01:44,"Asking because you say you have been working with Horizon and GRID since 2015 - have you ran into an issue with a black screen while the desktop is loading for about 15 - 20 seconds?After that it loads and works fine. This black screen while the desktop is loading only occurs on the GRID enabled pool. I just set this up a month or so ago and I believe all the driver versions are consistent.Trying to determine if this is something that can be resolved, or if it just a ""quirk"" with GRID."
53.json,t3_97oi8g,t1_e4arx7a,2018-08-17 00:03:02,"Hmm, not particularly.  Is there a Windows legal caption splash policy on these VMs?  If you hit ctrl-alt-delete immediately does it come up or do you still have to wait 15-20 seconds?"
53.json,t3_97oi8g,t1_e4d5ue2,2018-08-18 01:47:06,"Update:! Here is VMware's response to the ticket:""I found that this is a known issue for Windows 10 LTSB with Horizon 7.5.1 and Tesla M10 vGPU.As a workaround I would request you to try with Windows 10 1703 or 1709 Build  with same agent and vGPU. ""apparently the issue is that attaching a grid vgpu profile to the VM and attempting to deploy a pool breaks DHCP, which then causes customization to timeout.  I asked if earlier versions of Horizon will work, they said they should, and that the issue appears to be isolated to 7.5.1."
54.json,t3_59sroj,[UPDATE] - ScaleIO massive performance issues: Resolved,2016-10-28 13:03:41,"Well, this has been a long journey, but we've gotten there. History:  https://www.reddit.com/r/vmware/comments/4c45et/scaleio_on_vmware_massive_performance_issues/ So, the TL:DR - What was wrong? 2 Things, it turns out. The first was basically as simple as upgrading from 1.32.3 that we were on, to 1.32.6 -- This took away the majority of metrics we were seeing really poor figures on. Latency improved, read throughput improved significantly. But, we were left with poor write performance. This last metric turned out to be R1Soft backup software! We'd only installed it recently, but essentially it installs a VSS writer driver, that captures writes, and well, it severely cripples write performance. So in short, the key thing was upgrading. It bothers me that EMC support (local) were unwilling to upgrade, and flat out refused to even acknowledge that upgrades were important. Repeatedly stressed that we would be best to resolve the performance issues first, then upgrade to a newer version. So that really set things back a lot. So the longer version of events. We installed ScaleIO back in September 2015, and went live with it around October. Benchmarks showed massive performance, hundreds of MBPS, hundreds of iOPS, everything spot on for our hardware. Once we moved our production systems over to it tho, everything went to shit. We had issues from the start, down to complete outages of the cluster, to the point that we eventually migrated all our VMs back to our old environment for a month or so. This was eventually tied to 2 underlying causes. One spinning disk was identified as fault, spamming SCSI bus resets. This caused the H730mini on that host to shit itself, effectively, which in turn took that node out of service. Since the SVM's for ScaleIO were also on that RAID card (later learned this is against best practice), it caused ScaleIO to not be able to detect this outage properly, and the whole cluster ground to a halt when it happened. Rebooting the host fixed the cluster, until the disk failed again. It bothers me that the PERC/iDRAC never picked this up. We only found it because I was physically AT the server when it happened and heard clicking sounds, and noticed one disk had a light on. The iDrac still through everything was happy. Dell claim this has since been fixed. Dell also claim the H730 won't be so affected by SCSI Bus resets like it used to either in newer firmware, but I'm not so convinced on that front. Dell replaced the HDD, and we got some stability back. We were now seeing very poor performance (as above and as per my other threads linked) -- We had trouble proving it to EMC tho, as benchmarks tended to come back with nice figures, but real world use was terrible. We invested a lot of time and effort into building up Grafana & InfluxDB to capture as many metrics as possible to be able to get a grasp on everything. We were still occasionally seeing issues where the whole cluster could be affected (copying multiple large files to a file server, for example could cause ALL VM's latency to blow out) -- Turns out this was H730 again. Since the ScaleIO SVM's were on the same PERC as the data disks, if the SAS bus got saturated, the SVM would lag out, causing the whole cluster to lag out. Dell came to the party here, and supplied us with 4 PCI H730's, 8x 300GB SAS drives, and a back-plane mounting kit to add 2 additional drives to our R730xd's. Once these were installed, the ScaleIO SVM's were moved here, and this did seem to solve this issue. This left us mainly with our performance issues. Eventually, in desperation my boss got escalated higher and higher within EMC Australia, basically to the point of threatening to throw ScaleIO out and get a new SAN. They started to take the SR seriously (it had been open since October 2015, and this was in March 2016 or so), but this basically led to daily phone calls, a lot of log capturing and gathering, and no real action. This is where EMC Australia were pushing the ""don't upgrade yet until it's fixed"" mantra. Around May 2016 we also started seeing instances of one node just going 'offline' for about 10-15 minutes, then coming back slowly to life. Occasionally it would also crash the ESXi on that box. Always the same node, but nothing was logged/faulted. We opened a second SR at this point, and this is what led to the 1.32.6 upgrade occurring, as it was identified that the crashing ESXi was a known issue in 1.32.3 and upgrading would resolve that. Finally, I made my more recent post on Reddit, where I wanted feedback on VSAN vs traditional SAN, as we were very serious about moving to a new storage system, and damn the expense, as our systems were crippled. This post got the attention of EMC America and people within ScaleIO in Israel. Very quickly I had some PM's and a couple of phone calls. Basically at this point within 2 days I had confirmation that a high level engineer from the USA was flying to Australia and would be on site to ""get this fixed"". And to his credit, he did. I promised him I'd post a follow-up here that all is well, so here we are. He was on site for over 2 days, and put in some mad hours, even more so when you consider the jet-lag he would have been facing. By the time he got here, we'd had support to remotely assist us with the 1.32.3 to 1.32.6 upgrade. Assessment of that showed a massive improvement in latency, and throughput. Backups that took 48+ hours now took < 24 hours (3.6 TB) , nightly backups went from 6-7 hours -> 3-4 hours. Latency figures looked much much better. But still the writes were horrible. So we focused on that, and by trial, error and a LOT of testing, we identified that it was the R1Soft backup agent and it's VSS driver causing that issue. (Note: R1Soft had only been installed around March/April 2016 -- So if we hadn't had this installed, the 1.32.6 upgrade would have been seen to fix everything in one more) While the engineer was on site, the other issue of the host dieing happened again, but now in 1.32.6 we found the behavior was different. Thankfully, it no longer crashed the ESXi host, but it also didn't offline the whole node, the node lost its disks. All of them. All 23 disks on that node went into error state, then came back about 5 minutes later. ScaleIO one by one online'd them again, and everything went back to normal. So we got stuck into that one too with the engineer, and found that 1.32.6 also ups the logging and details (albeit, hidden away in a log file) -- Another failed disk. No error light this time, no noises, no apparant faults, just that one disk would go off, and take the H730 out with it, thus all the other disks also affected. Identified this by the facts that went ScaleIO re-online'd all the disks, each time it happened, one didn't come back online automatically. It could be done manually later on and it was fine, but then it would do it again. Dell again replaced the disk, and so far, touch-wood, we're back online without any issue. We've since asked support to assist us upgrading to ScaleIO 2.0.1, which has now been done. I don't see any major improvements in speed or latency, but the new UI and some of the back-end features are very nice. So all in all, EMC Support finally came through once we rattled enough cages, and about 13 months after we installed ScaleIO and went live with it, I have to say, I'm impressed. I wanted to love it from the start. On paper, it seemed amazing (all hyper-converged systems do in theory), and I wanted it to work. When the boss said he was over it and we just had to get something else, I was disappointed, but it's hard to keep defending even the concept of something if it is constantly causing performance issue, outages, and even ESXi host crashes! At a certain point, I was fearful of nighttime emails or SMS's, because I knew it would be ScaleIO having caused another outage somewhere and I'd have to stop what I'm doing, look into it, put it right as best I can, and then let my boss know. And I think the level of me hating those emails was only slightly less than the feeling of having to ring the big boss AGAIN at 9PM or later to say that AGAIN, storage caused a problem, explain how long it was and what the impact was to our clients. So, finally, now I can say I do love it. It's easy to use, it's great to manage, and the concept is still amazing. Virtual storage on a virtualised platform. And the scalability concept. We've got 4x R730xd's now with half SSD, half spinning disks. We know we'll run out of storage long before we run out of compute and RAM, but for the same price it would cost to expand a SAS by a 20 or TB, we can add in a 5th node of computer and RAM. Maybe we don't need it, but more hosts = more redundancy and technically a speed boost in some aspects too, so it's win win really. So thank's to "
54.json,t3_59sroj,t1_d9b2d3d,2016-10-28 13:10:18,"Great content in this post.  Just wanting to confirm, did the release notes from v1.32.3 to v1.32.6 mention anything about fixing the problems it did fix?"
54.json,t3_59sroj,t1_d9b3wcx,2016-10-28 14:07:48,"The release notes are massives each point release has many many fixes, all fairly vague.But yes, there were quite a lot of ""may improve performance"" or ""improves performance when.."" in the notes.I guess I can see the logic in wanting to solve before making changes, but kind of frustrating at how long it all took in the end, for what ended up being a relative simple step to fix."
54.json,t3_59sroj,t1_d9begt4,2016-10-28 21:55:50,Very happy to see this ended well for you :). Those CRE's flying all over the place to pull things together are heroes to many. I've also had a node pull a whole cluster down for a short time because of a failed disk so you're not alone on that one. The handling of failed components has tremendously improved in newer versions of ScaleIO.
54.json,t3_59sroj,t1_d9bem1i,2016-10-28 21:59:20,hugs nimble arrayhugs nimble storage support team
54.json,t3_59sroj,t1_d9bghqe,2016-10-28 22:42:50,"It really is too bad that it took you posting to Reddit to get EMC's attention and fix the issue.  the AU support should have escalated it to U.S. support while you were still having issues.With that said, I'm glad they did help you and you were able to resolve the issue."
54.json,t3_59sroj,t1_d9boj9x,2016-10-29 01:28:02,I'm jealous.  EMC VNX support is horrible.
54.json,t3_59sroj,t1_d9d41ld,2016-10-30 04:05:57,This guy should be compensated by Dell EMC for accepting his challenge .. most people would be done months ago!
55.json,t3_6w4tk5,Hyperconverged solution: Dell VxRail,2017-08-26 18:15:59,"Hello everyone, Since we're shopping to replace our current Fujitsu based environment, running PAN Manager 6, to something more streamlined and, well... standard; we've been on a little researching spree. After comparing several solutions, HPE Simplivity for instance, we seem to lean towards HCI more than the traditional converged solutions. Now, after comparison: Dell's VxRail seems to be a pleasing candidate. However, I'm rather curious to the sysadmins in this here subreddit that have actually used or are still using former mentioned HCI solution. So, any of you willing to share your thoughts and experiences? Much obliged already, it'd make choosing the right solution a lot easier. P.S. How on earth do I get proper white spaces between sentences?"
55.json,t3_6w4tk5,t1_dm5bsvc,2017-08-26 18:25:43,"Take a look at Dell VRTX, that is hyperconverged."
55.json,t3_6w4tk5,t1_dm5fxi3,2017-08-26 21:15:01,"Not posted that long ago:https://www.reddit.com/r/sysadmin/comments/6vnbjf/vxrail_experience/In fact I can't find many positive reactions here... While you'll often  hear more bad comments than good, the complete lack of positives is ... worrying."
55.json,t3_6w4tk5,t1_dm5g7vh,2017-08-26 21:24:04,I've talked to a few former EMC guys. They said vxrail has a ton of issues
55.json,t3_6w4tk5,t1_dm5gd7s,2017-08-26 21:28:09,Kind of a difficult recommendation without understanding scale requirements. It's a neat solution for a handful of hosts and some storage but lacks scalability
55.json,t3_6w4tk5,t1_dm5gi57,2017-08-26 21:31:58,"This is more of a piecemeal solution. Dell's ""true"" HCI solution is Nutanix on Dell hardware. Most Dell Nutanix users have a longer/lower quality support experience because it goes through Dell then punts to Nutanix.If you're looking to go HCI, check out Nutanix (OEM, not on Dell/Cisco/Lenovo/etc), Simplivity, vxRail. Then go with the warm and fuzzies."
55.json,t3_6w4tk5,t1_dm5gq76,2017-08-26 21:38:16,Nutanix on dell for all the things.
55.json,t3_6w4tk5,t1_dm5hh1b,2017-08-26 21:58:26,"Honestly, there's two messages on that? Like two whole replies.."
55.json,t3_6w4tk5,t1_dm5igiu,2017-08-26 22:24:00,"I don't have experience with vxrail, but if I was looking at hci I would be taking a close took at Netapp's hci solution, as it solves some of the problems I've had in general related to hci and tying your compute/storage upgrades together.http://www.netapp.com/us/products/converged-systems/hyper-converged-infrastructure.aspx"
55.json,t3_6w4tk5,t1_dm5ij8b,2017-08-26 22:25:55,"We looked into Simplivity as well as VxRail before finally deciding on Nutanix. The timing was bad for the other two because HP had just acquired Simplivity and the whole Dell/EMC/WhoeverownedVxRail merger was in progress. We ended up with no one trying to sell us Simplivity and about 4 Dell reps trying to sell us VxRail.Sales aside, the decision ultimately came down to manageability. Simplivity seemed very messy, it was all vSphere plugins to get anything done. VxRail was all vSan based, and if your team already has several experienced VMWare experts, this would not likely been an issue. Neither seemed to add value beyond the whole HCI component either. Nutanix has its own interface for managing storage, which is stupid simple to use. It literally takes less than 15 minutes to create a storage container and add it to all the hosts on the cluster. In addition to VMWare, Nutanix also supports Hyper-V as well as their home grown AHV (I don't know if the others do this).TL;DR: VxRail requires vSan knowledge/expertise, Simplivity uses convoluted vSphere plugins, and Nutanix has a stupid easy interface for storage. We decided to drink the Nutanix kool-aid."
55.json,t3_6w4tk5,t1_dm5ivzf,2017-08-26 22:34:51,Check out Cisco hyper flex
55.json,t3_6w4tk5,t1_dm5juh5,2017-08-26 22:58:06,"I would stick to the things that have good reviews, like Nutanix."
55.json,t3_6w4tk5,t1_dm5jxow,2017-08-26 23:00:18,"We looked at Simplivity too, but it was 2x the price of nutanix and 3x the price of SAN + Servers."
55.json,t3_6w4tk5,t1_dm5ko0p,2017-08-26 23:17:35,"Cisco's HyperFlex solves that problem as well, and is probably a more well-known HCI solution."
55.json,t3_6w4tk5,t1_dm5kvhr,2017-08-26 23:22:25,"I can't speak to VxRail, but we tried getting a quote for VxRack. They took forever to turn it around, got it wrong, tried two more times, never got it right. We went with UCS and Nimble for 1/6th the cost. We may also add some HyperFlex nodes in the future for specific workloads."
55.json,t3_6w4tk5,t1_dm5meo9,2017-08-26 23:57:58,Particularly data loss.
55.json,t3_6w4tk5,t1_dm5npa4,2017-08-27 00:27:50,"I really do like VxRail in concept, but the execution does still leave a lot to be desired. The promised ""one click upgrade"" of hosts isn't... at least not yet (it's ""coming any day now"" for over a year) and in general the promise of the prepackaged solution is a little ahead of the actual delivery in my experience. However, it's improving at a rapid clip and has made some interesting strides lately to make it more competitive. It's still a lot better than Simplivity in my opinion, but I'd say it'll be early next year before the promise is really realized.Now having said all that, the tech on which VxRail is based (VSAN) is solid as all get out. I've done a bunch of VSAN Ready Nodes and had nothing but great experience (again, Dell... they've got this down). Plus, you can spec out single CPU boxes if you don't need massive single-box compute, and since VSAN only supports 8 drives per disk group per host (one cache and 7 data), you don't hit any limitations on PCIe lanes you might hit with more drives even if you load it up with NVMe (actually an option with Dell's 14G servers which I find impressive). This halves your VSAN licensing cost due to per-socket licensing, and instead of scaling up on per-box you can scale out with more boxes which has the side effect of giving you more fault tolerance or more storage... and definitely more compute and memory. In fact, it's generally what I recommend to my customers unless you have a specific need for few boxes in a cluster (small datacenter or half-height rack for example)In the HCI realm on Dell as well you have the XC boxes, which are Dell servers with Nutanix which is also a great solution. Nutanix is not perfect, but generally I feel it's solid and you get a choice of hypervisors if you're not VMware-centric. Hell, you can even change the hypervisor.I'm hopeful that ScaleIO gets more of a first-class treatment under the new DellEMC stewardship. It's a damned fine product that doesn't even NEED a hypervisor (though you can optionally have one) and you can even mix and match in the same cluster. I am pretty sure it languished after EMC bought it because they felt it would cannibalize their own core business (big monolithic storage arrays) so they just put it out there as a ""me too"" product in the then-nascent HCI marketplace. Already I'm seeing recent versions have come a long way from the early versions, and Dell is now selling the ScaleIO ready-nodes (similar to the VSAN ready nodes) that are pretty competitively priced and flexible in configuration. ScaleIO still can be pricey once you add in your operating system license costs since it needs a host OS to run, but once up and running makes for a very compelling solution in this space. I just hope Dell can get away from the occasionally-awful SATADOM they use for the OS boot in these things and get that highly advertised BOSS out the door instead."
55.json,t3_6w4tk5,t1_dm5nu9k,2017-08-27 00:30:48,FWIW I've been doing some of the same shopping around for HCI over the past few months. What struck me is that my VMware account team will all toe the company line when someone from dellEMC is in the room and say vxrail is great. As soon as the VMware team is on their own they trash vxrail and say I should be looking at literally anything else that's vSAN compatible. Vxrail has thus far always been the most expensive option in the HCI market for what we've quoted out.I've also heard through a few trustworthy account reps that dellEMC is giving 1.5x commission to account reps on all vxrail deals which explains why every conversation with dellEMC somehow devolves into HCI.
55.json,t3_6w4tk5,t1_dm62wz2,2017-08-27 06:02:23,"Dell VRTX plus vSan is a kind of build your own HCI solution. Will likely be kinder to your wallet, but may take a little more management than vxRail."
55.json,t3_6w4tk5,t1_dm62x04,2017-08-27 06:02:24,"Dell VRTX plus vSan is a kind of build your own HCI solution. Will likely be kinder to your wallet, but may take a little more management than vxRail."
55.json,t3_6w4tk5,t1_dm63r3e,2017-08-27 06:21:27,"Nutanix is the answer you're looking for. Either running VMware, hyper-v, or its native AHV (with veeam and rubrik supporting it now). I've installed and troubleshot both and you will do very little of the latter with nutanix."
55.json,t3_6w4tk5,t1_dm659qh,2017-08-27 06:57:40,"The first 1.0 release of the code had some bugs, as well as teething issues around early deployments needing polish and training.Those have been patched, and the 4.5 release that's catching up to vSAN 6.6.1 fixes all the issues I've seen complaints about.It's a product that had some growing problems (They sold like 1500% of target the first 2 quarters) but they have been staffing up, and catching up on patches."
55.json,t3_6w4tk5,t1_dm67hfc,2017-08-27 07:51:09,"Tis a dangerous question to ask, only madness lies down this road."
55.json,t3_6w4tk5,t1_dm6b86k,2017-08-27 09:21:48,"VxRail provides zero value over a standard VMware and VMware VSAN setup. Actually, zero value is being too kind. The VxRail software additions and support provide significant negative value. So much time wasted, so many production impacting problems.Our SOP for a VxRail problem is to immediately open a Sev 1, wait out the SLA, and then call the escalation engineers until someone answers and takes action. I've made it to the regional support manager (or whatever his position is...) multiple times. Don't bother opening anything other than a Sev 1. By the time support gets around to youBuy yourself some 1Us and a SAN on the VMware HCL. You'll be much happier. Skip VMware VSAN as well. It depends far to heavily on VCenter Server, and running a vCenter virtual appliance on a VSAN scares the hell out of me these days."
55.json,t3_6w4tk5,t1_dm7102p,2017-08-27 23:40:45,"I've not used them myself, but had a demo and sales pitch from VCE.  My thought was it was far too simplified for my taste, but I could see that having its place.Also, keep in mind with the Dell acquisition, EMC is no longer using Quanta hardware for this, they are now on Dell hardware.  Whether this is an improvement or not, I have no idea, but it makes older reviews/comments potentially no longer relevant."
55.json,t3_6w4tk5,t1_dm74j6w,2017-08-28 00:54:24,"HCI is a great solution for vendors' profit margins, that's for sure."
55.json,t3_6w4tk5,t1_dm8tigu,2017-08-29 02:07:59,this guy knows his stuff
55.json,t3_6w4tk5,t1_dm8tk23,2017-08-29 02:08:47,"the nutanix Kool-aid tastes like 2048, lol great stuff and company"
55.json,t3_6w4tk5,t1_dma2khv,2017-08-29 21:17:56,"Looked at Nutanix as well, but the prices bite. Especially if you don't need much infrastructure to be implemented, some kind of SMB. Of course it has some sweet options like ultimate managing, but in some cases building a cost-effective solution makes sense.We were comparing Nutanix, HPE VSA and Starwinds VSAN recently, and finally went with Starwinds hyperconverged solution, saved a lot of money and works well with two nodes. Yeah, it doesn't have its own hypervisor atm, but it provides fault tolerance, and they of course support the whole appliance."
55.json,t3_6w4tk5,t1_dnz0unf,2017-10-06 09:39:48,"I see this myth about HA too (HA doesn't need vCenter to work, just to configure). vCenter for vSAN is nothing more than a management/monitoring plane, but you can configure and monitor from hosts directly if need be (albeit with reduced functionality but enough to deploy a new VCSA. The 6.5 release closed a lot of gaps here on the monitoring side).It's not dependent on vCenter really. You can run the health check's on a host directly from the UI (or CLI).  I've recovered a cluster from a dead VCSA, and GSS more than familiar with the process. No metadata, or ""state"" lives in the VCSA (it keeps the primary copy of the member lists, but if it dies the local CMMDS's persist even through reboot with their version).  Even the vSAN performance service is stored in the object store directly."
56.json,t3_cguhpf,failed - file /vmfs/volumes/vsan,2019-07-24 00:02:54,"I needed to shutdown the VxRail and it's 4 hosts due to swap of the UPS. I've restarted the hosts and after starting vxrail manager vm , I can see all the VM's. I can start about 90% of them , but some give the following error message: Power On VMKey haTask-823-vim.VirtualMachine.powerOn-159583661 Description Power On this virtual machine Virtual machine VMware vCenter Server Appliance State Failed - File /vmfs/volumes/vsan:52a6dc1103bec2ea-ecdd3af1fcecde03/5434955a-4554-d764-f61c-246e966e5318/VMware vCenter Server Appliance 6.5.vmx was not found Errors If I search the datastore, I can see the files. So the storage is certainly visible. I'm not that into VmWare, so i'm a little lost as to where to search for the problem. Anyone that can steer my in the right direction ?"
56.json,t3_cguhpf,t1_eum5blt,2019-07-24 03:50:42,ok problem resolved. A simple restart of the vCenter service fixed it.Puttyservice-control --stop --allservice-control --start --all
56.json,t3_cguhpf,t1_eunow98,2019-07-24 09:26:58,I was just going to suggest a restart of vcenter services.  Vcenter is the brain. Glad it’s working.
57.json,t3_9x28ib,Backup and DR for VxRail,2018-11-15 01:38:24,"What is everybody doing for backup and DR for the VxRail? Short of having second VxRail to DR to what other options? A cheaper SAN? We are still just getting started with our VxRail and are using Veeam. However, soon we will be migrating some bigger items and I will need to plan out the DR."
57.json,t3_9x28ib,t1_e9pseos,2018-11-15 08:30:58,I am using Azure Site Recovery for DR and Veeam also for backups.
57.json,t3_9x28ib,t1_e9qma6x,2018-11-15 18:39:35,Have you looked at RP4VM? You get 5 free licenses per node bundled into your VxRail System - if you wanted to test feature functionality. VM's can be replicated to an external vSphere cluster. RP4VM is now optimised for vSan and can be deployed and configured in an hour or less.
57.json,t3_9x28ib,t1_e9qmba3,2018-11-15 18:40:35,I’m also using Veeam for backups. Backups go to a EMC Data Domain which is replicated to another datacentre.
57.json,t3_9x28ib,t1_e9r1lfs,2018-11-15 23:23:17,"I was aware we have the licenses, but I haven't explored this. Thanks!"
57.json,t3_9x28ib,t1_e9r1mdy,2018-11-15 23:23:37,Thanks...I will check it out.
57.json,t3_9x28ib,t1_e9r1qw9,2018-11-15 23:25:13,Thanks!
57.json,t3_9x28ib,t1_e9rm7js,2018-11-16 03:41:22,"We initially considered RP4VM, but decided to go with vSphere replication + SRM."
57.json,t3_9x28ib,t1_ealfalv,2018-11-28 04:36:48,Any specific reason you chose not to use Recover Point?
57.json,t3_9x28ib,t1_ec8ihvi,2018-12-21 15:03:17,"In an ideal world, RP4VM would work wonders, but there are limitations if the CG (Consistency Group) is not ""Active"". If a VM is powered down within a CG, the whole CG is in an ""Error"" state and RP4VM would fail. Have you run into any of these sorts of issues?"
57.json,t3_9x28ib,t1_echwqsp,2018-12-25 10:36:33,How do you fail that back?
57.json,t3_9x28ib,t1_eck66ox,2018-12-26 10:02:21,"The fail back procedures are to heavy for our liking, so since we are primarily SQL, we just copy the databases back to onprem and call it a day. I see us   100% in the cloud by end of 2019, so it is currently meeting our needs."
58.json,t3_43m0id,[Tech] 'VxRail' looks like EMC's next-gen hyper-converged appliance,2016-02-01 09:07:32,[]
59.json,t3_6vnbjf,VxRail experience?,2017-08-24 08:39:50,"Hey everyone, just like the title says, I'm looking to see how everyone's experience has been so far with VxRails.  I'm on an effort that's going to be purchasing five of the S470 nodes to run NSX on top of for a multi-tenant environment.  It seems like it's a pretty slick offering but it is a big change from a more traditional approach with a bunch of racks/blades and hooking them all up to a beefy SAN.  The other thing that will be weird as well is the ""chicken and egg"" problem where the VxRail installer needs a DNS server but this project will be standing up new domain controllers on the VxRail itself for a shiny new environment.  I'm wondering if anyone has solved this before as well. Thanks in advance for all feedback!"
59.json,t3_6vnbjf,t1_dm1s685,2017-08-24 12:01:59,"Since no one else has chimed in yet, I'll provide my very limited experience with it.  Back in November 2016, VxRails was brought in as a POC at a previous place I worked at.  I was on the networking team and therefore, responsible for all things networking related for the POC.  I worked right next to the server guy that was doing the majority of the POC work.VxRail never made it into production because the POC failed miserably.  Here is what I remember:-From day one, there was inconsistent information on implementation details from the PS team helping with the POC-A full OS rebuild needed to be done more than once due to configuration errors/changes--The rebuild could not be accomplished by us or even the PS team, it had to be escalated to someone in support who had access to the ""special script""-It really felt like there was a lack of maturity with the productThere were other ""issues"" but I'm hesitant to call them out because I don't remember the specifics.   Hopefully things have improved significantly since then."
59.json,t3_6vnbjf,t1_dm1sxlr,2017-08-24 12:23:07,"Hate VxRail with a passion. We've replaced 50% of our nodes due to hardware failures. Bad memory? That's a node replacement. Bad fan? That's an node replacement. So we wait for EMC to send the node, and, and then we wait for them to dispatch an engineer with the USB to install the VxRail software. And then we hope that the engineer understands that the start time is a hard deadline because our hardware is at a remote datacenter so if they don't show up it wastes our time. Engineer no shows are extra fun.We won't be extending the warranty on this hardware. As soon as it's had it's three years it's getting shut off and recycled. VxRail has wasted so much of our time compared to a rack full of 1U servers and fiber channel. EMC has lost our business over VxRail. Everything EMC is getting swapped out for other vendors over the next 18 months.Others we've talked to have similar experiences with multiple VxRail appliances have similar experiences.Maybe the Gen 3 hardware is better, but we're not spending money on it to find out.Edit: Oh, and write I/O on an all SSD GEN2 sucks. May as well get a USB 3.0 spinning drive. Not sure how you fuck up SAS SSD storage that badly, but it's possible."
59.json,t3_6vnbjf,t1_dpndbzg,2017-11-11 10:04:11,"Shit.  I just got approval to purchase.  We're really small, 50 users and 5 VM's that I need HA for.  wow."
6.json,t3_8yot4s,vCenter 6.7 and repointing SSO domains,2018-07-14 07:23:52,"I came across many articles about this feature being back in 6.7 but I haven't tried repointing it just yet. Anyone else? It'd be to an external PSC, but it's 6.5 one so I'm not hopeful...I can't upgrade that until VXRail has an upgrade package for it...meh. That alone is a long frustrating topic for another day."
6.json,t3_8yot4s,t1_e2d09k7,2018-07-14 12:28:58,Not supported. Must be 6.7.
6.json,t3_8yot4s,t1_e2d1u4a,2018-07-14 13:05:47,"Womp, womp, goodbye linked vcenters for awhile. At least I found out this fling is bad ass: https://labs.vmware.com/flings/cross-vcenter-workload-migration-utility"
6.json,t3_8yot4s,t1_e2dtlgi,2018-07-15 01:00:45,"The only feature being ""back"" in 6.7 is repointing vCenter to another PSC in a different Site within the same SSO domain. This feature was/is ""broken"" in 6.5 (I use quotes here because I have yet to hear if this was an actual design decision, or if it is indeed a bug that has yet to be addressed).In 6.7, you can migrate vCenter to another completely different SSO domain."
6.json,t3_8yot4s,t1_e2dze9u,2018-07-15 02:39:44,"I'm voting it was a bug or simply forgotten about since it isn't too common of an issue. Then again, when you do need it it really sucks not to be able to.One day I'll have a single SSO domain and linked vCenters....then find something else to complain about."
60.json,t3_8epyvk,Hyperconverged,2018-04-25 09:56:39,Has anyone had any success/experience with Hyperconverged infrastructure? Any pointers/recommendations to go on? Looking to replace a 3 host blade + storage with it. Using vSphere/ESXi for the hypervisor.
60.json,t3_8epyvk,t1_dxx668g,2018-04-25 10:03:54,Check out starwinds. I'm running a few HCA from starwinds and all of them have been solid.
60.json,t3_8epyvk,t1_dxx7vym,2018-04-25 10:33:42,"I'm not sure how deep down that rabbit hole you want to go.  VMware just dropped an updated version of their VMware Validated Designs at their VMWare Empower event in Atlanta last week.  Good luck.https://docs.vmware.com/en/VMware-Validated-Design/index.htmlThat said, why would you want to do this?  HCI and SDDC sound great, but I don't see any advantage there unless you are willing to buy the (expensive new) gear on the hardware compatibility list and are planning on deploying rack after rack of gear.  IMHO VSAN is a gimmick.  BTW ... DON'T do VSAN on just three hosts.  Imagine what happens when one goes down ..."
60.json,t3_8epyvk,t1_dxx8dkk,2018-04-25 10:42:23,"You just can’t beat a traditional fiber channel SAN and a bunch of 1Us. Maybe the new Dell chassis that lets you pack four compute nodes in 2U is worthwhile.I never want to see another blade chassis or hyper-converged solution again. Blades made sense when you needed that density, but current processors allow you to get that core density without the complications of a chassis and blade solution.Everyone on our team hates VxRail (inherited it from previous admins) and instead of expanding that solution we got ourselves a shiny new SAN and and 1Us. When our VxRail hardware goes EoL it will also reduce our VMware costs by a significant amount as we’re also moving Citrix workloads back to XenServer."
60.json,t3_8epyvk,t1_dxxa2tz,2018-04-25 11:13:50,"Deployed a Nutanix 3 node cluster recently - Ended up running Vmware on it based on a Lenovo sales pitch - next time id just use the Nutanix AHV. Pricey, but time will tell."
60.json,t3_8epyvk,t1_dxxaudp,2018-04-25 11:28:46,"Similar to saying don't do RAID 5 on three drives, imagine if one dies."
60.json,t3_8epyvk,t1_dxxb406,2018-04-25 11:33:59,"We have 4 Nutanix AHV clusters, very happy with the solution."
60.json,t3_8epyvk,t1_dxxbg8m,2018-04-25 11:40:42,"Experience with HCI from two manufacturers: Maxta and vSAN. Both work. My experience with vSAN has been that it just runs. I can't recommend enough that you go all flash with vSAN as it just cuts out any question about performance of the storage tier and it doesn't cost much anymore. Also, don't go in with less than four hosts, even if you have to buy slightly less beefy hosts to fit budget.Four 1U or 2U servers (depending on your storage needs) will run a Hell of a lot of stuff very fast. One last piece of advice: make sure everything is on the HCL, particularly the I/O controller and all the disks."
60.json,t3_8epyvk,t1_dxxhwkw,2018-04-25 14:21:04,[deleted]
60.json,t3_8epyvk,t1_dxxmcci,2018-04-25 17:00:48,Sure you can beat it.  I've been running Ganeti for years.  No SAN needed.
60.json,t3_8epyvk,t1_dxxoo8c,2018-04-25 18:31:09,"How's the performance of the units going? Any gotcha's of Nutanix? How does AHV compare to VMWare? Can you easily shift the VMWare workload to it or is it basically a ""p2v"" style migration?"
60.json,t3_8epyvk,t1_dy28tvu,2018-04-27 22:44:41,"Just deployed Cisco Hyperconverged cluster with 7 tb storage. They are ESXi boxes (3) and are performing beautifully. They are running WS 2012 R2 (AD) x2, HighJump WMS (WS 2012R12 OS VM, A Domino server, etc.  Couldn't be happier. It's all hooked up to Cisco Fabric Connects and a full Meraki network - switches, security appliance, etc. Performance is markedly better than our old traditional VM/storage setup, but to be fair old storage was mechanical drives, new setup is all SSD...I was somewhat dubious about the whole hyper-converged thing, but the demos and then real-world performance has made me a convert.  Administration is a breeze between the hyperconverged cluster and the Meraki management tools..."
61.json,t3_6g61oc,How difficult is it to deploy vm horizon?,2017-06-09 11:52:37,"Just purchased VXRail setup and VM Horizon with credits that were suppose to be  used for professional services. However, looking at the SOW it looks pretty basic and my team thinks we can handle it and use the credits towards training and vmworld tickets. Anyone have any experince either deploying or working with VMWare professional services? Thanks in advance"
61.json,t3_6g61oc,t1_dinuhlb,2017-06-09 12:40:24,How big / complex is your environment and what level of VMware and windows skills do you have?
61.json,t3_6g61oc,t1_dio367i,2017-06-09 18:33:04,It comes down to how you want the end user experience to be. If you are just doing a basic desktop horizon is quite easy.... But... If you are looking to serve up multiple different desktops and have people who log into each one and you want their setting to travel. It becomes much more involved
61.json,t3_6g61oc,t1_dio8prx,2017-06-09 21:28:46,"From my experience the Horizon infrastructure is very straight forward if you understand your org's needs and follow the documentation.But like intensityjunkie stated, it's the user experience design that is going to be the most difficult component. You need to build + design your desktop environment to the needs of your users. You may find you need to use UEM or Writable Volumes (AppVolumes) for user settings/configurations.My advice for desktop resources + design: Start small and take your time. You can't take your standard desktop PC and replicate it in View.  I can guarantee that 99.9999% of the time it won't work.Also - Build it to please the ""normal"" 95% of your users. You will not be able to please everyone, and that 5% you will never please. Just let them have their own laptop/desktop and let them be on their own.Build your OS image - optimize it, make it as lean and quick as you possibly can. Then (my preference) use AppVolumes where possible or gradually introduce applications into your View template. Install -> Recompose -> Test -> Optimize -> Repeat."
61.json,t3_6g61oc,t1_dio8s4k,2017-06-09 21:30:19,pretty basic environment. VMWare wise id say the team is intermediate with vSphere and vmware in general
61.json,t3_6g61oc,t1_dio9tgw,2017-06-09 21:53:30,"Setting up the basic infrastructure was super easy. Connection servers, composer server, even the non essentials like AppVolume servers and setting up UEM services were fairly straight forward to establish. I did have some trouble with the KMS server and Windows 10 LTSB but I believe I have that largely resolved. I even setup the VMware Access Points in our DMZ (Now VMware Unified Access Gateway/UAG) and they were fairly straight forward. Dealing with some of the certificate stuff was a little burdensome because initially I had set the connection servers to use our internal domain. This caused issues because when I went to setup Horizon for access from the internet, I wanted to bind a valid SSL cert to it and because of the difference from my external DNS name and internal DNS name we got a lot of errors. I ended up having to make everything setup to the external domain for simplicity sake.Setting up all that infrastructure was pretty easy. The hard part for me, is getting back into the desktop frame of mind. You have to do a lot of research on various applications and how to set them up for VDI. You'll have challenges like, how you want users to store and save data. Where you want users to save data. How much of an impact will this user saved data have on your current storage footprint. Do you have the IO necessary for a good experience on that storage? Should power users be able to install their own apps? If so how do you tackle this? What icons should be on a user's desktop? How do you deal with endpoint protection? Do you want users to have audio and video on their desktop? How do you deal with their experience for audio and video? Do you use Blast or PCoIP? Do I do task x in UEM or do I do it via GPO? Which one do I chose? Does it have any impact on login times?I rolled out Horizon back in March and now I am finishing up AirWatch and Workspace1. I am still trying to wrap my head around the entirety of the solution because there's just so much to it. I can tell you that the infrastructure part is easy for Horizon, but getting the desktop piece just right, is going to be a long and arduous process for me. I am going to need to sit with users from different departments and understand their individual needs. I am going to have to sit with them after I deliver and work with them on their experience. I will then need to do the whole experiment over again with our remote users and make sure they are okay as well. VDI setup... pretty basic. VDI administration and deployment... crap, it's a lot of work."
61.json,t3_6g61oc,t1_diofsay,2017-06-09 23:48:12,"I just went through a Horizon View setup. The base configuration (Connection Server, Composer, Golden Images, etc...) was super easy. Getting all of the onesy-twosy hardware devices working properly using thin clients was the PITA.Just found out that our EMV card readers won't work with thin clients because they present as COM ports and the Linux View Client doesn't redirect COM ports, only Windows supports COM redirection."
61.json,t3_6g61oc,t1_dip33kt,2017-06-10 07:22:23,"Good advice. This was my teams thinking for calling off our professional services.  If where going to start small and learn, we felt that paying someone to come in and do the easy stuff didn't make sense. Thanks for the advice"
61.json,t3_6g61oc,t1_dip35zi,2017-06-10 07:23:59,Great feedback. How has Vmware support been with your linux COM issue?
61.json,t3_6g61oc,t1_dipf8gb,2017-06-10 12:06:54,Pluralsight has some courses that might serve as a reasonable crash course.
61.json,t3_6g61oc,t1_diuxv4f,2017-06-14 03:52:54,"Agree 100% with everything except using writables for profiles unless it's targeted, like just for OST files and so on. Use UEM for profiles."
62.json,t3_71bmu9,I need help disabling OMF on a Nexus 3k for a VxRail deployment.,2017-09-20 23:00:47,We are standing up our first VxRail deployment. We have Nexus N3K-C3172PQ-XL ToR switches running NXOS release 7.0(3)I2(2e). The VxRail network configuration documentation requires IGMP Optimised Multicast Flooding to be disabled on the VLANs used for peer discovery and management. The output from show ip igmp snooping shows that OMF is enabled on the those VLANs.  A search of the Cisco documentation says the no igmp snooping optimise-multicast-flood command was removed in NXOS release 7.0(3)I2(1) (yay me!) and I am not seeing the replacement command. Have any of you run into this or know how to disable OMF in the post 7.0(3)I2(1) releases? Thanks.
62.json,t3_71bmu9,t1_dn9rrle,2017-09-21 02:04:55,"Feel free to call me names for asking this, but have you actually tried the command on your 3Ks? Also, can you link to where it states that was removed? I did a quick search in the 3K release notes for that command and don't see any mention of it being removed."
62.json,t3_71bmu9,t1_dn9uzg9,2017-09-21 03:00:52,If you do a search for NXOS release 7.0(3)I2(1) then for igmp snooping optimise-multicast-flood that command shows it has been removed. I've added the link to where i found it in the release notes.https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus3000/sw/release/70321_Overview/CiscoNXOSRelease70321Overview.html
62.json,t3_71bmu9,t1_dna2a62,2017-09-21 05:06:36,"No names for asking, it is a reasonable question. I tried to keep my post brief and in this case it was possibly too brief."
62.json,t3_71bmu9,t1_dra6lwv,2017-12-15 17:34:29,"so, any news ?"
62.json,t3_71bmu9,t1_drcve4j,2017-12-17 09:10:48,"The system is in operation and it is running as expected. I will probably end up putting in a TAC case, and if I do, I will post their answer."
63.json,t3_574dr9,Has anyone moved from a vblock to vxrail?,2016-10-12 22:38:10,"Our lease is up on the vblock, and the vxrail looks inviting.  We plan to utilize unity with either choice, away from vnx, but we are not sure if the vxrail is actually a wise replacement to the vblock."
63.json,t3_574dr9,t1_d8ou6z3,2016-10-12 23:00:43,Most of our customers are buying Vxrails and moving test workloads to them to see how the consumption goes.
63.json,t3_574dr9,t1_d8p05p4,2016-10-13 00:59:09,Super interested in the responses to this - We're in the midst of debating another vBlock purchase vs moving to Rails.
63.json,t3_574dr9,t1_d8p0g7d,2016-10-13 01:04:43,What type of customers/applications?
63.json,t3_574dr9,t1_d8phw52,2016-10-13 07:05:27,"So, a friend of mine in the industry is a VXRAIL guy for VCE / DellEMC. Here is his take on your post:""I had three meetings today. All 3 of them were with customers that had vblock expiring, and they are looking to VXRAIL. One of those customers has $5 million in vblock. They're looking to migrate SQL off the block first.""So, I'd say that customers are moving from block to rail :)"
63.json,t3_574dr9,t1_d8pnh0x,2016-10-13 09:19:01,"When you can push 6.7 million iops in a VSAN cluster and have customers running over a PB in a cluster, the questions about HCI vs CI becomes an interesting one.If you have any VSAN questions hunt me down in Barcelona next week or DM me and I'll get you in touch with the right people."
63.json,t3_574dr9,t1_d9c5pro,2016-10-29 07:53:10,"To help with your research, here's a link to VxRail in the Dell EMC Store.  You can also search and compare products within the Dell EMC family.  https://store.emc.com/Product-Family/VCE-Products/VCE-VxRail-Appliance/p/VCE-VxRail?CMP=listenRDvxrail"
64.json,t3_5mu0rs,Has anyone here used VMWare VSAN?,2017-01-09 07:27:42,"Looks like VSAN may be added to a network that I look after. Looks like it requires multicast. There's no multicast in this network currently. Can anyone make any recommendations or suggestions on the config? For example, is the PIM sparse mode the way to go? Any pitfalls to watch out for? We are using N9K's. It's a small deployment. Thanks"
64.json,t3_5mu0rs,t1_dc6ihke,2017-01-09 09:44:45,"I'm assuming you are starting with a fairly small install, so less than a rack with a single ToR pair. If it is larger the design could become more complicated.VSAN only requires IPv4 multicast on the VSAN VLAN, this should be a non routed VLAN dedicated to VSAN traffic. To do multicast on a non-routed VLAN the recommendation is to have IGMP snooping on, and configure IGMP querier on the VLAN.So on one of your 9ks you would do ""ip imp snooping queried IPADDRESS"" under the interface configuration for the VSAN non routed VLAN.Are you running bring your own hardware / ready nodes or a VXRail appliances? VxRail has some additional networking requirements to support the automated hardware discovery and setup.Edit : on a routed VLAN sparse mode is the way to go, but there is no need to route the VSAN VLAN so querier just works."
64.json,t3_5mu0rs,t1_dc6oehf,2017-01-09 12:06:36,"You don't need to do anything special - it is layer 2 multicast, not layer 3 multicast which is where a PIM deployment would fit in.In the VLAN which you are supporting L2 multicast for VMware VSAN, please do not place any other endpoints that don't need to be in there.  That eliminates the need for IGMP snooping and simplifies your design even further through good VSAN architecture.This is the type of deployment where it is going to make a lot of sense to prune your VLANs manually and only permit what is absolutely necessary in places where you need them to be.This documents gives a good rundown.http://blogs.vmware.com/vsphere/2014/09/virtual-san-networking-guidelines-multicast.htmlAs far as general VMware VSAN comments, the last time I looked at it, it was a weak product compared to something like Nutanix.  The main issue we had with it when we evaluated it was the minimum amount of nodes that need to be deployed to provide enterprise-class redundancy.  VMware was not upfront about what the true requirements were so we got sour on it rather quickly and never looked back as other HCI products were technically superior."
64.json,t3_5mu0rs,t1_dc6og9e,2017-01-09 12:07:52,Thanks. I'll go with L2 then.I've never used querier before. Can I select any unused IP address for it? Are there any guidelines on what I should use?We'll be using VxRail appliances. I'll have a look around to see what's needed to get them going. Anything unusual?
64.json,t3_5mu0rs,t1_dc6oopl,2017-01-09 12:14:01,"Thanks. It's looking simpler (for a small deployment) than I expected.The Nutanix products look awesome. I wish we had some. I would rather them to VxRail. I'm not really the 'compute guy' though, so it won't affect me too much.Out of curiosity, If we grew to two sites, would that require L3 multicast, or would VSAN be better with a stretched VLAN, OTV or VxLAN? Or are they best left isolated?"
64.json,t3_5mu0rs,t1_dc6ujlv,2017-01-09 15:15:46,See also vmware
65.json,t3_5u5ibr,Navision admin absolutely refuses to let us cluster SQL servers. Anyone use Navision in a clustered environment?,2017-02-15 13:01:26,"Hi All, I'm a sysadmin/solutions architect for a small MSP. So a couple months ago we took over a client from another MSP that had seriously neglected the client's systems. No updates on any servers or workstations in years, Windows 2003 terminal servers/file servers/DCs, port forwarding for RDP instead of VPN, 192.168.0.0 network, ESXi 5.0 that had not been updated in ages, a maxed out DAS with no redundant paths to the ESXi servers and a dead controller card (thankfully not the one where the ESXi servers were attached). We were given a large budget to upgrade everything so instead of fixing all of these issues in a series of smaller projects, we bought a VXRail and will be building a brand new environment in parallel to the existing one and moving the users over to that. About the only thing relatively new in the existing infrastructure are two Windows 2012 server VMs running Microsoft Navision 2009 R2 and SQL Server 2012, one production and one dev. Being their ERP app, they have a Navision consultant who, unfortunately, has been a thorn in our side since day one. For example, when we initially proposed the solution, he turned to the CEO asked why not put Navision on physical server with two SSDs and ""back them up with RAID1."" A couple months before, he threw a fit when we refused to give him a public IP to RDP to the SQL server and instead asked him to use Kaseya. ""I access all my clients servers using RDP, why should this client be different."" Worse yet, he's been around a while and has the CEOs ear. While the CEO definitely is sympathetic to our cause, he tends to side with the consultant to keep the peace. Apparently consultant is great with Navision but on all topics related to hardware/virtualization/windows he seems to know little enough to be of any use and just enough to cause trouble. Since we already have two licenses for SQL server 2012 and cluster compatible shared storage via the VXRail, we planned to put the SQL servers in a failover cluster. However, when we informed the Navision consultant,the pearl clutching was violent and immediate; he refused to work with a system that was in a failover cluster due to ""performance issues."" Now I know almost nothing about Navision and do not know how it would perform in a clustered environment so I suggested that we just have Navision server running on seperate VMs (using NLB) and use failover clustering for the SQL servers. Refused to do that because it would change the client configuration for Nav (not sure why this matters though because we're going to eventually reimage all the workstations and any new config can be tested in the new environment.) He ended by essentially threatening to drop the client if we used failover clustering and calling the CEO to make his case. Personally, I'd love to call his bluff but given that I know almost nothing about Navision  I'm not in a position to do so. Currently, we're imaging the Navision/SQL servers with Acronis and storing them on a NAS; every 2 hours, we take a SQL dump and back it up offsite. Basically, in the event of failure, we need restore from the SQL dump on the secondary server or spin up the image from Acronis and import the data from the last SQL dump. This is the solution the Navision consultant wants to continue using but it seems silly to do this when we already have all the hardware and licensing to instantly failover in the event of a windows failure. Would reduce data loss, recovery time, and make maintenance a hell of a lot easier, especially considering if the thing goes down, I'm the one who has to bring it back up, not the conultant. So with all that said, am I right to think the consultant is being unreasonable? Does anyone have experience working with Navision in a clustered environment?"
65.json,t3_5u5ibr,t1_ddrj12t,2017-02-15 13:18:32,"If you set it up you do realise any and every problem will be blamed on the SQL cluster.Don't do it, you're asking for a lot of pain from them."
65.json,t3_5u5ibr,t1_ddrlg26,2017-02-15 14:39:26,Its probably not worth the fighting if the consultant has the CEO's ear on this.Set up an offline VM Replica solution and that way you're at least prepped for hardware failures. Assuming the issue isn't data corruption (which would cause issues with an SQL cluster anyway) you have a reasonably fast restore option.
65.json,t3_5u5ibr,t1_ddrms0m,2017-02-15 15:33:46,"I know modern versions of navision sql will run in a cluster. I couldn't tell you about ""performance issues"" though. Have you considered opening an incident with microsoft or a sales a call with their navision guys to ask them? while generally clueless, they are generally eager to do the research and get back to you at least.Also, seriously, find another consultant to cover this field and swap it. If you aren't comfortable with your external vendors, it always ends badly."
65.json,t3_5u5ibr,t1_ddrnv3s,2017-02-15 16:24:28,"What is the point of using clustering in this case? What are you trying to protect?Let vSphere HA do the work to cover the risk of hardware failure.If application is failing, then go back to the consultant to fix a crappy app."
65.json,t3_5u5ibr,t1_ddrusnv,2017-02-15 21:38:13,Why not check what the product supports and then throw Microsoft best practices at him?As far as performance issues talk to the CEO about your concerns that's he's circumventing simple stuff like raid is not backup etc. Let him stick to app support and let you do your job.
65.json,t3_5u5ibr,t1_ddrvy21,2017-02-15 22:09:04,Ask Microsoft.
65.json,t3_5u5ibr,t1_dds2j9h,2017-02-16 00:27:39,"Things don't always fail that would trigger a VMware HA event. Hung services, rebooting servers for patch updates, service packs and so forth. None of this would trigger VMware HA and would cause downtime otherwise."
65.json,t3_5u5ibr,t1_dds55wy,2017-02-16 01:17:17,"In this case it would most likely be used more for maintenance than for DR. Having a cluster with a single IP lets me move SQL to a different host while I run updates. It's hard to get downtime for this server because we have a lot of field sales people in a lot of different timezones using TS. Still, if I have a host in my VXrail go down for whatever reason or this consultant somehow fucks up windows, SQL will continue to run without downtime."
65.json,t3_5u5ibr,t1_ddvszoe,2017-02-18 07:16:35,What about always on availability groups instead of clusters?
66.json,t3_6amcre,VxRail,2017-05-12 03:22:21,Anyone have experience with VxRail? I am getting quotes from both Simplivity (HP) and Dell on a hyper-converged system and haven't seem much about it on 
66.json,t3_6amcre,t1_dhfpoex,2017-05-12 04:11:23,STAY AWAY FROM EMC.We have had a project thats well over a year in the process and its complete shitshow.I would stay away from the VXRail. They will take your money and never go through with it.We are pending legal at this point.
66.json,t3_6amcre,t1_dhfq406,2017-05-12 04:19:00,Can you explain a bit more? We sell some EMC models (vnxe line) so I'd like to see if we should be putting something else in. Thanks!
66.json,t3_6amcre,t1_dhfwniz,2017-05-12 06:22:14,"If you like HCI and you're already quoting Simplivity, why not look into Nutanix? Nutanix is usually quite a bit less money than Simplivity."
66.json,t3_6amcre,t1_dhfxwe8,2017-05-12 06:48:55,I've just this week gone through a demo with Starwind for their HCI product and was impressed.  Its also a fair bit cheaper than the other vendors mentioned.  Supports VMware and Hyper-V.  Integrated switching for the storage traffic means no having to fork out 100K to upgrade our ageing network infrastructure right now.
66.json,t3_6amcre,t1_dhfxwx1,2017-05-12 06:49:12,Yuck.......
66.json,t3_6amcre,t1_dhg1e98,2017-05-12 08:07:13,If you're a small shop take a look at Nutanix Xpress. Comes with a hypervisor so if your small shop is budget conscious you can avoid VMWare line item too.vxRail is purpose built for VMWare and aimed at larger shops more often than not. Sound product but if you don't want to wait around on support there are other options.
66.json,t3_6amcre,t1_dhg8bis,2017-05-12 10:42:49,"I just did a forklift upgrade of my environment and looked at various options of Hyperconverged vs 2+ servers and a SAN. Be careful with Simplivity, I asked the question if you can expand disk space by just adding drives and they told me you have to go out and buy bigger nodes to expand drive space.... NOPE.  Nutanix also was very expensive compared to a few HPDL360 G9 Servers + A PureStorage //M10 Array. I run VMware Essentials Plus on it.As someone else in the thread mentioned, you may want to take a look at Starwind Software's paid Virtual SAN. I have them running in my home-lab, used to use VSAN until I got my VCIX then dumped it for Starwind. It's pretty slick and I heard they are coming out with a Linux Appliance version of the VSA."
66.json,t3_6amcre,t1_dhg8fbm,2017-05-12 10:45:17,"Yes, you can just run DAS cables between servers via a few 10Gb HBA cards or 40Gb Mellanox cards."
66.json,t3_6amcre,t1_dhgbx3z,2017-05-12 12:17:13,How much does a Pure M10 go for?
66.json,t3_6amcre,t1_dhppc7n,2017-05-18 16:29:43,"While IT Central Station does not yet have reviews for VxRail, you can see how they compare to other hyper-converged infrastructure solutions here: https://www.itcentralstation.com/categories/hyper-converged-infrastructure/tzd/c330-sr-42.As an example, users interested in VxRail, or solutions like SimpliVity, also read reviews for VMware vSAN. You can see a direct comparison between SimpliVity and vSAN from the IT Central Station user community here: https://www.itcentralstation.com/products/comparisons/simplivity_vs_vmware-vsan/tzd/c330-sr-42.I hope this is helpful, and good luck with your search."
67.json,t3_an8bew,[Career Advice] Would you leave in my position?,2019-02-05 08:23:27,"My company was recently sold, so we're currently transitioning from being a subsidiary of a publicly traded company to being owned by a private equity firm.  As a result, I'm a bit spooked and have been testing the waters of the local job market.  Just not sure if the new owners plan on ""business as usual"" as everyone has been saying, completely liquidating, or something in between.  So there is job security element to be considered.  To add to that, I think I may be a bit underpaid (75k, Cleveland, OH) for the amount of responsibilities I have (manage entire 'infrastructure' across US and Canada - ~2000 nodes and two datacenters and all of the network and servers involved - along with one other guy). However, I work from home 100% so that's basically priceless, and next fiscal year we have a ton of resume-building projects in the budget (submitted, not yet approved), including implementing a new backup solution, VXRail and the associated migrations, setting up a new DR site and migrating from strictly MPLS to SDWan. Part of me says I should stick around and get the experience coming to me next fiscal year, but another part of me says I can be making more for a company that I'm more sure isn't going to be suddenly reducing their IT needs due to a complete or partial sell off. I guess I'm just looking for any input at this point.  Should hold on to this potentially underpaid, 100% telecommute, experience building job for dear life or should I leave for somewhere that pays more and has better job security?"
67.json,t3_an8bew,t1_efrgjhh,2019-02-05 08:28:48,"""Private equity firm"" is never great news. You could ride it out, sounds like a decent gig, but make sure your resume is updated and you have some savings."
67.json,t3_an8bew,t1_efrgyoc,2019-02-05 08:34:41,Yeah that's what I'm thinking.  I've recently established relationships with three recruiters in the area and told them that it's not urgent at the moment but to keep me in mind.
67.json,t3_an8bew,t1_efrh9zr,2019-02-05 08:39:07,"If you are not sure if you should leave, then it sounds like you are not unhappy but maybe just feeling uneasy about what is to come (understandable).  Don't be afraid to start looking at options and maybe even networking with other people at other companies in your area.  If something comes up that is too good to pass up, you have your answer.  Keep in mind, things could turn out to be too good to pass up there as well if you get through the transition."
67.json,t3_an8bew,t1_efrhlrt,2019-02-05 08:43:39,It doesn't hurt to go on a few interviews to get a feel for what you're worth and what is out there.
67.json,t3_an8bew,t1_efrhm4m,2019-02-05 08:43:47,"Good points, appreciate the insight."
67.json,t3_an8bew,t1_efrhw1x,2019-02-05 08:47:40,I'd keep one eye open on what's out there. I have always believed if your not keep one eye on what's available your not doing yourself any favors. It's a job not a marriage.
67.json,t3_an8bew,t1_efri2jo,2019-02-05 08:50:11,Honestly a 75k 100% at home job is pretty ridiculous. You should think hard about what it's going to mean to actually enter the workforce again. You could easily make less for more time spent each day
67.json,t3_an8bew,t1_efrk6r2,2019-02-05 09:19:35,When is the last time you were on the market?
67.json,t3_an8bew,t1_efrkmjz,2019-02-05 09:25:28,"Been with this company for 6 years, moved from desktop support to systems administrator about 3 years ago."
67.json,t3_an8bew,t1_efrlodw,2019-02-05 09:39:42,Leave and get 25k more.
67.json,t3_an8bew,t1_efrp8y8,2019-02-05 10:28:36,"Start looking now, if it goes to shit, by the time you realize it, you'll probably have a few offers in hand."
67.json,t3_an8bew,t1_efrw2i1,2019-02-05 12:04:38,"Stay, work through your projects, but have a backup.Cleveland isn't a great Market, I worked there for two years as a consultant making way more than the locals.Like others have said, if you aren't unhappy and have a sweet gig with the telecommute, there's no current harm with staying until that status quo changes."
67.json,t3_an8bew,t1_efso0x1,2019-02-05 22:18:40,"One thing that happens in a merger is overlap so there will always be layoffs.  One thing that always happens with an equity firm buyout is they try to maximize value before selling the company off, in pieces or as a whole.  That means cutting the headcount quickly and outsourcing.  In either scenario, it's time to refresh the resume and start looking.  Yes, it will suck to give up a 100% work from home gig (I'm jealous by the way) but it is better to be prepared then to be left hung out to dry by people who don't give a rat's ass about you."
67.json,t3_an8bew,t1_efsq63y,2019-02-05 22:48:01,"Good luck finding another 100% remote position. Me personally, I would ride that out as long as I could. Although, if I had to lay money down I bet they ax your 100% remote once things settle down from the transition."
68.json,t3_bsyntt,Possible to see who created a VM ~1 month ago?,2019-05-26 03:44:25,"Hey guys, We are running a VCSA 6.7u1 environment operating on Dell VXRail Hardware. Running into a little mystery in our VMWare environment. Last night I noticed that a VM was duplicated but with expanded hardware settings. The best I can see is that this VM was created on or before 5/1/19, which is at least when the machine started being backed up with VEEAM. I did poke through the VEEAM logs and I did not see any machine restores that could be used to duplicate the machine. Is there any way to see who created this machine? And if this machine was created using the  Thanks!"
68.json,t3_bsyntt,t1_eorvv1o,2019-05-26 03:48:15,"Best bet would be Log insight.If you have  vCenter Standard, 25 OSI licences are included. Great product."
68.json,t3_bsyntt,t1_eorx6p6,2019-05-26 03:58:03,"I will take a look into this. If log insight is installed after the fact, can it still retrieve the information I'm looking for? Do you know what constitutes a license to be used?"
68.json,t3_bsyntt,t1_eos6gvq,2019-05-26 05:06:20,Did you try clicking on the VM and going to Monitor > Events? It might be an entry in there if it’s been less than a month.
68.json,t3_bsyntt,t1_eos85zs,2019-05-26 05:18:53,"Or tasks, that usually has administrative operations in it"
68.json,t3_bsyntt,t1_eosbipy,2019-05-26 05:43:33,I thought the Log Insight that comes with vCenter is no more and you have to purchase the full version now?Get-VIEvent $vmname | sort CreatedDate  | select -First 1 | Format-List
68.json,t3_bsyntt,t1_eosd9xe,2019-05-26 05:56:54,How many people have the administrator@vsphere.local password? It was probably one of them...
68.json,t3_bsyntt,t1_eosgw8z,2019-05-26 06:23:07,"I had reached out to the people that had the password, but haven't heard back from everyone. There is an issue of lack of accountability, so I'm fully expecting everyone to respond that they weren't the one who did it."
68.json,t3_bsyntt,t1_eoshasy,2019-05-26 06:26:30,The earliest task or event I was able to see regarding this vm was the modification of two different settings done by the administrator account.
68.json,t3_bsyntt,t1_eotquob,2019-05-26 12:54:25,You already know who created it. Administrator@vsphere.local.Stop letting everyone use the damn admin account.
68.json,t3_bsyntt,t1_eots2si,2019-05-26 13:07:06,"Teust me, if I had any choice in the matter this isn't how it would be done!"
68.json,t3_bsyntt,t1_eoukbf8,2019-05-26 19:47:03,I would change the Administrator password then pull the NIC on the mystery server and wait to see who complains.
68.json,t3_bsyntt,t1_eov5zud,2019-05-27 00:48:20,"You need to AD integrate your SSO domain, have people start using individual admin account (not their normal account). Cant let everyone use SSO admin account.Change SSO admin password, write the password down and seal it in en envelope, and give it to your boss to open if you get hit by a bus.Can your OS logs give you any info? If the OS logs auth (it should) I would look and see who the first person to log in after the clone was. If it is domain admin or root you have more problems to clean up."
68.json,t3_bsyntt,t1_eov6k4b,2019-05-27 00:54:22,"In this case, the password is already supposed to be in a digital vault system that we use. The issue here is that the person who created the password is the senior technician, and we don't have the boss change after it is uploaded. This traditionally isn't a problem until recent. We do utilize SSO and I at least always use my account.Unfortunately (or fortunately) the server is offline and I don't think it's been used, so I can't go into the logs to see who hopped on after creation."
69.json,t3_agqqwq,Cloud Foundry Infrastructure Question,2019-01-17 06:31:37,"We are working with Pivotal on a project that will live in  a PCF environment, but also use their cloud cache engine (PCC/Gemfire) as well as a Greenplum DB instance. We have looked at Public Cloud (Azure), adding to our existing hosted private cloud/private storage environment, and on bare metal or hyperconverged environment like VX Rails. All 3 options are expensive based on the footprint required (80 VMS, 500GB memory, 3.5TB storage). At this point we are leaning towards vxRails in a colo rack located in a local hosting company. Before pulling the trigger, I wanted to reach out to the smart folks on Reddit and get some additional perspective. Thoughts / Experiences?"
69.json,t3_agqqwq,t1_eerv298,2019-01-24 00:08:53,What exactly do you want to know?
7.json,t3_58i3fg,"Dell EMC Brings PowerEdge Servers to VxRail, VxRack",2016-10-21 01:21:08,[]
70.json,t3_8uk2au,VXRail Feedback,2018-06-28 22:45:25,"Hi All, I've got a cluster with 3 x Dell R720's running dual E5-2670's and 256GB RAM each along with a 10Gbe EQL 5110 with a tier of 24 x SSD and a tier of 24 x SAS. The kit's now 5 years old and coming to end of Pro Support so we're looking to upgrade / move on. Capacity / speed has been great and we haven't outgrown the equipment, nor do we perceive we will for the forseeable future. Speaking to my Dell account manager they're heavily pushing HCI and in particular, VXRail. They've quoted me some E560's with Xeon 4110's, 384GB RAM, 2 x 800GB NVME cache along with 8 x 2.4TB 10k SAS capacity drives. I'm seekingany real world feedback with using HCI with 3 hosts and in particular, VXRail. I'm pretty nervous about the whole HCI piece given we're only looking at 3 hosts as I can't justify the expense of another server as the HCI solution is already more expensive than a 'traditional' setup."
70.json,t3_8uk2au,t1_e1g152d,2018-06-28 23:59:38,"I'm grumpy about my VxRail install, so take everything I say with a HUGE grain of salt.I'm running a pair of larger clusters (10 nodes each, all-flash).  Setup was a breeze - out of all of the HCI solutions that we looked at, they were the only one that we could actually get from the box to fully configured and running in a single day.Day-to-day operations are fine.  VSAN took some getting used to, but it's not a big deal once it's running.  And since the VxRail setup handles that automatically, there's not a lot of work to it once it's up.The VxRail Manager is mostly fine, but we see A LOT of false positive hardware failures.  There are bugs in the Dell agent software the talks between ESXi and the VxRail Manager.  Because we have ESRS call-home set up, there are at least weekly SRs automatically generated for non-events.VxRail software upgrades are supposed to be one-click affairs, but I've never experienced that.  My last upgrade attempt was a disaster.  I believe it was a 16-hour process per cluster.  I'm sure that's not typical, but I've talked to several internal Dell support people, and I'm not the only one who has had serious problems.Dell/EMC support for VxRail has been abysmal.  Legacy EMC support was always bad, and I don't know exactly how it's gotten worse since the merger, but it has.  I've gone weeks without responses on multiple issues.  Getting my account team involved helps some, but we just spent 8 figures on new storage recently so that may play into it.What's your use case?  General server virtualization?  ROBO?  VDI?A 3-node cluster should be fine, although I like 4 better, just for redundancy.  I'm nervous like that.If you're set on HCI, take a hard look at Nutanix before you commit to VxRail.  There are tradeoffs, but I'm feeling happier with that right now, as opposed to VxRail.  It does get you away from the VMware tax, if you're willing to use AHV.  I like that, even though it doesn't make any sense in my environment right now.If you can save money with a more traditional setup, and that's what you're used to running, then I'd do that.  There's no reason to change just for change's sake, and HCI is not enough better (or at all?) than existing hardware to necessitate a change on its own.Like I said, I'm grumpy about both HCI in general and VxRail specifically.  A small environment is a good place to go with an HCI solution.  But if it won't save you money and won't make your life easier, it doesn't do you any good.  That's my $0.02."
70.json,t3_8uk2au,t1_e1g2tfd,2018-06-29 00:22:17,I can't wait to rant/further this debate in just a bit...HOLD PLEASE.
70.json,t3_8uk2au,t1_e1g6jnr,2018-06-29 01:13:04,Currently deploying now; got a few sites done... My advice is to stay FAR away from VxRail!
70.json,t3_8uk2au,t1_e1g6knf,2018-06-29 01:13:25,You’re welcome.
70.json,t3_8uk2au,t1_e1g73wo,2018-06-29 01:20:43,"I am more of a fan of building your own VSAN versus vxRAIL due to flexibility, personally.  I just like the flexibility of hardware and licensing configurations.Have you engaged your preferred VMware reseller at all and inquired about doing a VSAN assessment?In my experience, it's much less pricey and more flexible to build VSAN using ReadyNode configurations.  There are a ton of things you can do to manipulate ReadyNodes as well (for instance, I'm a fan of building VSANs with single processor servers because you can lower the licensing costs while increasing the utilization because it alters the percentages of resources you need to keep ""free"" for high availability).Also, because all-flash enables features like erasure coding, deduplication, and compression, and because all-flash only uses the cache tier for writes, you need less raw storage to achieve the same effective usable storage, and all-flash VSAN configs tend to be less expensive actually than hybrid hard drive backed VSANs and you get the bonus of much higher performance.The VSAN assessment will actually analyze your storage usage statistics in your current VSAN environment and help visualize the ROI of looking at all-flash ReadyNodes.If you're in the US, I'm happy to help facilitate and connect you to a reseller that can assist if you'd like.  (Full disclosure: I work as a VMware consultant within a VMware distributor that supports VMware resellers in the US.)"
70.json,t3_8uk2au,t1_e1gabka,2018-06-29 02:04:34,"This is really the heart of it for me. I can't stress enough how terrible our support experience has been. They like to talk a big game about the advantages of having a single POC for support which is great if they respond. Even when they do respond I have found they often have little to no idea what is going on.I literally had the exact same experience with my last upgrade, and have had similar experiences with the other two upgrades we performed."
70.json,t3_8uk2au,t1_e1glnxq,2018-06-29 04:42:08,"VXRail! A single pane of glass! Everybody shares everything! How about no.We have E470 nodes (I believe) and it hasn't been fun. The install team (from Dell) set it up as it's own vCenter (it's on SSO domain despite our current one that is 6 times larger), took maybe 4 extra days and never got it connected to the brocade switch. Our infrastructure dude is the one that made that happen. *this is second hand knowledge, I started my job with this thing in place and running on hopes and dreams). So aside from a god awful install and flying by the seat of our pants initially I decided to get all the little features working because why not?It couldn't update from the vxrail manager, at all. It took 2 1/2 weeks for support to rebuild the vxrail manager appliance so it would finally update properly. The support guy has been awesome and blunt about everything though. I asked if that appliance was ever completely corrupted how do they do a fresh install...Are you ready?They don't and can't. They don't have a way to do it yet and expressed disdain in the fact that engineering has not yet developed a way to restore some of those special vxrail features/appliances if they fail. Just a little concerning in my book.So he spent hours upon hours upon days fixing ours, removing tmp files and all kinds of crap just to get the manager working properly again. When it works, well yea it's pretty cool to hit the easy button and update EVERYTHING. But if you like to actually manage your cluster that thing will spit out so many errors and warnings it's ridiculous because you changed some default settings to fit your environment. Heaven forbid.To be frank (unless someone else already is) I am frustrated with it. It was setup in it's on SSO domain (not the manufacturer's fault) but since it's proprietary crap to manage it if you repoint the VCSA to a different PSC you'll pretty much break those fun features/void warranty/who knows. Not sure if I can repoint it, deploy a different PSC to the VXRail, and allow it to keep working but eh, I need a break from fighting this and the servers are running :)I completely understand the vSAN purpose and all, but personally I'd rather have separate storage that isn't local on a bunch of hosts that share it with the others. I can't throw another host into this cluster to help balance anything or migrate off of a screwed up one. . Expanding the cluster involves buying more nodes from Dell/EMC...you can't do it on your own so get out the checkbook. You'll never need to add storage or anything again!...as long as you only use VXRail nodes the entire time. That limitation is not something I'm a fan of at all.If I'm managing an enterprise network I think the flexibility to add hosts/storage/etc that isn't brand specific is crucial. That and we can't go to 6.7 until that update comes out for the VXRail. (SSO issues is the only reason I know that. Stick with 6.0 for awhile!)"
70.json,t3_8uk2au,t1_e1gltjm,2018-06-29 04:44:24,3 nodes? You won't have HA or anything with that. Yes it's a starter cluster but you lose one and you're down...Usually they suggest it to get you started with the appliance because later when you want to expand....you can only reliably add VXRail nodes....bought from them.
70.json,t3_8uk2au,t1_e1gnmo9,2018-06-29 05:10:45,"Good God, why would you ever consider trading stone-axe simple setup like yours for a proprietary complex mess like VxRail, especially if what you have already meets your existing needs (other than aging out of the hardware)?I really struggle to see where HCI fits into any shop that doesn't buy infrastructure by the rack or maybe even by the row."
70.json,t3_8uk2au,t1_e1gstoc,2018-06-29 06:30:59,"This is very good feedback, thank you.Our use case is general I’d say so the usual file and print servers, RDS, SQL, Exchange (Hybrid), our ERP (not ms), Cognos TM1 and BI etc.I posted this thread as i had serious reservations as I’ve heard horror stories, not specifically vxrail but vsan related and I was worried about maintenance especially with only 3 nodes.Our current setup has seriously been rock solid other than a few spinning disk failures in our now five year old SAN but all software upgrades etc have been trouble free.I’m not convinced on HCI, I was just pitched and encouraged on the option by Dell and I was considering based on:Im committed to VMware based on existing investment in licensing as well as spend on Veeam and Zerto for DR so we are not considering Nutanix. Additionally Dell Pro Support has been fantastic on a hardware level.From the general consensus in this thread my concern was well placed so I appreciate all the feedback and welcome any further discussion."
70.json,t3_8uk2au,t1_e1gsxqw,2018-06-29 06:32:52,This was my primary concern with 3 nodes and adding a 4th is out of the question for financial reasons.Typically Dell keep insisting that 3 will be fine and is extremely common but I just don’t share their level of comfort.
70.json,t3_8uk2au,t1_e1gszvy,2018-06-29 06:33:50,Surprisingly given end of financial year specials the vxrail option worked out cheaper than ready nodes by about 7k over my total solution.
70.json,t3_8uk2au,t1_e1hfl8i,2018-06-29 13:39:42,"That’s not quite accurate, you lose one and now you lose resiliency to “another” host failure. This is a concern if you have a host in maintenance and hit another issue, but in steady state operations you of course have HA and would mitigate reduced resilience of a 3 node config by being more cautious in when I would perform maintenance operations ( which is actually why I’d want 4 nodes)."
70.json,t3_8uk2au,t1_e1hkt12,2018-06-29 16:14:51,"So we're a predominantly HP shop and we 'roll our own' clusters and have done for.. well.. a long time.A colleague elected for us to procure a basic Dell vx:rail for a project and despite our concerns, we went ahead with it.I'm sure our (Dell) vx:rail deployment was drawn out as much down to our tedious procurement processes and the fact that it was very much on/off in terms of whether it was going to go ahead... but the experience of actually getting setup was a bit of a nightmare.Even the technical presales was a complete cliche ridden load of fluff. They just don't send people who know the products intimately.. just top level fluff designed to sell to suits. If you bring technical people with you to ask questions, they didn't have any answers about how things worked under the hood.I think the worst of it all is that it's sold as this self contained solution and you only have one point of contact to fix everything.. the reality is that everyone you deal with, acts like it's the first vx:rail they've ever seen and even simple questions seem absolutely baffling to them.. and things always need to get escalated. The lower level engineers who are your first point of contact are basically.. useless. But that is a theme that has carried on right through the process.. the engineer who set it up, literally know how to run through the install wizard. As soon as they encountered something that went off-script, it was hands up in the air and they had no idea how to troubleshoot or do anything.I personally wouldn't want to touch another one.. at least not from Dell.If you're a greenfields site and you want to run stuff on premise then.. yeah, maybe consider it. However I fail to see what the value add is if you already have on-site expertise for the various layers of the infrastructure.. having a vx:rail just complicates it as you have a non-standard deployment and maintenance model. Involving Dell's support for the product is just AWFUL."
70.json,t3_8uk2au,t1_e1i46ha,2018-06-29 23:38:31,"My take as someone who knows someone who works for a reseller is that with the Dell/EMC merger there are a lot of Dell shops now trying to push EMC stuff and don't have much experience with it.  But the sales people must have incentives for it that rival a religious conversion because it's all they talk about.I also question how big and established a product it was considering how relatively new vSAN is -- maybe something existing EMC customers would have considered pre-merger, although I never quite understood the logic of a company whose principal products were storage devices selling against their historical product line.  It seemed like a me-too product.I'm always deeply suspicious of the all-in-one composite product with the secret sauce wizard-driven setup.  What are they hiding?  Why is the wizard functionality hidden/not documented and why can't I know how to do it manually?  And what is the long-term agenda here, why do I feel like you just want to lock me into some kind of long-term hardware-as-a-service?"
71.json,t3_a1gtwg,vSAN Performance - wrong Physical Adapter shown?,2018-11-29 20:15:15,"Just looking around a new 14G VxRail cluster we have been running for a few months now.  I noticed that under Monitor - Performance - vSAN Physical Adpaters there is oscillating % Packet Loss - sometimes as high as 200+%.  What I did notice though is that this page in vCenter (6.5u2 so still on flash client) is showing vmnic0 for vSAN.  The vSAN vmk port is configured to uplink2 on the dvSwitch - which is vmnic1. This has now got me confused - either we have a misconfigured vSAN (unlikely despite VxRail) or we have a bug in the flash client showing the wrong nic.  Either way I do not think i should be seeing packet loss but dont know what to trust. Of note all our other vSAN clusters (13G) show the same - vmnic0 under the monitor tab.  Anyone else able to confirm? EDIT: Hmmmm, we have one non-stretched cluster and that is showing both network cards. UPDATE: So actually think we have got to the bottom of it.  VMware have confirmed this looks to be a bug in the reporting.  All interfaces were showing clean for drops and errors.  Turns out the packets are being combined as they traverse the dvswitch and these are being reported as dropped rather than filtered.  Fixed in 6.7 I believe and a KB is being generated.  Once I get it I will update this post in case anyone else stumbles on this.  Don't think this covers the incorrect interface being shown but  at least the packet loss is not actually packet loss"
71.json,t3_a1gtwg,t1_eapq25u,2018-11-29 21:34:32,You are not alone. I have teh same thing. I have two vnmics in use for vsan (active/passive) and that same screen shows the passive one. So its useless. Non stretched clusterNot a big deal as i can just look at the vmkernl adapter section.I see this on 2 of my 3 VSAN clusters. They have slightly different counts of NICs (one additional) in the one that works.Not running vxrail.
71.json,t3_a1gtwg,t1_eapsdq0,2018-11-29 22:10:49,thanks for sharing.  glad i am not the only one.  Massively frustrating.
71.json,t3_a1gtwg,t1_eaqftty,2018-11-30 02:57:48,"If you are using the Intel X710 NIC as in our VxRail S570 (14G) I have to tell you, that there are many problems with the Intel driver i40en in version 1.4.2 or 1.5.8, which is included in the VxRails software bundles (MDD events, NIC flapping, etc.). A few weeks ago we had discussed several cases with Dell EMC and their VxRail Engineering Team and Intel because of similar issues. Intel as well as Dell EMC recommended to install the i40en driver in version 1.7.11. Originally it was only released for ESXi 6.7, but VMware has now backported it for 6.0 and 6.5.See: https://www.vmware.com/resources/compatibility/detail.php%3FdeviceCategory%3Dio%26productid%3D37996I would therefore recommend to upgrade the VxRail software to the latest version, so that the firmware is up to date and install the i40en driver in version 1.7.11 from VMware: https://my.vmware.com/web/vmware/details?downloadGroup=DT-ESXI65-INTEL-I40EN-1711&productId=614If you would like to discuss this with Dell EMC in advance, you can refer to our SR: #1138976767"
71.json,t3_a1gtwg,t1_eau1o17,2018-12-01 12:09:12,The X710 was an odd duck and yes that family of driver has (for quite a while) been required.
72.json,t3_bubo1n,EMC DES-6332 Certification Exam Study Guide Materials - Crack4Sure,2019-05-29 15:30:19,"Worlds leading internet networking company  EMC DES-6332 exam deals with the topics related to IP routing, bridging, non-IP desktop protocols, some equipment commands and switch-related technologies. This is basically a closed book exam. If you are already an IT professional then this certifications will not only enhance you current career but it will also offer much better opportunities in the market. Due to high demanding certification EMC DES-6332 guide and other helping materials for exams are easily available in the market. Why  There are many sites which provide information on EMC DES-6332 exams and provide you study material like EMC DES-6332 dumps and others. To make a good preparation for this highly professional exam you must have a complete knowledge and for that you must use an authentic source. Crack4sure is the best source to prepare for your  EMC DES-6332 Exam Questions with Experts Verified Answers The DES-6332 answers in the Crack4sure books are written in detail to explain each and every point and completely answers DES-6332 questions that can come in your final exams. Moreover Crack4sure provides you every thing online and you can download anything anytime you want. Books are available in EMC DES-6332 pdf format so they can be downloaded and used easily  100% Passing Ratio with Crack4sure Value Pack No one else except Crack4sure assures you 100 percent ratio with its value pack. This value pack offers complete DES-6332 training to get top grades. This value pack is specially designed and includes things like DES-6332 real exam questions as well as DES-6332 notes to clear certain points that are complicated in the syllabus. Another key feature that makes Crack4sure’s value pack important is that is has all DES-6332 simulation in it that are very important. These important features in the Crack4sure value pack has increased its importance for passing EMC DES-6332 test with top ranks. The DES-6332 Real Questions guarantees with Crack4sure value pack is the hottest issue among information technology professionals and it’s been on the top list of DES-6332 forum for discussing network related issues. This is the only easiest way to get excellent results in your highly professional and demanding certification exam."
73.json,t3_7h6p4k,Whose using Starwind Hyperconverged appliance? Are you happy with it?,2017-12-03 09:11:01,"After several weeks of research and getting quotes from Nutanix, vxrail, nimble, and simplivity, all in the neighborhood of $225k. I also received a quote from Starwind for an all flash 7.2TB (6Gbps SATA) 3 node hyper converged (Hyper-V) appliance for $85k which includes windows and SQL licenses.  I feel this is more than enough to run 5 virtual machines for 25 users todday and maybe 50 users later for a warehouse management system and file sharing. What I would like to know is who else uses this product and what their experience with it has been.   I do have a technical demo scheduled with Starwind, but it would be great to have some insight prior to that.  Their website does have info but nothing I find helpful."
73.json,t3_7h6p4k,t1_dqolhkn,2017-12-03 10:00:11,I had a demo with them that wasnt very great. The product feels cheap and I would never want to use it in production for mission critical environments.
73.json,t3_7h6p4k,t1_dqolw83,2017-12-03 10:08:44,[deleted]
73.json,t3_7h6p4k,t1_dqomalo,2017-12-03 10:17:00,We used Starwind in our Datacenter to cluster two of our vhosts for about a year before tearing it out. When it works it's beautiful but it caused too many late night phone calls and disk latency issues (even with high performance SSDs) that it wasn't worth the headache. There was a 50-75% degradation of performance on our hosts that we weren't expecting. Not only that but the connection between the two servers would just drop randomly. Sometimes it was at 2 am. Other times it was at 11 am with two downed hosts.We ended up switching over to just replicating our VMs (Veeam Replication) across the hosts and to a dedicated backup server. It works beautifully with no issues since implementation.
73.json,t3_7h6p4k,t1_dqomlvw,2017-12-03 10:23:06,"The appliances are much newer than the VSAN only product.I've never tested this theory, but I'm pretty sure you could use a non-appliance with their software with one of their appliances should the appliance fail and a replacement have an issue getting to you.  I'm just guessing, but you could ask them."
73.json,t3_7h6p4k,t1_dqon5st,2017-12-03 10:33:44,"StarWind has a great team and technology behind their product.  If you are looking for a number of customer ""reviews"", hit up the SpiceWorks community.  Starwind has loads of users active there so you can get a lot of feedback from real world users.  Might be worth popping in as you'll get a range of long term and new users.  As for the appliance, I'm not sure I've met anyone yet who is using one as they really are new, so that you might be a vanguard.  But the VSAN product and their support, you'll get loads of feedback."
73.json,t3_7h6p4k,t1_dqopobr,2017-12-03 11:24:44,ummm.... 225K for 3 nodes of vmware and nimble is about 3x or more what you should pay.
73.json,t3_7h6p4k,t1_dqox4js,2017-12-03 14:06:28,"Ran it for a few years.  It was great when it worked, but when it didn't it was a real disaster getting back in order.   We've switched to S2D, and I'm pretty happy with it so far."
73.json,t3_7h6p4k,t1_dqp1nqe,2017-12-03 16:49:58,"we kind of switched from starwinds -> s2d after ws2016 ga ,but after a couple of catastrophic s2d/refs failures msft could do nothing to troubleshoot we replaced s2d with starwinds .."
73.json,t3_7h6p4k,t1_dqp1qua,2017-12-03 16:53:40,proactive support rules ! we only get reports after everything is fixed :disk was offline or connection dropped or node didn’t came back after windows os update ..
73.json,t3_7h6p4k,t1_dqp2gye,2017-12-03 17:26:48,"I often read threads such as this and think that the whole idea of virtualizing is that it lets you consolidate which lets you save money.Not knowing the license costs $85k sounds quite a lot for ""just"" 5 virtual machines.Do you need the availability a HCI cluster gives you?  Asking as a single box will eat your workload so it may be that there's a much lower cost option if you don't need the availability and RTO a HCI cluster can give."
73.json,t3_7h6p4k,t1_dqp4pq9,2017-12-03 19:18:02,"Every time I read such thread on Spiceworks I feel like it's mostly astroturfing, almost bot like replies about how cheap it is but never about support and reliability. Harder questions get completely ignored.Would not recommend."
73.json,t3_7h6p4k,t1_dqp56kj,2017-12-03 19:41:05,"Can you describe the quote from Nimble? The AFA shouldn't be more than 60k for 11tb with 4h support. 3x Dell R640 or HP equivalent are maybe 10k per node. It shouldn't be much over 120k even if you include switches.For 50 users and 5 VMs, are you sure you aren't overspeccing this cluster by a lot? It sounds like it should run fine on two R740s sharing a simple disk shelve for $50k total. We run our infra on a site on the following for €35k total:If I would be buying now, I would look at something similar, 2 hosts and some disks from Nimble / Dell in between. Nimble hybrids are fine for most users and databases, and AFA can handle everything as long as there's enough capacity.I've also looked at Hyperconverged but the seem way overpriced. Good if you need to drop in more nodes to automatically grow the cluster constantly, but useless for SMB where the load is static and 2x R740 are so overpowered that you'll never run out of cores and RAM."
73.json,t3_7h6p4k,t1_dqp8t2h,2017-12-03 22:04:36,"We bought a small one for a turn key client project. Haven’t installed yet, due to receive hardware soon. You are paying for a couple Dell servers, their vSAN software, their support, their pre-config of the hardware/software. It’s a pre-built hyper-v cluster. They would help design and implement upgrades + managed scaling (add nodes). We will help the client do the initial implementation but turn over the keys at end of project and he will interface with their support team longterm.We could spec and set everything up manually but the budget allowed for us to buy through them and will give StarWind more sense of responsibility longterm since they built the rig. I felt comfortable with the purchase based on industry reviews, their support, and knowing there is nothing proprietary (other than their proven vSAN software)."
73.json,t3_7h6p4k,t1_dqpaaae,2017-12-03 22:48:01,"Thanks for responding and for sharing your setup.  It probably is overkill. I gave the sales people my requirements for the app and they kept giving me what I felt was something overpowered, but it seemed to be the same thing regardless of the vendor so I accepted it.   I would really like to get the cost down but it does have to perform quickly so I am trying to stick with all flash, but the flash requirement is actually only 2TB for Oracle, the rest can be spindles, totaling 6TB for all.  Below is the pricing I have which is about $185k ($40k is for Windows licensing).  It's a really small business and currently will support 25 users.   ThanksHPE Nimble Storage AF3000 All Flash Dual Controller 10GBASE-T 2-port Base Array     Hewlett Packard Enterprise - Part#: Q8B41A 1 $30,251.07 $30,251.07 2 HPE Nimble Storage AF1000/3000/5000 All Flash Array R2 11.5TB Flash Bundle     Hewlett Packard Enterprise - Part#: Q8B72A 1 $33,132.12 $33,132.12 3 HPE Nimble Storage 4x10GbE 2-port Adapter Kit     Hewlett Packard Enterprise - Part#: Q8B88A 1 $3,121.14 $3,121.14 4 HPE Nimble Storage C13/C14 PDU Base Array Power Cord     Hewlett Packard Enterprise - Part#: Q8F97A 2 $0.01 $0.02 5 HPE Nimble Storage NOS Default Software     Hewlett Packard Enterprise - Part#: Q8G27A 1 $0.01 $0.01 6 HPE Nimble Storage HPE Foundation Care Exchange Service - extended service agreement - 3 years     Hewlett Packard Enterprise - Part#: HT6Z0A3 1 $0.00 $0.00 7 HPE Nimble Storage All Flash 11.52TB Flash Support     Hewlett Packard Enterprise - Part#: HT6Z0A3#X45 1 $5,186.55 $5,186.55 8 HPE Nimble Storage Foundation Care Exchange Service - Extended service agreement - replacement (for 2x10GbE 2 ports adapter) - 3 years - shipment - 24x7 - response time: 4 h     Hewlett Packard Enterprise - Part#: HT6Z0A3#W6S 1 $995.12 $995.12 9 HPE Nimble Storage AF3000 Base Array Support     Hewlett Packard Enterprise - Part#: HT6Z0A3#X4Y 1 $7,812.43 $7,812.43 10 HP Installation and Startup Service     Hewlett Packard Enterprise - Part#: HA114A1 1 $0.01 $0.01 11 HPE Nimble Storage Array Startup Service     Hewlett Packard Enterprise - Part#: HA114A1#5MR 1 $1,886.41 $1,886.41 12 HEWLETT PACKARD ENTERPRISE : HPE DL360 Gen10 5118 1P 32G 8SFF Svr/SB     Hewlett Packard Enterprise - Part#: 874461-S01 4 $3,658.98 $14,635.92 13 HEWLETT PACKARD ENTERPRISE : HPE DL360 Gen10 Xeon-G 5118 Kit     Hewlett Packard Enterprise - Part#: 860663-B21 4 $1,398.88 $5,595.52 14 HEWLETT PACKARD ENTERPRISE : HPE 16GB 2Rx8 PC4-2666V-R Smart Kit     Hewlett Packard Enterprise - Part#: 835955-B21 18 $301.27 $5,422.86 15 HEWLETT PACKARD ENTERPRISE : HP Dual 8GB microSD EM USB Kit (comes with two 8GB MicroSD cards configured for redundancy)     Hewlett Packard Enterprise - Part#: 741279-B21 3 $110.26 $330.78 16 HEWLETT PACKARD ENTERPRISE : HPE 5Y PC 24x7 DL360 Gen10 SVC     Hewlett Packard Enterprise - Part#: H8QM1E 4 $3,596.74 $14,386.96 17 HPE Enterprise - Hard drive - 1.2 TB - hot-swap - 2.5"" SFF - SAS 12Gb/s - 10000 rpm - with HPE SmartDrive carrier     Hewlett Packard Enterprise - Part#: 872479-B21 5 $556.09 $2,780.45 18 HPE FlexFabric 5700-40XG-2QSFP+ - Switch - L3 - managed - 40 x 1 Gigabit / 10 Gigabit SFP+ + 2 x 40 Gigabit QSFP+ (uplink) - rack-mountable     Hewlett Packard Enterprise - Part#: JG896A 2 $3,928.66 $7,857.32 19 HPE Foundation Care 24x7 Service - Extended service agreement - parts and labor - 3 years - on-site - 24x7 - response time: 4 h - for FlexFabric 5700-32XGT-8XG- 2QSFP+, 5700-40XG-2QSFP+, 5700-48G-4XG-2QSFP+     Hewlett Packard Enterprise - Part#: U4VF3E 2 $716.00 $1,432.00 20 HPE 58x0AF Frt(prt) Bck(pwr) Fan Tray     Hewlett Packard Enterprise - Part#: JC683A 4 $69.28 $277.12 21 HPE A58x0AF 300W AC Power Supply     Hewlett Packard Enterprise - Part#: JG900A 4 $287.97 $1,151.88 22 HPE X130 10G SFP+ LC SR DC Transceiver     Hewlett Packard Enterprise - Part#: JL437A 4 $597.46 $2,389.84 23 HPE X240 10G SFP+ SFP+ 0.65m DAC Cable     Hewlett Packard Enterprise - Part#: JD095C 2 $78.33 $156.66 24 HPE 562SFP+ - Network adapter - PCIe 3.0 x8 - 10 Gigabit SFP+ x 2 - for ProLiant DL360 Gen10, DL380 Gen10, DL560 Gen10, XL230k Gen10; StoreEasy 3850     Hewlett Packard Enterprise - Part#: 727055-B21 3 $370.41 $1,111.23 25 Microsoft Windows Server 2016 Datacenter - License - 16 cores - Microsoft Qualified - Open License - Single Language     Microsoft - Part#: 9EA-00122 6 $5,264.00 $31,584.00 26 Microsoft SQL Server Standard Edition - License & software assurance - 1 server - Open License - Win - Single Language     Microsoft - Part#: 228-04628 2 $1,151.00 $2,302.00 Subtotal $173,799.42 Shipping $228.31 *Tax $11,964.41 Total $185,992.14"
73.json,t3_7h6p4k,t1_dqpadn4,2017-12-03 22:50:50,I estimated Windows licensing at about 25K including SQL.  This is for 3 nodes based on an estimate from their 2 node coming in at $36k.  thanks!
73.json,t3_7h6p4k,t1_dqpagre,2017-12-03 22:53:10,Can you tell me more about S2D?Do you call MS for support when things don't work? Have you done any tests such as unplugging a node or have you had a chance to see how it works when something fails? Did you need switches?  or is it like Starwind where nodes connect to eachother? What did it cost you? thanks
73.json,t3_7h6p4k,t1_dqpagvd,2017-12-03 22:53:14,"You have hit most of the major vendors but you see to have left out Scale Computing who would be a natural fit for the size and configuration that you are mentioning.    Typically for running 5 VMs and 25-50 users, you'd only be looking at a $30K - $50K price range for the appliances.  I don't know your CPU and RAM needs, but $225K seems orders of magnitude almost out of scope with what appears to be what you need."
73.json,t3_7h6p4k,t1_dqpajol,2017-12-03 22:55:12,"I like your solution with Veeam which is what my DR plan is based on, replicating over gigabit to another building.  For my requirement the owners of the business want no data loss as much as possible.  Even a minute is too much.  So I need some sort of cluster."
73.json,t3_7h6p4k,t1_dqqdvt0,2017-12-04 11:15:32,"You can drop Hyper-V based Nutanix from your list immediately, it just doesn't work well. Everything else listed are good options indeed. I'd start with Nimble however.We run these guys in production for 2,000+ customers. Good news they deliver. Bad news you really have to know what you're doing if you run them in self-supported mode. Another caveat is make sure they quote you new rather than refurbished Dell equipment, so you won't compare apples to oranges."
73.json,t3_7h6p4k,t1_dqqk9ks,2017-12-04 13:30:41,"We are in the process of implementing a 5 node compute and 2 node storage. Not exactly hyperconverged but we are using Starwinds VSAN.We have a lot of high compute/memory apps, and buying 5 R730d's were too costly and would over-replicate the storage. So we went the other route and saved a bit of money.Plus if we ever found Starwinds to not work out, we can simply use S2D on the 2 node R730d's and the R730's compute just need to point to a new storage source."
73.json,t3_7h6p4k,t1_dqqt1yx,2017-12-04 19:08:54,"Have you considered options that don't involve shared storage?For example using native SQL HA via Always on Availability clusters (no shared storage needed) - this might need SQL Enterprise but with Software Assurance you don't need to license the passive SQL instance.DFS or HyperV Replica should be able to offer near-time replication for file servers which is typically ""good enough"" for most setups.It might be worth running the numbers just to see what they look like as it may work out cheaper and/or slightly less complex setup."
73.json,t3_7h6p4k,t1_dqqwfv9,2017-12-04 21:11:54,"So the other route meaning VSAN? So you saved on Storage? As for using the servers for something else, that was one of my thoughts too.  Dell R630's. Starwind quotes SATA 6Gbps drives, I know dell has 12Gbs SAS drives too.  Just not sure how big a difference the performance would be."
73.json,t3_7h6p4k,t1_dqqzw94,2017-12-04 22:37:11,"Really? Nimble reams me for SSD builds on their storage stack alone (so a shared shelf), let alone a hyperconvereged setup."
73.json,t3_7h6p4k,t1_dqr1kus,2017-12-04 23:11:31,"dfs-r has no write locks making it barely usable for any write-intensive setups ,hyper-v replica is dr by design - data loss and downtime on failover isn’t what your ha file server typically does .."
73.json,t3_7h6p4k,t1_dqr1nv4,2017-12-04 23:13:11,what did you buy instead ?
73.json,t3_7h6p4k,t1_dqr9njh,2017-12-05 01:29:53,"Did you get major dc-to-dc connectivity issues, or how did you manage having all hosts going down at the same time?Veeam saved our bacon numerous times. Great solution!"
73.json,t3_7h6p4k,t1_dqrey4j,2017-12-05 02:55:47,"We have already installed a bunch of their apliances for our customers. What could I say? It is a great experience actually to work with. Of course, we had some minor issues. However, skilled guys at Starwinds support know their job. So far everyting works great. I think you made a good choice."
73.json,t3_7h6p4k,t1_dqw5yg5,2017-12-07 16:30:37,"Weird situation, we have more than a few customers with Starwind and never faced such critical situation to use DR site as the main production site. There were a few issues and what can I tell, their support was at the highest level and solved all the issues right on time. I've never tried their appliances, so I've got nothing to say about it, but their support works much better than a lot of enterprise companies."
73.json,t3_7h6p4k,t1_dqxh7iw,2017-12-08 09:21:09,Fully agree. 5 VMs and 25-50 users can very likely run on a single host. You can cram a lot in a host these days. Then if HA and/or failover is a requirement you can add a second host and StarWind or replicate frequently with Veeam.
74.json,t3_8gm3hx,VXrail SD card formatted,2018-05-03 07:59:29,"Stupid me, I added and attached a USB device on a VM, formatted it and overwritten it. That “USB” device ended up being the internal Dell SD card on the ESXi host. That’s where ESXi is installed on...how the hell do I correct this on a VXrail system?"
74.json,t3_8gm3hx,t1_dyd6hlo,2018-05-03 12:25:33,"Easy: by not doing that again!  :POn a more serious note: not much.  You need to know what device is what before doing any sort of passthrough like that, then do some checking on the guest OS to make sure you are using the right device.  Unfortunately, there is no simpler answer than this that I know of."
74.json,t3_8gm3hx,t1_dydco8m,2018-05-03 15:12:18,If it's VxRail I would say open a chat with support. They can probably help you out one way or another. And knowing EMC support (all to well) it should get fixed.
74.json,t3_8gm3hx,t1_dydkulk,2018-05-03 19:58:43,"It depends if it's a single sd card or a dual sd card in raid1 (dell rack servers do this).Dell support will be able to guide you, but the esxi host should keep running perfectly fine as long as you do not reboot.From an esxi point of view you could take a config back up so you can restore it fresh etc, but with VXrail there might be a simpler way"
74.json,t3_8gm3hx,t1_dyeawya,2018-05-04 02:57:40,"I guess you just will have to reinstall the ESX, as you won't lose any data of your VSAN."
74.json,t3_8gm3hx,t1_dyeb27o,2018-05-04 02:59:49,What would happen if I reboot a host where I run a rm ¨* from the ssh console?Does that delete the files on the sd card or just the in memory ESXi?
74.json,t3_8gm3hx,t1_dyejloi,2018-05-04 05:05:03,Who knew it would be wise to backup those SD cards?
75.json,t3_61535y,What has your experience been with hyper-converged systems such as EMC VxRail?,2017-03-24 06:17:23,I'm working on a plan to replace our current vmware system as it's over 6 years old now.  We are a Dell shop and Dell is pushing this EMC VxRail very hard.  I'm wondering what other people's experiences have been with this product in comparison with a vsphere system with all separate components?  What do you see as the benefits vs pitfalls?
75.json,t3_61535y,t1_dfbtqtc,2017-03-24 07:10:18,"The benefit of individual servers with separate licensing is that it's easy to upgrade nodes in the future. I've had issues with the price of vxrail and nutanix, but I also needed things like grid gpu, so they offered an insanely overpriced 2u server as an add-on. Get quotes and see which looks better, but also consider how node upgrades will occur."
75.json,t3_61535y,t1_dfbua2o,2017-03-24 07:21:36,We use Pivot 3 and it's been great.
75.json,t3_61535y,t1_dfbvhrd,2017-03-24 07:47:25,"YMMV, but my experience with 4 VXRail appliances has shaped my opinion.  VXRail is a no go.  We have one now and I can't wait to replace it.  I've had very good luck with Nutanix however.  Of course that is running AHV, if you want to stay with VMWare, I would suggest doing  UCS personally.  Just don't do VXRail, you will wish you hadn't."
75.json,t3_61535y,t1_dfbx5xa,2017-03-24 08:22:18,[deleted]
75.json,t3_61535y,t1_dfbxm0b,2017-03-24 08:31:46,"Have a 4 node Scale IO cluster. It's been a nightmare, and I wouldn't touch it again if you paid me. At this point, were moving to AWS 1 year ahead of planned life cycle dye solely to how bad ScaleIO has been."
75.json,t3_61535y,t1_dfc0j75,2017-03-24 09:33:13,"This has been our experience as well. Nutanix has been a performance and reliability downgrade for SQL Server clusters. You'd have to go with all-flash for it to work but we need hundreds of TBs.Our 4 year old Cisco UCS with a mix of different SANs, including PureStorage, has been great for us even today.We're going to move everything into the cloud soon anyway."
75.json,t3_61535y,t1_dfc0mpo,2017-03-24 09:35:19,Do yourself a favor. Don't fall for all the Dell sales BS.If you go hyper-converged because you arent ready for the cloud yet then go with Nutanix and DO NOT use Dell hardware. Buy straight from Nutanix or splurge and put it on Cisco UCS hardware.Dell is the Walmart of IT and its great if you're google or AWS and have built your applications so that they can handle hardware failures and still not miss a beat. Not so great if you're still depending on DR at the infrastructure level. We're not going to spend any more on sever hardware. Why do it when you use that money to modernize your applications so you can put it all in google or amazon's severs? You don't need a hypervisor anymore and I assure you that Dell + VMware can NOT get you per VDI prices that are anywhere near as cheap as AWS Workspaces. Go serverless if possible for new applications. Utilize cold storage when possible.
75.json,t3_61535y,t1_dfc0rxh,2017-03-24 09:38:21,"So not completely pertinent to your question but have experience with emc converged and hyper converged systems. We use a 4 node vxrail system for one of our regional offices. So far it has worked well and been fairly easy to manage. We have our first major upgrade scheduled for this weekend so my feelings may change.We also have several vce vblocks that we run the majority of our systems on. In my experience they are not worth the overhead. You are really locked in with their rcm baselines and upgrade schedules and processes.If I had my pick for an on premise solution I would pick rack or blade servers (whoever your preferred vendor is) and a quality all flash array (pure, nimble, solid fire, etc). You have much more freedom and aren't locked into converged/hyperconverged cookbooks to be in a supported cookbook."
75.json,t3_61535y,t1_dfc1y6n,2017-03-24 10:03:04,"Not really possible for me... I work in aerospace, military.  Everything has to be local or on our WAN."
75.json,t3_61535y,t1_dfc22ha,2017-03-24 10:05:38,"Spent a while evaluating nutanix, simplivity, vxrail, hyperflex, and HP's hyper converged solution that I can't even remember the name of. We went with AF vsan on new c240s in our existing UCS environment. We've just gone production on it for VDI and so far so good, but I think what we learned is that it takes a lot of hardware in complicated configurations to make it work well. I love UCS and am glad we stuck with for it for this instead of adding a new hardware vendor to our datacenter, but I think if you can afford the amount of compute needed to make hyperconverged worth it, then you can probably just buy a beefy FC flash array too; and that's just simpler IMO."
75.json,t3_61535y,t1_dfc3oz9,2017-03-24 10:40:57,[deleted]
75.json,t3_61535y,t1_dfc4kkp,2017-03-24 11:01:25,you work for vmware?
75.json,t3_61535y,t1_dfc56cu,2017-03-24 11:15:41,"I'm in the middle of some things tonight, but I will come back in the morning and share my thoughts further."
75.json,t3_61535y,t1_dfc65iu,2017-03-24 11:39:20,"I'm running Nutanix on their absolute garbage supermicro and it's been awful. 10 memory replacements in 15 months, 1 complete node failure, and and IPMI interface that literally makes me mad just thinking about it.Is it that much worse on Dell?"
75.json,t3_61535y,t1_dfc88f9,2017-03-24 12:37:13,Can you elaborate? This sounds important.
75.json,t3_61535y,t1_dfcabr5,2017-03-24 14:12:29,No problems with Dell here. Nearing 8000 VMs globally and pro support is great. Looked at UCS during tender and found the price point to be too high in comparison with other players.
75.json,t3_61535y,t1_dfcfxt4,2017-03-24 18:52:58,[deleted]
75.json,t3_61535y,t1_dfci78j,2017-03-24 20:19:37,"Our environment doesn't scale linearly.  One quarter we may be heavy on CPU, the next we may be heavy on storage.  Hyper-converged as an architecture just doesn't work efficiently for us.Specifically in regards to EMC, our experience has been on a steady decline for years in regards to our sales and support experience.    We are divesting from their solutions as much as possible.We have been extremely happy with our UCS + Infinidat solution."
75.json,t3_61535y,t1_dfcqhgh,2017-03-24 23:25:59,agreed vxrail is trash
75.json,t3_61535y,t1_dfcqk5o,2017-03-24 23:27:24,We are a ucs/pure shop and I evaluated all flash vxrail and nutanix. They both offer great products but really pigeon hole you for very little to no benefits.Right now I dont think its really worth the trouble for a competent admin that knows how to administer UCS and a flash array.
75.json,t3_61535y,t1_dfcredb,2017-03-24 23:43:00,Why is everyone downvoting...
75.json,t3_61535y,t1_dfcs3y7,2017-03-24 23:55:59,"One thing to note if you need ""compute light, storage dense"" you can go single socket to cut your licensing costs in half (with VDI, licensing is per user so it doesn't matter).FC, FCoE, and FI's are not ""simpler"" than Ethernet and a VMkernel Port in my book, but if your a traditional FC storage guy I can see how ethernet might seem more complicated.  In my experience UCS adds a LOT of complexity which is great for automation at 100+ hosts, but at small scale isn't worth it.Now that Redfish is becoming the new standard API for underlay management Cisco is kinda the odd duck out and we'll see if shops that went to UCS for automation will stick around when they can get gear for 1/2 the cost that can do the same thing."
75.json,t3_61535y,t1_dfcsesl,2017-03-25 00:01:29,"Wow...Thanks for all the comments... I have to say I was very hesitant when the guy was pushing it and man was he pushy.  He even said some BS like ""this is the way the market is going"", what I learned was that it's the way that hardware manufacturers like Dell want it to go, not necessarily what customers want.  I was already thinking about failures and the complexity of a proprietary system like this without the options of upgrades and changes later."
75.json,t3_61535y,t1_dfczqtg,2017-03-25 02:15:04,"I suggest to the customers of mine consider EMC, Nutanix, HPE, StarWind, and Cisco when it comes to entirely new production. That said, it completely depends on project requirements (IOPS, capacity, estimated grows, overall resilience) for the available budget.Since we have no information about your project, I would suggest you quote HPE Storage and StarWind HyperConvergerd Appliances first. I'd requested quotes for SMB\Low-Enterprize customers some time ago and can recommend these vendors to start with due to affordable offers. https://www.hpe.com/us/en/integrated-systems/hyper-converged.html https://www.starwindsoftware.com/starwind-hyperconverged-appliance"
75.json,t3_61535y,t1_dfdz8sd,2017-03-25 18:31:05,Can you share anything about them with us please?
75.json,t3_61535y,t1_dfe8bq1,2017-03-26 00:04:56,VxRail supports a real active/active stretched cluster.
75.json,t3_61535y,t1_dff5i86,2017-03-26 14:12:52,"If you are short on capacity, Nutanix has storage only nodes. They run AHV, so don't require any vSphere licensing and can be mixed into any cluster with different node sizes."
75.json,t3_61535y,t1_dffjy5a,2017-03-27 00:13:06,That's the key right there.
75.json,t3_61535y,t1_dfrkj4w,2017-04-03 18:58:05,How did your upgrade go? Smoothe as butter or problems?
76.json,t3_8lum74,VXRail Monitoring in VROPs beta coming in June,2018-05-25 01:46:58,"Blue Medora (who I'm with) is developing a monitoring extension for VXRail in VROPS. This will supplement the native VMware experience with hardware status alerting, power consumption details, inventory reporting, direct performance-to-hardware correlation and more. If you are interested in taking a look or helping improve the solution, email "
76.json,t3_8lum74,t1_dzj5o5l,2018-05-25 08:06:52,Me Please
77.json,t3_b3vsa0,vSAN write latency Troubleshooting,2019-03-22 04:46:47,"I have two vxrail clusters with the same configuration, 8 hosts per cluster each with 3 disk groups all flash. 3 capacity disks in two disk groups and 2 capacity disks in the last disk group. One cluster for production and the other for DR of the the same workload(splunk). We noticed the production VXrail is running at about 11ms write latency with ~500iops and 15MB/s throughput. That seems quite high for such a light workload so as a sanity check I did my best to replicate splunk workload with HCIbench to run on the DR location cluster. VDbench was configured to use 64K IO as I believe that is what splunk does. The benchmark landed me at about .7ms write latency on the DR cluster. One last thing of note is the prod cluster has significant outstanding IO (roughly 1000) whereas I never see much on the DR cluster even during benchmark."
77.json,t3_b3vsa0,t1_ej2s0k0,2019-03-22 06:45:10,What release of VxRail are you running?
77.json,t3_b3vsa0,t1_ej341l5,2019-03-22 09:22:01,"what data protection setting are you using, isnt that typical of HCI because it has to replicate the writes to another host before it can ack back to the application?"
77.json,t3_b3vsa0,t1_ej3588d,2019-03-22 09:37:38,What Model nodes and software version?  Do you have a SR open?  I work for Dell EMC as a presales engineer.
77.json,t3_b3vsa0,t1_ej434km,2019-03-22 20:52:12,4.7.001-11073037
77.json,t3_b3vsa0,t1_ej437oh,2019-03-22 20:53:31,P470F at version 4.7.001-11073037. Yes there was an open SR. It was closed suggesting the performance is normal. Which I question as vSAN call's out the outstanding IO as being abnormal.
77.json,t3_b3vsa0,t1_ej46or9,2019-03-22 21:42:25,"I never had latency issues but it's always interesting to run proactive tests.https://kb.vmware.com/s/article/2147074And check the network just because:https://livevirtually.net/2018/07/16/troubleshooting-vsan-networking-issues-with-health-checks-vsan-health-check-and-vsphere-distributed-switch-vds-health-check/On a VSAN Stretched cluster I have the Health Check for site latency, it stays below 0.5 ms on both sites and witness.https://kb.vmware.com/s/article/2146133"
77.json,t3_b3vsa0,t1_ejcaxol,2019-03-25 22:46:14,"5600, is 25MB per ASIC and thankfully not the garbage VoQ buffer system of the 5500. Not ultra deep but not the shallowest.Intel 5xx is a rather stable NIC just one of the ones with the fewest offloads."
78.json,t3_bh1px9,poor performance View 7.8 / vcenter 6.7,2019-04-25 07:43:50,"Our view connection server (7.5x) crashed last week.  I was already planning to upgrade, so we stood up 2 new view connection  servers (7.8) and attached to a newly deployed vcenter 6.7 integrated with vxrail.  While i was at it, updated view composer and app volumes. I installed Win 10 LTSC (2019) and office 2019, ran the optimizations as usual, latest updates, etc...  I assigned 4 cpu and 3.5g of ram (higher than our old settings)...  Everything seemed good in my first pool recompose, login times were ~25 seconds, so i recomposed the remaining pools and went to bed.  The following morning performance degraded as more people connected.  To avoid getting too wordy, in the end i reverted to win 10 LTSB (2016) and 2cpu, closer to original specs and the issue still continues.  Once ~60-70 users connect performance tanks and at 130 users its completely unusable Hardware has not changed except the hardware that vcenter lives on (vxrail).  The vms run on 9 hosts (various generation M series blades) in a dell m1000 chassis and hybrid equallogic arrays.  i'm completely out of ideas, is there a setting i missed on the hosts when moving them to the new vcenter server?  I forgot to set evc mode, but that did not have an affect i'm exhausted, i hope that made sense! Any help would be greatly appreciated"
78.json,t3_bh1px9,t1_elpaa4n,2019-04-25 07:58:53,"I'm a bot, bleep, bloop. Someone has linked to this thread from another place on reddit: If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads. (Info / ^Contact)"
78.json,t3_bh1px9,t1_elpt4p7,2019-04-25 11:34:16,PCoIP or Blast?
78.json,t3_bh1px9,t1_elptefz,2019-04-25 11:37:12,pcoip but the same performance when using the vm console.  makes me think something other than view.  I've also removed app volumes agent and updated to the latest vmtools
78.json,t3_bh1px9,t1_elpu8ee,2019-04-25 11:47:04,"I had to scrap my win 10 project until I got new servers with nvidia m10s. Windows 10 x64 is just too resource intensive. Also buying the vdi design guide book helped me plan out that upgrade.... I know that doesn’t help you now but look into graphics for your backend, I also don’t run optimization tool any more so I know exactly what’s been changed."
78.json,t3_bh1px9,t1_elpuqbt,2019-04-25 11:53:14,"yea, my first rollout of 10 on view was a failure.  I eventually overcame those issues and had been running LTSB in horizon for nearly a year until these upgrades"
78.json,t3_bh1px9,t1_elq2g2f,2019-04-25 13:48:07,Have you looked at esxtop?This sounds like a resource issue. I’d also make sure that all your firmware and drivers are current per the HCL.
78.json,t3_bh1px9,t1_elq3e0a,2019-04-25 14:03:53,"no i havent ran esxtop.  Updating the esx hosts to 6.7 was the last upgrade we did.  Performance was already degraded before that update, so i'm leaning toward an issue with vcenter or perhaps a windows update.  I'm putting LTSC on now with no updates"
79.json,t3_7w5ges,"Moving to a Colo, am I missing anything?",2018-02-08 23:15:30,"Hey Guys, We have 3 plants, one plant hosts all our services currently. Currently, all our LOB services (ERP, RDS, SQL etc, 6 hosts 75 VMs) are hosted out of an ""HQ"" plant that has microwave based internet capping out at 10/10mb, we then provide these services via VPN to 2 other plants, the plants have local DFS servers and AD/DNS/DHCP/Print. I'm looking to move into a full rack at a colo 10KM's away, and what I am wondering is, am I missing anything? The Colo will be provided me with redundant power, and 2 fiber cross-connects at ToR (my choice of vendor). I am still shopping for gear, but at this time I am leaning towards; Pretend below is a rack *     Palo Alto PA-820 (HA Pair) We would have 2 vertical PDUs in the back as well. From here we will have a VPN connection to the other sites serving the same services on the new and improved 500/500mb connection. Licensing, MS, Veeam, VMWare etc is already taken care of. Really what I am wondering is if I am missing anything that would be cause an issue. If you guys have any questions, feel free to ask."
79.json,t3_7w5ges,t1_dtxmu6w,2018-02-08 23:39:54,UPSes?
79.json,t3_7w5ges,t1_dtxnsi4,2018-02-08 23:55:10,"I was wondering whether or not I should get my own UPS(s) when I toured the facility I noticed that many racks lacked any UPS.But it does make sense to procure some, ill have to look into what power requirements I would have."
79.json,t3_7w5ges,t1_dtxop39,2018-02-09 00:09:11,Any colo worth a shit should have diesel or natural gas generators in the event of a power outage.
79.json,t3_7w5ges,t1_dtxpghb,2018-02-09 00:20:58,"IP-KVM? Or are you all-in on iDRAC?RS232 Console Server so you can get on the PAN or Dell LAN switches if they crash.Managed, Switched PDUs so you can hard power-cycle devices?Does ANYTHING in your rack have a single power cord? Do you need an ATS?Some people opt to add a small Lock Box in the cabinet to store spare SFPs, DAC cables and other technology-specific parts guaranteed compatible with your equipment.The CoLo should be able to provide simple patch cables or power cables on request. But it's hard to expect them to have a DAC cable guaranteed compatible with your PAN or Dell LAN devices.Some shops even throw a corporate-standard laptop in the drawer in case you need CoLo staff to help you grab a packet capture via a SPAN port, but your security policy forbids a foreign laptop from being used..."
79.json,t3_7w5ges,t1_dtxpipx,2018-02-09 00:21:57,"Don't forget basic crap, especially if this is your first colo build. Extra ethernet cables,  power cables, extension cords for monitors, extra monitor/keyboard/mouse, velcro straps, cage nuts, sfps, label maker, screwdrivers, serial cables, anything else you think you might need."
79.json,t3_7w5ges,t1_dtxq97y,2018-02-09 00:33:15,"One thing everyone forgets is cable management and labeling.  Remember remote hands dont know your setup so label all devices and cables as much as you can, and take pictures/documentation.  Color your storage network different than your IP network, etc..   Put a copy of network layout in the rack.  Place the pictures in an offsite location so when 3am comes and you are talking to some NOC monkey you can explain the work."
79.json,t3_7w5ges,t1_dtxqc2m,2018-02-09 00:34:30,"Isolate Management from production.I'd suggest a separate switch for your iDRAC and MGMT interfaces that connects back to a separate physical interface directly on your firewalls. Then ACL the hell out of that network segment to prevent unwanted access. Not only is this a good security practice, it's an isolated failure domain. If there is ever an issue with your TOR switches, you don't lose visibility and control of your other assets.You can go pretty cheap on that switch, since you'll need no more than 1GB copper interfaces, and a lot less redundancy than your production gear. Looks like you are an all dell shop, but something equivalent of a Cisco 2960X or Nexus 3048 would be just fine."
79.json,t3_7w5ges,t1_dtxrjmz,2018-02-09 00:53:01,"I am looking at an IP KVM, something like this,https://www.tripplite.com/netcommander-16-port-cat5-ip-kvm-switch-1u-rack-mount-4+1-user~B072016IP4That would provide OoB access to the servers if iDRAC is goofing around, it has 2 Serial ports as well, but that would only cover one PA and one switch.The ATS is not something i have looked into, I will check that out, this right;http://www.zonit.com/products/micro-ats/ ?The Lockbox is not a bad idea, ill keep that in mind as even just for when I am caught off guard and do not have my laptop on a unscheduled visit.Thanks!"
79.json,t3_7w5ges,t1_dtxsma7,2018-02-09 01:08:56,"All good stuff,I have a bunch of cabling I intend to use that will be colour-coded, with a rhino label maker as well as all kinds of velcro. Additionally a lot of blank plants and a shelf for storage."
79.json,t3_7w5ges,t1_dtxssd6,2018-02-09 01:11:16,"Pictures/Documentation for the hired help, will do."
79.json,t3_7w5ges,t1_dtxsynn,2018-02-09 01:13:41,"I intended to do this by means a addtional switch, but ill take this into consideration. Makes sense to isolate the MGMT network on the PA.Would I want to separate my Backup traffic to the MGMT switch as well? I was looking forward to having Veeam fly at 10GbE.Also, that's the exact switch i was looking at :)"
79.json,t3_7w5ges,t1_dtxt668,2018-02-09 01:16:40,Always Always always check rack depth. A lot of Colos I've used don't have enough rack depth to support the cable management arms that can be attached to the rails of the servers/SANs.
79.json,t3_7w5ges,t1_dtxthry,2018-02-09 01:21:12,"I will check that out on my next visit, Thanks!"
79.json,t3_7w5ges,t1_dtxtk4z,2018-02-09 01:22:07,What about a cellular backdoor into your management network? Something like this from opengear.https://opengear.com/products/acm7000-resilience-gateway
79.json,t3_7w5ges,t1_dtxtv5h,2018-02-09 01:26:24,How would this device fit into the network? In front of or behind the Palo Alto? Or entirely OoB?
79.json,t3_7w5ges,t1_dtxw2dn,2018-02-09 01:57:34,Are the power cables and colo power sockets compatible?
79.json,t3_7w5ges,t1_dtycwlo,2018-02-09 06:01:18,Is that 500/500 connection served by one provider?If so........make it not.
79.json,t3_7w5ges,t1_dtygx7h,2018-02-09 07:08:14,"At the moment it is just the one provider, looking at Cogent. 500/500 is just what they offer at the Colo, I didn't purposely seek it. I am considering having 2 providers though."
79.json,t3_7w5ges,t1_du06sjr,2018-02-10 05:17:13,"Are your IP addresses staying the same during the move, or do you have to re-ip everything?"
79.json,t3_7w5ges,t1_du077ke,2018-02-10 05:24:13,"IPs will likely change, we have a couple internet accessible services but the main hiccup will be cutting over the VPN for all the sites. Already planning that move."
79.json,t3_7w5ges,t1_du07alx,2018-02-10 05:25:37,"The colo provides 10A 240V A&B circuits (2 circuits) into the rack with L6-20 handoff, so I'm planning my PDUs accordingly."
8.json,t3_57zrf2,VDI environment,2016-10-18 04:51:23,"Hello, We are currently looking at different VDI options.  We are pretty sold on Horizon 7.  We already have a Cisco UCS environment. We are looking to add about 2k users.  We are looking at the Vxrail product, but are debating expanding our UCS instead.  How many users does Vxrail stop making a good use case? How does all flash VSAN stack up against a dedicated Xtremio or Unity?"
8.json,t3_57zrf2,t1_d8xchi3,2016-10-18 23:54:54,"I would also recommend looking at StarWind HyperConverged Appliance. It is based on Dell or SM (depends on your preference) hardware and was designed for VDI environments and can be customized to fit exactly you needs. StarWind also fully supports the solution, thus for any question/issue/difficulty you will speak only with one vendor. Also, the system comes to your site fully preconfigured. As we share the discount we get from the hardware partner, we are way less expensive than our competitors."
8.json,t3_57zrf2,t1_d913s3r,2016-10-21 08:33:23,"We run a 3000 seat XenDesktop environment on VMware/ UCS / NetApp AFF. I am a huge fan of UCS. The only thing I would change is Tintri as the storage. It rocks.2000 is a large deployment so UCS seems like a better fit but I guess it would depend of the types of desktops deployed. We have contact centers, Developers and a few high end use cases which ups the compute requirements.We are setting up a third datacenter for BC/DR and are considering a few Hyperconverged solutions so we'll see.My last environment was vBlock with XtremIO. Storage layer was great."
8.json,t3_57zrf2,t1_d927np4,2016-10-22 04:02:26,"I have heard that a few times, and I'm not against it.  What has me wondering though...is no can tell me why UCS makes a better use case.  VSAN seems pretty mature now and should provide comparable iops. For the price of a Dell Readynode that has 4 nodes.....I could barely get a B200 m4 with roughly the same specs."
80.json,t3_aqnhdm,How are people finding VxRail performance,2019-02-15 03:21:47,"I'd be interested to hear how people are finding VxRail performance. Are people using it for non-VDI workloads seeing decent reduction rates with AF setups? I know reduction rates are wholly workload dependant, but it’d be good to hear some experiences and workloads. Are people seeing the storage performance they expected from the platform?"
80.json,t3_aqnhdm,t1_egi3bj6,2019-02-15 10:07:25,Coming from a lefthand iscsi setup to an all flash array. It’s like night and day. Main line of business apps 30-50% performance improvement overnight. Most automated apps running jobs in 10 minutes that used to take an hour.
80.json,t3_aqnhdm,t1_egj5fil,2019-02-15 22:51:28,"VxRail ships node configs from single disk group hybrid configs to 4 disk group all flash configs with nvme cache.The basic rules of VSAN dont change, for high performance use all flash with fast cache disks and multiple disk groups."
80.json,t3_aqnhdm,t1_egkcghf,2019-02-16 07:32:49,"We have the ""basic"" 4-node E series VxRail with SATA SSDs and the vSAN pulled off +370K IOPS while we were testing it, and that was way before its limit - now it depends if that is the information you were looking for :-) I'm thinking about testing the vSAN to some brutal point where it hits the absolute limit, so I know the exact boundaries of the cluster performance one day. When that happens, I will let you know.As for the overall performance, I think it's quite a powerhouse. We are running some resource-heavy VMs on our VxRail and a lack of power is something unheard of for me."
81.json,t3_4bjgw3,HyperConverged: Simplivity vs. EMC VxRail,2016-03-23 05:20:24,We are getting ready to invest in new VMware Infrastructure.  Hyperconverged is a great option.  Both Simplivity and EMC have been on site and showed us their product but I am wondering what other admins think about the solutions.
81.json,t3_4bjgw3,t1_d19qmj8,2016-03-23 06:34:36,"I have a VxRail and I love it.  My issue with simplivity is that it requires the Simplivity controller to work, however the VxRail is just an EvoRail spec vmware environment, which is a standard vs simplivity's own solution."
81.json,t3_4bjgw3,t1_d19sz1g,2016-03-23 07:33:49,"I just completed our transition to SimpliVity this weekend, and it has been awesome. Their support is top notch, and the boxes integrated really well with our existing infrastructure. We've seen some noticeable performance increases, and the dedupe rate on our data is pretty good so far (around 3 to 1 at the moment)."
81.json,t3_4bjgw3,t1_d19wrm8,2016-03-23 09:11:08,[deleted]
81.json,t3_4bjgw3,t1_d19xj5o,2016-03-23 09:30:13,Do you have a review of Simplivity? We are looking at it for our DMZ.
81.json,t3_4bjgw3,t1_d19yfhl,2016-03-23 09:53:19,We just went with Cisco HX - pricing is very favorable right now(we saw 70%+ of list) and ended up with some insane specs for a smaller secondary site - so good in fact we are thinking of using it as the primary for external VDI users.
81.json,t3_4bjgw3,t1_d1adjmj,2016-03-23 20:31:09,Make sure you get your 9-1 Dedup ratio after 30 days!! hehe.  yea the product sounds good.  Just nervous investing so much in such a new company.
81.json,t3_4bjgw3,t1_d1admm3,2016-03-23 20:34:17,"Interesting.  One of my greatest fears is how their data works.  This whole ""Metadata"" thing really makes me hesitant."
81.json,t3_4bjgw3,t1_d1adn61,2016-03-23 20:34:52,I hadn't really considered Cisco's HyperFlex because they are JUST breaking into the market.
81.json,t3_4bjgw3,t1_d1aedsz,2016-03-23 21:01:05,How many nodes do you run? IOPS?
81.json,t3_4bjgw3,t1_d1bl93v,2016-03-24 18:07:58,Half a year ago we have also been looking for a hyper-converged solution and Simplivity was one of the major options. After comparing all the features and pricing we have purchased Starwind HCA https://www.starwindsoftware.com/starwind-hyperconverged-appliance. These guys sell commodity Dell servers and propose cool pre-built and pre-configured 2-node setups powered by both Hyper-V and VMware.We decided to go Hyper-V since it was a couple of branch offices infrastructure upgrade. We didn’t touch them since implementation and are very happy so far.
81.json,t3_4bjgw3,t1_d1bqclc,2016-03-24 21:43:22,Controller? Is it FPGA dedupe number crunching device you're talking about?
81.json,t3_4bjgw3,t1_d1kaoe1,2016-03-31 19:42:00,"Honestly if you are looking at just EMC vs SVT, go EMC, way too many downsides with SVT.You really should do yourself a favour and have a good look at Nutanix, they are the market leader in HCI for a reason."
81.json,t3_4bjgw3,t1_d20yju0,2016-04-13 17:46:48,"While IT Central Station does not yet have reviews for EMC VxRail, you can find real user reviews for SimpliVity and all the other major hyper-converged infrastructure solutions here: https://www.itcentralstation.com/categories/hyper-converged-infrastructureUsers interested in HCI solutions also read reviews for VMware VSAN. This user writes, ""VSAN is really simple to manage. Its GUI is part of the eco-system so it looks and feels like the rest of VMware. So a VMware engineer or a VMware operations guy's is going to be able to manage the provision storage without having to touch an array, which is generally higher profile so there's a cost reduction through headcount."" You can read the rest of his review here: https://www.itcentralstation.com/product_reviews/vmware-virtual-san-review-34889-by-ben-gentI hope you find this helpful!"
81.json,t3_4bjgw3,t1_d6e0m5c,2016-08-12 10:18:37,"If you don't mind me asking how much storage - real disc space - are you running? So for writes - sql would be an issue since it is already compressed and has a lot of writes... I'm not quite following what you mean about "" if I don't store them locally"" - What data on Simplivity isn't stored actually on their own storage? Sorry for all the questions- I'd really like to hear it from people like you who have real experience on it vs the sales guys. :)"
82.json,t3_9hach3,remote office multi-node vmware hardware?,2018-09-20 07:50:07,"I'm not quite ready to move all of our applications off-site, and I don't see that happening before my equipment goes off support in the next year. Does an inexpensive 2-4 node server cluster exist that's around the same cost as rolling my own cluster with 2-4 1U HP Proliant rack servers +VSAN? Others in a similar situation, what did you do, and what would you have done differently?"
82.json,t3_9hach3,t1_e6ai2lj,2018-09-20 09:08:57,Here's what I've found so far:Dell EMC vxrail e-series Cisco HX220c Edge HPE SimpliVity 380
82.json,t3_9hach3,t1_e6ajlf5,2018-09-20 09:33:38,VRTx or fx2 would do as well.
82.json,t3_9hach3,t1_e6al07s,2018-09-20 09:56:44,Thanks. I think the vrtx is $10k to start. Fx2 I haven't heard of yet.
82.json,t3_9hach3,t1_e6alnrh,2018-09-20 10:07:33,Keep in mind fx2 is supposed to use VSAN for shared storage. I’m not a fan but it might make sense for your use case.
82.json,t3_9hach3,t1_e6biejj,2018-09-20 22:44:15,Supermicro BigTwin?
82.json,t3_9hach3,t1_e6bu3pa,2018-09-21 01:23:33,You might consider StarWind HCA. They suggest hardware with preconfigured vSAN.https://www.starwindsoftware.com/starwind-hyperconverged-appliance
82.json,t3_9hach3,t1_e6bve5r,2018-09-21 01:40:47,"Interesting.  Looks to be $3750 for a 2-node vsphere ROBO environment, which is pretty cheap.  I'll need to see how much better this is than the big names and if it's worth the risk."
82.json,t3_9hach3,t1_e6muwb3,2018-09-26 05:20:57,I’ve only started my journey down the path too.  I’ve seen mention of Datrium and Stormagic as well but I have no idea on pricing.What are your thoughts so far?
83.json,t3_bmmavv,Question about use of FreeNAS connecting to,2019-05-10 01:03:16,"I'm curious if installing FreeNAS could help us with upgrading some VMware VMs to newer hardware and newer ESXi software. I'm thinking about using FreeNAS because our current SAN hardware that we connect to via FC on our old ESXi hosts does not support iSCSI or NFS.  I was thinking I could install this either natively on an oracle blade or if the FreeNAS didn't support the hardware (especially the chassis 10GBe interfaces) I could install it in a 5.0 ESXi VM. We're moving from old 5.0 ESXi host to new 6.7U2 Dell VXrails hosts.  It will be a whole new vcenter we're migrating the VMs to, so we will have to figure out how to do the upgrade, might have to do multiple steps since I don't think it's supported to go from 5.0 to 6.7u2 directly.  We have under 20 TBs of storage on the old VNX SAN to try to move.  When we migrate we hope to do multiple concurrent live migrations, possibly CPU then Drives separately. Another option we're thinking about is buying fiberchannel cards for the new VXrails and connecting them directly to the old VNX, but not sure if that would be a lot faster or if VMware even supports connecting to the older VNX. If you have any  experience with this or suggestions please let me know. Thanks."
83.json,t3_bmmavv,t1_emxvbsw,2019-05-10 02:06:43,??Why not just migrate using VMWare migration tools?  VMWare converter?
83.json,t3_bmmavv,t1_emxzpdt,2019-05-10 02:48:00,"The new VXRail system doesn't have fiber channel so can't connect to the old VNX SAN directly so you can't vmotion the systems.  That's why I was thinking about soind a FreeNAS that would host the VNX and allow the VXRail to connect to the old VNX via mapping through NFS.VMWare converter we would have to take the systems offline and it's very slow, plus would take a long time with that many systems."
84.json,t3_9zxi2r,Anyone upgrade to 4.7,2018-11-24 18:30:09,
84.json,t3_9zxi2r,t1_eactfae,2018-11-24 20:25:50,Yes. No issues. 4 different systems via offline bundle.https://insidetherails.wordpress.com/2018/11/16/vxrail-4-7-000-upgrade-walk-through/Edit Added Link
84.json,t3_9zxi2r,t1_earxcoo,2018-11-30 16:03:37,"I’m planning it out now.... I have an external vCenter and PSC, so I have to figure out what to do about that."
84.json,t3_9zxi2r,t1_edrv3iv,2019-01-11 11:58:10,Follow SolVe - it'll tell you what's needed.  vCenter will need to be upgraded first.
84.json,t3_9zxi2r,t1_efix8t5,2019-02-01 23:34:31,"We upgraded our secondary cluster to 4.7 and started seeing what Dell have described as ""errors that can be ignored"".  As far as we can tell, the cluster is functioning normally, we just keep getting critical alarms in vCenter and in VxRail Manager.  They told us they'd be fixed in the next release, but we're running 4.7.100 now and still see the errors.  Dell have advised us to silence them."
84.json,t3_9zxi2r,t1_efjqzcl,2019-02-02 05:08:30,"I was looking at doing it a couple of months ago, but my account rep advised me to wait.  Since then, I have changed jobs, and I am trying to get my new employer to go with VxRail."
84.json,t3_9zxi2r,t1_efsb124,2019-02-05 17:29:04,We're deploying a new E series cluster at the end of the month and the Dell engineer working on the PEQ also suggested that we keep with 4.5.300 for now. We have 2 other G series clusters running 4.5.300 and I've also been wondering if I should upgrade them.
84.json,t3_9zxi2r,t1_efsb2ru,2019-02-05 17:30:38,We saw these type of events from when we initially deployed our cluster with 4.5.210. Constant temperature warnings. Since upgrading to 4.5.300 these have gone away. I think I will hold off on upgrading to 4.7.
84.json,t3_9zxi2r,t1_eg5rj19,2019-02-10 23:49:02,"I’m about to do the deed, let you all know how it goes."
84.json,t3_9zxi2r,t1_egh9chy,2019-02-15 03:47:56,It'd be good to hear how you get on with the deployment and what your first impressions are
84.json,t3_9zxi2r,t1_egh9ipf,2019-02-15 03:49:52,So far complete disaster; upgraded PSC & vCenter and every VxRail Manager crapped out...
84.json,t3_9zxi2r,t1_egrial1,2019-02-19 07:30:26,Just upgrade 4 node P570F cluster today from 4.5 to 4.7. No issues at all. An initial shrink wrap install helps. Kept it simple...
84.json,t3_9zxi2r,t1_ej1mm2y,2019-03-21 23:30:43,"I'm experiencing something similar and now a downgrade to 4.5, but the required firmware is repeatedly failing to install.Utter fucking disaster"
84.json,t3_9zxi2r,t1_ej1osaa,2019-03-21 23:52:07,"I got word yesterday that it’s PTAgent that’s failing for us. They have pulled 4.7.100 due to a high impact performance problem with HBA firmware, so once I get the upgrade to complete I get to do it all again with 4.7.110..."
85.json,t3_8n07o5,"Cisco Hyperflex, VXRail or something else",2018-05-29 23:58:22,"I have been tasked with creating a new Virtual Environment, both servers and desktops. I have decided on VMWare but having some difficulties deciding on hardware.  I am looking for suggestions and reasons for each suggestion."
85.json,t3_8n07o5,t1_dzrqevg,2018-05-30 00:11:14,"How big is the environment? What's your budget? What are your HA/DR requirements? How much storage do you need? How many VMs? What size VMs? What kind of applications, what kind of IO?I'm not a fan of hyperconverged in most implementations - I see a good place for it with VDI and web server hosting, but when you get into large databases or very elastic workloads (like my private cloud for developers) it can be difficult to make sure you've got enough of the right storage."
85.json,t3_8n07o5,t1_dzrru2d,2018-05-30 00:31:34,"ON the VDI, we are looking at about 400 desktops but we may also do some application or desktop sessions and that may be another 300.  For servers we have about 150.  Budget is not set right now.  Average size of VM desktop is probably 80 GB.  Windows 10 would be OS for desktops with a few applications like our company software."
85.json,t3_8n07o5,t1_dzrsly7,2018-05-30 00:42:25,"last time I looked at the Cisco Hyperflex, i wasn't impressed.  basically all the IO has to go through this Storage VM that manages the storage. It felt jankety, and very 2012"
85.json,t3_8n07o5,t1_dzs2kdb,2018-05-30 03:05:12,"Had to put my .02 in. I just got off a VXRail deployment for the same idea, a 200 desktop VDI environment. I will say that the EMC folks have it down, and the VXRail deployment is pretty smooth.My only thing is, that unless you are restrained by amount of staff, knowledge or time..You can build yourself a small vSAN for cheaper than what a lot of these Hyper converged vendors offer."
85.json,t3_8n07o5,t1_dzs7i12,2018-05-30 04:15:31,"Look at HPE Simplivity, it excels in VDI deployments due to its hardware data accelerator card. We rolled out 2 small clusters about 6 months ago and are converting everything else to Simplivity over the next year."
85.json,t3_8n07o5,t1_dzs8mir,2018-05-30 04:31:44,Nutanix?
85.json,t3_8n07o5,t1_dzshmf1,2018-05-30 06:52:33,"The only two I'd bother looking at would be VXRail and Nutanix. If you're using VMware for the VDI piece VXRail is the easy pick. If you're using Citrix, than Nutanix and their custom KVM hypervisor is usually sufficient.For small deployments it seems easier to just buy pizza boxes and build your own VSAN cluster. The whole HCI thing hasn't really been that impressive."
85.json,t3_8n07o5,t1_dzsmeqf,2018-05-30 08:14:31,"Long time HX customer and love the all flash cluster for VDI (we also have a traditional hybrid HX at small secondary site which has been great as well).  That said going from hybrid or spinning disk to all flash with any vendor is going to be beneficial. If I could do it again I would also look at VSAN more closely but at the time it was still riddled with horror stories and I like the idea of not having all my eggs in one vendors basket.  A downside to that is that the HX VM is not at kernel level so it eats up a chunk or RAM.We had our DC completely lose power for the first time I know of a month back and the process to shutdown and come up was simple.  I feel I would have been fine on my own although TAC was involved and was immediately available given the circumstance.Even more important IMO is getting the software side built correctly.If you are not already doing instant clones, UEM, and AppVolumes or some equivalent I highly recommend doing so as the system we have is completely dynamic and can easily scale either by adding disk to existing HX hosts or adding hosts.Back to hardware, If you already have UCS you can do a 1:1 for compute only and storage boxes.  For us we got smoking deals on both of our clusters.  I would make a shortlist of vendors and play them against each other for the best price or other extras.If you have specific HX VDI questions let me know."
85.json,t3_8n07o5,t1_dzsopws,2018-05-30 08:54:24,Have simplivity and been happy so far. No VDI in our environment. The built in backup feature is nice.
85.json,t3_8n07o5,t1_dzyrusq,2018-06-02 06:22:23,VDI should be single instances through linked/instant clones.  With 6.7 putting instant clones on parity with linked clones I expect them to become the new default.I’m not sure why a 6 year old FPGA card plays into VDI unless it can single instance memory like an instant clone.  Getting 2-3X denser on memory potentially is just too big of savings to pass up.  I can avoid boot IO entirely with instant clones.
85.json,t3_8n07o5,t1_e0qeact,2018-06-16 02:59:54,Just installed a new all-flash VXRAIL solution this week. It is very beneficial to have all products/support within the same company.   I am very pleased with my decision.
86.json,t3_8t4uvh,EMC VxRail Deployment Screens ~ vCloud Info,2018-06-23 05:11:11,[]
86.json,t3_8t4uvh,t1_e14qbxy,2018-06-23 05:11:30,Like an unboxing video but for software. :)
86.json,t3_8t4uvh,t1_e14t8de,2018-06-23 05:59:32,Just set one up as well. The setup wizard works really well and seems pretty fault tolerant too (had a few network and DNS issues).
86.json,t3_8t4uvh,t1_e14v6zc,2018-06-23 06:33:56,I love the vxrail build animations.  I have 10 deployments scattered hither and you.  What I hate is that sometimes they don't come up from reboots and support has me RASR them back to life...
86.json,t3_8t4uvh,t1_e14wp5s,2018-06-23 07:00:47,"We opted to go with a customized vsan solution rather than vxrail,  gives us more flexibility."
86.json,t3_8t4uvh,t1_e15gnq9,2018-06-23 14:14:26,vxFAIL!  The make the animations so lively to distract from the fact that installing them sucks.I HATE deploying vxRails and the EMC support for our customers has been crap.
86.json,t3_8t4uvh,t1_e15iwse,2018-06-23 15:31:17,Still hate Marvin...
86.json,t3_8t4uvh,t1_e15mo5j,2018-06-23 18:03:00,"Flexibility in what sense? Hardware or software? I run a 4 node r730 cluster with vsan all flash. Getting it up and running was tedious at first, there's a lot to learn if you're new to vcenter and vsan in terms of concepts. Plus I found drivers for perc controllers painful initially. VXrails would have saved me a vast amount of time. And it seems in terms of future proofing most of the limitations are vsan rather than hardware. For example can't attach mechanical drives for capacity to an all flash setup without complete rebuild."
86.json,t3_8t4uvh,t1_e15v8ex,2018-06-23 22:11:16,"That's weird, I have a customer who ran one without a UPS for 8 months and it recovered from multiple power failures without issue.What VxRail version, hardware gen? What is the symptom when it doesn't recover."
86.json,t3_8t4uvh,t1_e16plec,2018-06-24 07:28:23,It’s come a long way from the old EVO:Rail days. Automation is easy. Automation that handles brownfield misconfigurations is hard :)
86.json,t3_8t4uvh,t1_e17fkhq,2018-06-24 19:34:45,[deleted]
87.json,t3_78nqa1,VSAN dedupe/compression Savings 0.00B Ratio: 1.00x (VXRAIL),2017-10-25 21:39:58,"We have a default test cluster setup and enabled compression/dedupe 3 weeks ago and dedupe/compression seems to be faulty. This cluster is on the latest VXrail build available to me (ESXi, 6.0.0, 6765062). Virtual Disks: 1.72TB Deduplicaion and Compression overhead: 2.2TB Savings: 0.00B Ratio: 1.00x I am using a mix of default storage policy, FTT2R6 and FTT0. All Virtual Disks are compliant and there are no recyncing components listed. Force provisioning isn't enabled."
87.json,t3_78nqa1,t1_dovcx61,2017-10-25 23:36:27,We have seen the same issue but it was with 6.0 U3. updating to 6.0u3 patch 5 seems to have resolved that issue.Are your VM's thin provisioned? Were all the VM's already on the cluster when you enabled dedupe/compression?
87.json,t3_78nqa1,t1_dovd4mo,2017-10-25 23:40:06,Thin provisioned yes and some were on the cluster before turning it on but I've deployed 10+ since.
87.json,t3_78nqa1,t1_dovsyn4,2017-10-26 04:04:45,Is OSR (Object Space Reservation?) set to 100% by any chance?
87.json,t3_78nqa1,t1_dovt1x6,2017-10-26 04:06:18,"not specified at all in my storage policies, i believe default is 0?"
87.json,t3_78nqa1,t1_dovyvrd,2017-10-26 05:47:31,"VM Swap files are hard coded to have a OSR of 100% and are mirrored. Not certain how many VM's you have, but depending on how much VM RAM you've assigned (and 2x that to storage), but you can easily enable vSAN Sparse Swap using this setting:esxcfg-advcfg -s 1 /VSAN/SwapThickProvisionDisabledOr you can use my PowerCLI script: https://code.vmware.com/samples/1665VM's would need to be powered off and powered back on if only vSAN capacity is available, or they can be Storage vMotioned to non-vSAN storage and then Storage vMotioned back. Really depends on whether you are willing to power your VMs off or not."
87.json,t3_78nqa1,t1_dp5dyej,2017-11-01 00:49:22,"Ok, so I just remembered something. You may have data ""still in"" the cache tier. As you write more data it will force evictions. The Cache tier is not compressed or deduped."
88.json,t3_6vrlfq,HPE/Aruba advice needed,2017-08-24 23:29:47,"I work at a fairly small organization.  I currently have an end of life HPE 8212zl switch.  It is being used for core and edge functionality. For refreshers this switch is the twelve module chasis with dual management modules (only six modules are currently being used).  I am looking at converting from a two node hyperv cluster with emc san, to a four node dell vxrails cluster.  My current setup has two cisco 1gb switches on the san side, and can tolerate one of the switches going down or rebooting.  On conference with the dell reps, they want to land eight 10gb interfaces on my chassis, rather than having a dedicated san switch.  I am all for simplifying things, but I see some issues here.  The major issue I see is that the 8212zl is still just one switch, it does have dual management, however if I want to upgrade the switch firmware there will be downtime on the storage ports that I assume would cause issues. So here is my idea of upgrading our admin office network.  Remember, this is a small government org.  I am not trying to build out a major data center.  I have a fair bit of experience with hp/aruba so my preference would be to stick with them.  Cisco is not on my radar (juniper? maybe..)  Anyways, I am thinking of reusing my v2 port modules and placing them in a new 5406r chasis and using it as my edge switch.  I would then uplink that to a pair of 3810m's with stacking.  I would outfit them with 10gb sfp to uplink the new vxrails system.  The 3810m's would be my new core.  This would allow me to upgrade each switch individually with minimal downtime. Do you see any issues with this?  Any other recommendations?  Any opinions on the 3810m's?  Thanks for any help"
88.json,t3_6vrlfq,t1_dm2gh7i,2017-08-25 00:11:30,"Even if you have a redundant management module on current HPE Procurve Switches, your switching will still be down while upgrading the firmware."
88.json,t3_6vrlfq,t1_dm2hrw6,2017-08-25 00:33:30,"Correct, on my current HP 8212 switching will be down while a firmware update is occurring.  That is why I am looking to have the two 3810m's be used for the connection to vxrails.  I should be able to update each switch separately.  The two should provide redundant paths for the vxrails."
88.json,t3_6vrlfq,t1_dm2i2qp,2017-08-25 00:38:35,"your storage switches should be separate, if you can.you don't need a pair of separate switches, but if you can get one chassis (5406R ZL2 v3), you can get all the port density you require, redundant management, etc... and still get a 3810 or 2930[with 4x10G ports] and have redundant switching for storage."
88.json,t3_6vrlfq,t1_dm2ihxo,2017-08-25 00:45:19,Can the 5406r have non disruptive firmware upgrades?  I just assumed it was similar to the 8212.
88.json,t3_6vrlfq,t1_dm2vf8k,2017-08-25 04:31:26,"I haven't used the 3810m's, but generally if you stack switches you can't upgrade the firmware of each switch individually."
88.json,t3_6vrlfq,t1_dm2wm7j,2017-08-25 04:52:29,"After doing a few minutes of research, I think you are correct.  Which may push me into setting up vrrp instead of using stacking."
89.json,t3_69czf0,Newbie storage question,2017-05-05 14:02:55,"Hi! Forgive my ignorace... I'm a storage newb. We just upgraded our ESXi clusters to VxRAIL with VSAN based storage. The current plan is to use our old NetApp SANs (FAS2040 and FAS2240) for backup storage and low-priority workloads. The new vxrail hosts have 2x10GbE networking, with one of those 10Gb ports dedicated to VSAN traffic. All port groups on dVswitch. The NetApp SANs have 4x1Gb networking. What would be the most effective/speedy/reliable way to present this storage to the vxrail esxi hosts, considering they have only one 10Gb nic per host for this additional storage traffic? iSCSI mpio? NFS with LACP? THANKS!"
89.json,t3_69czf0,t1_dh5vs7k,2017-05-05 20:32:03,I would choose NFS over iSCSI if i have the choice.check this: https://forum.synology.com/enu/viewtopic.php?t=79657
89.json,t3_69czf0,t1_dh5wcxx,2017-05-05 20:47:47,"Yes, I'd agree completely with this, far simpler - I'd also be tempted to just live with 1 x 1Gbps NIC from each NetApp head/controller to make the configuration simpler as it sounds like performance is a secondary concern."
89.json,t3_69czf0,t1_dh5zpya,2017-05-05 22:06:42,"Keep in mind (unless fixed in the most, most recent code) that you can't finish the VxRail upgrades unless you unmount and detach your iSCSI LUNS on a per-host basis.Last step in the upgrade is doing the host VIBs and it won't complete unless you do these steps. One host at a time. Brutal.Gigantic pain in the ass and incredibly frustrating. Can't wait until this is resolved."
89.json,t3_69czf0,t1_dh60d8p,2017-05-05 22:20:12,"Wait... What??? This might be a deal breaker for iscsi, then. What is the rationale behind this?I might just go NFS... Are there any issues with that and vxrail upgrades?"
89.json,t3_69czf0,t1_dh638lz,2017-05-05 23:16:01,"If you go LACP, keep in mind that affects ALL traffic on your 10Gb NICs.You cant do that on an individual PG/vmK basis.So you would need to check with Dell EMC first to make sure they support that.Is there a 10G upgrade for the NetApps that isn't stupid expensive?Personally I would do that."
89.json,t3_69czf0,t1_dh68cht,2017-05-06 00:51:42,One of the ports dedicated to VSAN.  What happens if that port fails?
89.json,t3_69czf0,t1_dh6c85c,2017-05-06 02:08:26,It has the other 10Gb setup as standby.
89.json,t3_69czf0,t1_dh6od68,2017-05-06 06:30:18,"Both ports are active for some traffic and standby for other traffic. So there is mutual failover and you can get by with only two NICs. You use NIOC to setup shares so if all traffic ends up on one NIC after a failure, your vSAN traffic gets a 50% share and doesn't get squeezed by vMotion or VM traffic."
9.json,t3_auw856,Routing on different network class in same LAN,2019-02-26 14:38:38,"in our cisco network, we have class C ip addresing Vlan 1 - 192.168.1.0/24 vlan 2 - 192.168.2.0/24 the in our new dell network (vxrail servers and dell switches) we have this vlan 100 - 172.16.100.0/24 vlan 101 - 172.16.101.0/24 for these 2 network to communicate, i created a new vlan (vlan 100 - 172.16.100.0/24 , vlan 101 - 172.16.101.0/24) in cisco network and trunk line between these devices. everything seems to work out, but there are some instances that we cannot ping vlan100,101 in my PC(vlan 2), but can be pinged from different PC in our network with vlan 100/101 configured. and sometimes i can ping the PCs configure in vlan100 in cisco network, but not on the dell network side and vice versa. is my setup possible or i'm missing something? or a VLAN record conflicts between 2 networks."
9.json,t3_auw856,t1_ehb19sj,2019-02-26 15:06:43,Is the gateway device the same for all the vlans?
9.json,t3_auw856,t1_ehb1e3g,2019-02-26 15:09:38,I don't get what Topology you have in mind but check that all switches are participating in the routing protocol / that the routes are present on all switches.
9.json,t3_auw856,t1_ehb3xka,2019-02-26 16:12:43,yes all same gateways
9.json,t3_auw856,t1_ehb4nqz,2019-02-26 16:32:10,"we currently have a cisco environment (192.168.0.0 IP) and a new separate network, all dell devices(vxrail cluster, dell S4048–ON switches 172.16.0.0 IP).our goal is to connect this dell rack in our current network so we can remotely manage the VM hosted in that cluster. for us to communicate to the dell side, both network must have the same VLAN IDs in their records (right?). This is what i did in our cisco core switchinterface Vlan100 ip address 172.16.100.254 255.255.255.0interface Vlan101 ip address 172.16.101.254 255.255.255.0..... Vlan 102, 103 104 105...interface GigabitEthernet0/24 description trunk_to_dell switchport trunk allowed vlan 100-105 switchport mode trunk switchport nonegotiatein dell switch, its just like thisinterface TenGigabitEthernet 1/25 description trunk_to_CiscoCore no ip address switchport no shutdownvlan 100-105 are tagged in this interfacethis pretty much worked. i can access vsphere web. but sometimes we cannot ping the other vlans.i tried this test, PC in cisco network with configure vlan 105, cant communicate to cisco but can in all 100-105 vlans.if this is a cisco device, a simple VTP config will suffice and we can allowed certain VLANs to pass on trunk line in our choice."
9.json,t3_auw856,t1_ehb5eao,2019-02-26 16:52:34,"Network Classes no longer exist, stop calling them ""class C"""
9.json,t3_auw856,t1_ehb5r2o,2019-02-26 17:02:44,"i know, i'm just wondering maybe different Ip addressing might be an issue in different devices."
9.json,t3_auw856,t1_ehbbocx,2019-02-26 19:50:09,where is your router?
9.json,t3_auw856,t1_ehd8uep,2019-02-27 10:35:40,this is only a L3/L2 switch connections.
9.json,t3_auw856,t1_ehh4pzh,2019-02-28 22:28:04,Neither does SSL but here we are…
90.json,t3_6d5quy,Any feedback on the Cisco SG500 Small Business Series with a VDI Deployment,2017-05-25 07:03:23,"So, working on a new vxRail deployment and we opted out of the Brocade VDX's because we already have some deployed as the core and were thinking we would just do some home runs to those. Now where moving towards a ""leaf and spine"" spine build with 2 Cisco SG500XG-8F8T. Was wondering if anyone has worked with these switches before and could offer up some feedback. TL:DR - Cisco SG500XG-8F8T  - is it good? Mahalos"
90.json,t3_6d5quy,t1_di04g7t,2017-05-25 07:42:48,[deleted]
90.json,t3_6d5quy,t1_di05wdu,2017-05-25 08:14:03,"yes, don't"
90.json,t3_6d5quy,t1_di06zr9,2017-05-25 08:37:06,"You are likely spending a good chunk of money on a vxRail, but you're going to skimp on the switches? Those switches have shitty buffers and shitty latency, and they don't do proper leaf and spine. What you should be looking at is Nexus 9k, or 3k if you want to go the cheaper route."
90.json,t3_6d5quy,t1_di0ds7p,2017-05-25 11:06:14,"Sure? They do switching and do what they advertise.Is this good for a VDI deployment? Most definitely not. It will work, but it's not optimal.For leaf/spine architecture, you need to be using data center switches, as packet_whisperer stated. The Cisco SG family is for SMB, not data centers.A Nexus 9K would be ideal - 9200 if you don't need ACI in the near future, 9300 if you do see ACI in your near future so you don't have to do a hardware replacement."
90.json,t3_6d5quy,t1_di0e9dq,2017-05-25 11:17:37,"Just clarifying for others - PE_Norris means that it is end of sale, not end of support."
90.json,t3_6d5quy,t1_di124no,2017-05-25 23:33:39,My personal opinion is that Nexus switches would be WAY overkill for connecting VDI clients to. I'd recommend 2960Xs at least. 3650 or 3850 would be better (but 3850 gets you up into the Nexus 3000 price range).
90.json,t3_6d5quy,t1_di1e4vr,2017-05-26 03:13:35,Awsome handle! Also your right this probbly isn't the project to be skimping on.
90.json,t3_6d5quy,t1_di461wr,2017-05-28 00:26:40,"Actually if you compared a 3850 with 24x 10G (WS-C3850-24XS-S) with a N3K with 24x 10G (N3K-C3524P-10GX) the 3850 will be twice the price (N3K @10K, CAT3850 @ 21K list price). It really depends on bandwidth requirements and features to be used but if OP wants some solid datacenter switches which offer 10G line-rate I wouldnt recommend catalyst since its way to expensive because it offers campus features not needed in the DC."
91.json,t3_cc1lmn,Dell VxRail Hyperconverged systems - Things to know before install/configuration?,2019-07-12 05:27:45,"Kickoff meeting for the install of our new VxRail system coming up next week. Anyone have any recommendations of items that would be helpful for me to know going into this? (I'm the new sysadmin hired getting up to speed on the ""new hardware projects"".) Any pain points, installation hiccups, disasters to avoid that I should look out for?"
91.json,t3_cc1lmn,t1_etjup4q,2019-07-12 05:51:41,Try posting in vmware
91.json,t3_cc1lmn,t1_etkjnb0,2019-07-12 10:49:22,"Double,triple, and quadruple check that you have all the networking knowledge you need including VLANS, IP addresses, etc. Network misconfiguration is the number one failure when deploying VxRail or any HCI solution for that matter.Oh and make sure they you have adequate power ready :)"
91.json,t3_cc1lmn,t1_etmfbya,2019-07-13 03:10:46,"Like crafty said, make sure power is ready and make sure that you have your network configured correctly.  The one other thing I would suggest is that you watch the engineer like a hawk when he is setting up your vsan redundancy (depending on if you're doing n+1, n+2, etc"
91.json,t3_cc1lmn,t1_etw78wz,2019-07-16 13:57:31,"VMware Mod, I’d go post that somewhere that Dell people watch (Dell or VxRail )"
92.json,t3_ah5v1a,Connecting Dell S4048T-ON switch to cisco network,2019-01-18 10:46:41,"so i posted something about this to dell community, so far no reply. hope reddit can help me with this. https://www.dell.com/community/Networking-General/Connecting-S4048T-ON-to-cisco-network/m-p/7188889 so we have these new dell switches for our vxrail cluster, separated from our current network. Everything worked fine after setting up the switches (as followed on the manual), the next thing I did was plugged a cable from our network so we can manage the servers we're going to setup in the vxrail cluster. A simple trunk cable, with specified allowed vlan, connecting to the dell switch, also configured as trunk with specified vlan. As a cisco guy, supposedly this will work just like that. But I'm not sure why I can't ping the other host in the dell network from our cisco network, perhaps there's something I'm missing to configure from both the devices. Hope that there's someone here who's familiar with dell networking. The cisco device i'm using from cisco network is C2960 UPDATE FIXED, i just placed a fuckin router between them. stupid me"
92.json,t3_ah5v1a,t1_eeboqt2,2019-01-18 10:53:54,"Checked your logs on both sides?Checked spanning tree messages/configurations?Any chance you're looping, even via the cluster?Checked basic things like MAC address learning, interface counters etc.FastEthernet to 10G interface is possibly a problem. It's rare for a 10G port to step down any lower than 1G."
92.json,t3_ah5v1a,t1_eebpcb3,2019-01-18 11:01:37,Check counters and spanning tree on the cisco side. Not sure how to change stp on the dell switches but may be worth looking into
92.json,t3_ah5v1a,t1_eebr76g,2019-01-18 11:25:30,"Do you have ""no shutdown"" on the interface Vlan on the Dell side?"
92.json,t3_ah5v1a,t1_eebvyv4,2019-01-18 12:29:39,"im pretty sure it has no shutdown commands, i can ping some of the host vcenter, the esxi hosts, but sometimes i cannot ping the VMs in other vlans which is kind of weird."
92.json,t3_ah5v1a,t1_eebxg7o,2019-01-18 12:50:38,"i didnt enable the STP in dell side since only the 2 ports are configured as port-channel, which i believe is considered as one logical connection(right?) for them to see each other. And there is only 1 trunk connection from the cisco to the 1st dell switch."
92.json,t3_ah5v1a,t1_eebz47x,2019-01-18 13:14:21,"i don't think there's a loop in here. yesterday i can ping the hosts in vlan 50, but today i can't. same with vlan 10, i can ping 10.11 but not 10.12 and others.i'll take a look at this."
92.json,t3_ah5v1a,t1_eec8krb,2019-01-18 16:13:09,"Network is straightforward. Just both side trunk and if you add vlans in Dell switch and appoint vlans it should work. So lets deduce problem first. First make port access just for 1 vlan and leave Dell ports in default. If you still cant reach then check mac address table, if you can see mac addresses of end points on Dell from Cisco then issue might be with ports."
92.json,t3_ah5v1a,t1_eec98om,2019-01-18 16:27:17,"with sh ip arp in my core cisco, our cisco network were able to see the mac address and IP of hosts from the dell side. yesterday i can ping all the hosts in the dell side, then today only a couple of it, its like the connection is dropping or what. no changes in both sides.i tried to plug a laptop from the dell side, access port then static ip and it can ping easily from the cisco side. i'm starting to wonder if it is the cluster problem due to this."
93.json,t3_58aibf,VxRack and VxRail systems feel the Edge of Power,2016-10-19 22:21:04,[]
94.json,t3_7jtsmx,How to VxRail upgrade with external vCenter?,2017-12-15 03:02:58,"We have a VxRail with 4.0.132( standard license) which has some bugs and needs to be uploaded to 4.0.400, since the VxRail is connected to external vCenter, we are being asked to go through EMC PS and its very expensive and we are planning to do it ourself, but I cannot find any resources at all. Wouldnt this be simple and step by step procedure, Vcenter(SSO-external), Vcenter server, ESXi, tools, VSAN disk formate and VxRail manager? I am not sure if we get separate binaries? There are no resources available. Any help would be greatly appreciated"
94.json,t3_7jtsmx,t1_dr9l4o7,2017-12-15 08:22:19,"Hello Vendor lock-in, my old friend...Does EMC have any RCM info for components so you can see what to move to?"
94.json,t3_7jtsmx,t1_dra5pix,2017-12-15 16:57:56,"You shouldn’t need Dell EMC PS, my understanding is upgrade assistance is included as a nice to have in Premium support but it’s not mandatory. Perhaps they are thinking you would expect them to upgrade the external vCenter, which I can understand would be chargeable as it’s outside of their environment.To upgrade yourself, you would perform the normal vCenter upgrade process. Then to upgrade the hosts and VxRail Manager, you would do that via VxRail Manager using the 4.0.400 bundle."
94.json,t3_7jtsmx,t1_dra9jb6,2017-12-15 19:38:12,"First of all, I am impressed you got VxRail working with the external vCenter; my experiences with this have been ... never working.Request the Upgrade Procedure document from Dell EMC Support; it should include a scenario for external vCenter. Short answer: PSC and vCenter must be upgraded first -- please check all integrated components regarding this, as Horizon, vRealize, SRM, etc. all have different requirements -- then run the software upgrade procedure for the VxRail.I found a basic blog post that walks through it (with an VxRail vCenter, so just make sure to get that squared away first)."
94.json,t3_7jtsmx,t1_drahnkj,2017-12-15 23:09:17,"The ""SolVe Desktop"" utility should be able to generate a step-by-step guide for you to upgrade the VxRail environment. The guide will not include steps on upgrading your external vCenter though. If you're looking for resources and documentation, this is definitely worth installing.304270 : How to download and install SolVe Desktop?The VxRail 4.0.400 Release Notes state that vCenter 6.0 Update 3c (build 7037393) is included, so I assume that's the target version you will want to upgrade your external vCenter to. As mentioned by others, you will want to upgrade vCenter/PSC first. Once that's complete, use VxRail Manager to upgrade the rest.If you have a valid support contract, you should be able to open a Service Request and get assistance with obtaining documentation, or even upgrading VxRail once you have already taken care of vCenter. Let me know if you have more questions."
94.json,t3_7jtsmx,t1_dralhso,2017-12-16 00:18:07,"Thanks for the reply, so EMC does not want to touch the box until we buy PS services.There are some hard drive errors(cosmetic) and dial home alerts are being raised continuously and we are told that 4.0.400 has a fix for thisAlso, the most confusing part is that most likely it is a VSAN issue and we are told that this PS upgrade will do that VSAN from 6.2 to 6.6 and 6.6 is part of vSphere 6.5 which is not supported under 4.0.400 and only from 4.5Also, solve desktop asks to contact EMC when we choose external vCenter and does not really give us any more options from there."
94.json,t3_7jtsmx,t1_drallut,2017-12-16 00:20:03,"Thank you, EMC says"" As this is a special case of external vCenter, this needs PS services to upgrade"" :/I believe, like you mentioned, PSC and vCenter and then composite package from the VxRail manager.There are some hard drive errors(cosmetic) and dial home alerts are being raised continuously and we are told that 4.0.400 has a fix for this Also, the most confusing part is that most likely it is a VSAN issue and we are told that this PS upgrade will do that VSAN from 6.2 to 6.6 and 6.6 is part of vSphere 6.5 which is not supported under 4.0.400 and only from 4.5"
94.json,t3_7jtsmx,t1_drbbui9,2017-12-16 08:15:47,"Just to counter the statement about VxRail and external vCenter ‘never working’, we have multiple VxRail clusters spread across multiple external vCenter without issues. This is using 4.5.What problems have you experienced?"
94.json,t3_7jtsmx,t1_drbqwvd,2017-12-16 14:40:44,"Upgrading a vcenter is a boring trivial process. Going from 6.0 to 6.5U1 is fun as you swing Upgrade migrate to the new Photon OS VCSA. It’s actually quite easy, just read the guide."
94.json,t3_7jtsmx,t1_ds4ymox,2018-01-04 02:09:00,"The upgrade from .132 to .400 I believe requires a manual download. It will ignore the external vCenter and only upgrade the hosts and VxRail Manager, automatically. Just supply the file and it will do everything for you.That said, I did a successful upgrade to 4.0.400 with external vCenter 6.0u2, however deploying 4.0.400 requires 6.0u2a. I would recommend getting vCenter to at least 6.0u3 before upgrading VxRail."
95.json,t3_a2psuk,AT-x908 Reallocation of skb has failed for protocol eapol,2018-12-03 23:45:34,"Hello Since we connected our new VxRail system to our core switch a couple of months ago we have these messages that repeat endlessly on the Allied Telesis x908 core. Apart from being annoying and interrupting your CLI to the switch every time it pops up, it doesn't seems to be causing any other problems. During the deployment of the VxRails I tested it  and this stops when you disconnect the uplinks from the VxRails, so I'm sure this is the reason. “2018 Dec  3 11:10:43 kern.crit CORE_SW kernel: Reallocation of skb has failed for protocol eapol” followed by “2018 Dec  3 11:11:50 kern.crit s_src@CORE_SW 0 kernel: Last message 'Reallocation of skb ' repeated 36 times, suppressed by syslog-ng on CORE_SW” This repeats every 30-40 seconds. We are running firmware version 5.4.4-4.13 on the x908. Anyone has any idea what would be the reason, or if there is a way to stop them? Don't want to get into too many details of our configuration, as I'm not sure what would be relevant to this, but if you need any more info, please let me know. Thanks."
95.json,t3_a2psuk,t1_eb19o23,2018-12-04 07:42:25,"have you engaged the vendor with this? This sub is not vendor support.quick 10 sec google searchWhen the LLDP packet with the destination MAC address 01: 80: c2: 00: 00: 03 is received, the following log is output. If necessary, please set not to output the log message by the log exclude command, or set to discard the corresponding packet in the access list. When transmitting and receiving LLDP packets between AlliedWare Plus switch products, this event does not occur because the destination address is 01: 80: c2: 00: 00: 0 e.awplus kernel: Reallocation of skb has failed for protocol eapol"
95.json,t3_a2psuk,t1_eb205d3,2018-12-04 14:52:42,"Thanks for your reply #justanotherplumberI think I did see this japanese article (release notes) you are referring to, but it got me chasing LLDP and why are the Dell switches causing this... anyway got lost.Allied support, believe it or not, we don't have active support for the core switches. Not my choice, but here we are. Going to replace core sw next summer anyway...We had an Allied engineer contracted on site for a couple of days to help with something else and I did ask about this. He didn't have much time to look into this but he thought it was not a big problem, just annoying. He was probably keen to get home and did not offer us a solution. We are not entitled to FW updates without support and I think he didn't want to open a can of worms.Anyway, thanks to your reference, I managed to at least get it to shut up by adding the following line to the config:log console exclude msgtext 'Reallocation of skb '"
96.json,t3_6ubme9,Nutanix vs Simplivity vs VxRail,2017-08-18 01:39:51,"Hey Everyone, So I'm currently very interested in these converge systems, but I'm having alot of trouble finding out pros and cons for each system. Was wondering if anyone with more experience dealing with hyper converage could guide me in the right direction... looking for details on what each of these has to offer and what makes them stand out from each other. Thank you!"
96.json,t3_6ubme9,t1_dlrh4yx,2017-08-18 02:28:33,"Nutanix has great flexibility regarding what hardware it sits on and what hypervisor it works with. Has a hypervisor of it's own even.Simplivity is within HPE's ecosytem, and will likely benefit from Nimble's Infosight at some point.vxRail is purpose built for VMWare, and converges a few great parts of tech from Dell/EMC together.Depends on workload/application, many comparisons to be made across the board. I am a Nutanix VAR full disclosure."
96.json,t3_6ubme9,t1_dlrixqu,2017-08-18 02:58:54,Do not get a VxRail. To put it professionally it had been an unpleasant experience. They actually recently told us they will not be honoring their SLO for support.
96.json,t3_6ubme9,t1_dlrk6s0,2017-08-18 03:20:20,"The gist:Nutanix has a range of hardware you can use, their own version of KVM with added features (you can scrap VMware licensing) or even go with Hyper-V, SimpliVity benefits from ASIC offloading gets better dedupe/compression ratios cannot do KVM or Hyper-V must be dual CPU and All Flash, vxRail is just DELL/EMC packaged VSAN with a little special sauce since they are so close with VMware.I'm a VAR that can sell all three - If you'd like you can PM me to setup a call to discuss (no strings) there is a whole lot to go over"
96.json,t3_6ubme9,t1_dlrksgi,2017-08-18 03:30:37,EDIT: Formatting
96.json,t3_6ubme9,t1_dlrkz2c,2017-08-18 03:33:46,"I'll be the one to play devil's advocate and say that you should quote a conventional build as well.The upside of HCI is that's it's turn key. Single vendor for purchase and support. Buy 1 sku and get storage, compute, virtualization etc all in one box. You save money on consulting to get it designed and built, easier to manage overall but it isn't necessarily a lower TCO. It really depends on your requirements.The downside is you are locked into one vendor for hardware. Support costs and upgrade costs tend to be steep, Nutanix is notorious for getting you in cheap and then raking you over the coals for upgrade parts. HCI also doesn't necessarily play nice with other products, like wanting to add 3rd party storage or something down the line.But this only applies if you plan on growing during the lifespan of the product. A bunch of my clients buy an HCI product once and they're good for 3-4 years because it was sized adequately.The upside to conventional is you can customize it how you want from bottom up. The downside is it takes a great deal more expertise to build out your compute, storage, licensing etc. Also you are dealing with multiple different vendors which is both a good thing and bad thing depending on how you look at it.Quote out a custom virtualization build and compare it side by side with HCI. Even including the consulting to set it all up properly sometimes you'll be shocked to discover it's cheaper, has more capacity and better features. Sometimes HCI is the clear winner. Important part is to check out both and make your VAR do the work for the cost comparison so you can make the right decision.Nutanix is built on their own virtualization platform based on KVM, they have great support but their sales department is pushy and hard to deal with.Not much experience with other HCI vendors, I'm an old school consultant who prefers to spec out and build everything myself.Proliant + Nimble + Nexus + Vmware + Veeam"
96.json,t3_6ubme9,t1_dlrmxjm,2017-08-18 04:06:51,"Dude, totally, I pretty much always recommend that everyone look at this stack, it's just so well-used it's easy to get a million responses regarding how it's worked for others.Conventional is funny because the downsides are upsides too, if you use them the right way.With HCI you've got a little lock-in, at least inside the lifecycle of your gear, whereas with conventional HW/SW the commoditization of the stack means you get easily interoperable HW and SW so you can always push back against any vendor you're engaged with by saying you're looking at others too.By doing that you can get a lot of 'free consulting' on the build and config because everyone is trying to explain how and why their's fits in your environment and bringing a Solutions Engineer to the table for that purpose.A place I see massive growth opportunities is rackmount compute and network with software defined segmentation, management and storage.This will continue to be figured out in the next couple decades, but what I'm foreseeing is an IT supplier channel with diverse options so that their customers can consume the resources exactly as they like with flexibility across the entirety of the infrastructure.HCI, cloud, conventional and software defined architectures will all have a piece of the puzzle moving forward based on use-case, not buzzwords."
96.json,t3_6ubme9,t1_dlrua34,2017-08-18 06:21:28,"Disclosure: HPE / Simplivity partner -> If you go Simplivity, they have some performance guarantees, and will even give you new nodes if they are unable to meet them, ask your sales engineer. -> this all being said, i find i end up with way more cpu and memory than i need to meet customer storage requirements.  It is often more cost effective to pick up/use an additional storage platform for data that does not de-dupe / compress that well. (multimedia etc.)"
96.json,t3_6ubme9,t1_dlulwuw,2017-08-20 01:53:44,dell + vmware + vsan
96.json,t3_6ubme9,t1_dlyd3zk,2017-08-22 11:39:40,"Doing the research ourselves, and so far I'm impressed with Nutanix. They seem to have figured out what the future brings, and that's cloud hosting/management. I'm going to stay far away from Simplivity, since it's an HPE product now."
96.json,t3_6ubme9,t1_dtf1sr5,2018-01-30 00:46:48,"Does this mean when there is a vMotion the data is also moved to the new host so that it is local? And presumably the other copies of the VMDK stay where they are?Is this true for all options? 3 copies of the data, 1 can be in maintanence and 1 can fail."
97.json,t3_9rn6qe,6.5 v 6.7 for NetApp/Dell?,2018-10-27 02:13:29,"I've a stack of NetApp FAS/AFF and some R740's arriving any time soon. I'll be using NFS and that's as far as I've got as my focus has really been on evaluating storage solutions. With 6.5 and 6.7 both on the HCL would anyone have any solid reasons not to be going with 6.7 please? We're on 5.5 right now and quite honestly I haven't focussed much on VMware as 5.5 has done everything we need. We're on Standard with vCenter so our requirements are very basic, just ESXi, NFS, vMotion and HA and that's about all. Any good guides on best practises around VMware network layout when you've got a pair of 10GbE NICs would be good too, as I said our 5.5 boxes got put in a few years back and other than updates have stayed pretty static. We do have 2x 1GbE NICs onboard too. Rich"
97.json,t3_9rn6qe,t1_e8i6vy0,2018-10-27 02:44:42,"Please create dedicated vmkernel adapters for your storage, dedicated ones for vmotion and use the standard one vmk0 only for management.I would say go with 6.7U1, but that just got released and isn’t really supported by common backup software. So I would stick with 6.5 U2. As long as you don’t need the GPU support in 6.7 e.g. for horizon, you don’t really miss  any features."
97.json,t3_9rn6qe,t1_e8ibkft,2018-10-27 03:50:19,[deleted]
97.json,t3_9rn6qe,t1_e8ie554,2018-10-27 04:26:49,Why not NFS? It performs on par with iSCSI and is far more flexible concerning (re)sizing compared to VMFS.The only downside I see with NFS compared to iSCSI is the need to do some special network planning and configuration to get the traffic evenly distributed across your uplinks while with iSCSI you just use port binding to accomplish the same thing.If you want/have to use NBD with your backup solution this will limit your backup speed to 400 MBit/s. Depending on the daily change size and the amount of VMs and the length of your backup window this might not be enough. (If you are using direct storage access for your backup solution this is of course of no concern.)
97.json,t3_9rn6qe,t1_e8imm7w,2018-10-27 06:39:41,"Because NFS is infinitely easier to manage, and it is what NetApp is known for.Need more space in your datastore? Resize it from ONTAP, the hypervisors don't need to know or care or do anything.Need to restore a VM from a snapshot? Mount the NFS volume on a linux host, copy the VM directory from your snapshots back to the active filesysten, reregister it in vcenter.Really the only place I personally use block at instead of file is for MSSQL servers, and I'm soon going to be toying with continuously available SMB on ONTAP 9.3 to see if that'll be a good replacement for that use case."
97.json,t3_9rn6qe,t1_e8ive1g,2018-10-27 09:13:52,"Absolutely, both vendors have you covered.  Try the NFS Best Practice guide to get an understanding of NFS.Ontap Best Practices for VsphereRead and follow the general documentation.Edit: The NFS documentation really want you to be familiar with ESXi Networking in general and VMkernel interfaces in particular."
97.json,t3_9rn6qe,t1_e8jbvum,2018-10-27 14:52:37,"Cheers, we've got some professional services to help with the NetApp install so I'll read those guides too.10GbE is a big change for us and I think because it's ""only"" 2x10GbE ports it feels a bit tight but I was just looking at the VXRAIL notes (this isn't VXRAIL it's just what came up doing a Google) and they do nodes with 2x10GbE too and as you'd expect just run everything over them.What was interesting is they seem to do active/standby to bind things to a particular NIC unless it fails.Rich"
98.json,t3_7a7eb1,hyper-converged appliance or standard build?,2017-11-02 07:10:04,"I hope I am asking the right question because I'm still unfamiliar with the definition. Basically, what is everyone standard setup (not cheap clients or those that have extra requirements) for a customer server-network setup? Do you guys use an all-in-one appliance such as Dell VRTX, Nutanix, Dell EMC, StarWinds HPE HyperConverged...etc? Just curious because I feel we are a bit behind but need some more information on other type of solutions other MSPs are exploring. MSPs on this reddit seems like leap and bound ahead of many of the MSPs in our area, including our own haha."
98.json,t3_7a7eb1,t1_dp7zjtt,2017-11-02 10:27:56,I have lost my trust in the idea of a hyper-converged system.  There supposedly isn't  a single point of failure in the box but we've seen a few with hardware failures that brought down the entire system.  I think having a cluster or at least multiple hosts with replication is much safer.  I'm partial to VMware but I think it ultimately depends on your needs and budget.  You can get a lot of mileage out of a vSphere Essentials license for a small environment that you would need SCCM for with Hyper V to get similar features.
98.json,t3_7a7eb1,t1_dp8a35j,2017-11-02 15:03:33,"A lot depends on your use case and scale. Most hyper-converged systems I've seen have the storage controller as a single point of failure. For a pure VDI cluster, that isn't a big deal and the density/space savings can make it worthwhile. For core servers/services host machine in any environment requiring high uptime not so much, not even in a clustered setup.Another consideration, with Dell at least their VXRail HCI solution is just one of their server chassis (R730/R740/FX-Chassis/etc) with a thoroughly tested hardware/software combo. They do seem to offer really good support on the HCI Products thought since they are providing all the pieces, you don't end up with the Storage manufacturer blaming the Server Manufacturer, and then both blaming the SAN Switch manufacturer. Do you have a handful of shit-hot engineers in house? If you do then it would possibly be more cost effective to spec out servers that meet the needs (to include resiliency) and work more of the software level issues in house and rely on support mostly for quick repair parts.For full disclosure, I'm not an MSP, but I'll share our current setup as well as what we are looking to shift to in the next year:Additionally we aren't fully there yet but we are looking at shifting to a design resilient enough to handle losing any single rack. Example being Rack 1 on Circuit A (with UPS of course) would have 2x FX Chassis, 2x R720, and 2x 10gb SFP+ switches stacked. Rack 2 would have the same setup but on a different circuit. Then for moderate uptime setup just configure HA and ensure a single rack is capable of supporting the load, even if it means a slight degradation of performance. Alternatively, configure Fault Tolerance so the 2nd rack will take over if a fault occurs. (That might be more appropriate if it's a situation where a 30 second downtime is going to cause the client to ask questions about root cause analysis and mitigation of future risk)Most of our equipment is on the tail end of it's lifecycle at the moment and my primary experience is with Dell products so apologies for using out of date examples, just wanted to use what I'm familiar with."
98.json,t3_7a7eb1,t1_dpb2bii,2017-11-04 04:47:39,"I came from a heavy CI environment using Cisco UCS, EMC storage with brocade FC switches, and Cisco 10Gb backbone using 7k and 5k switches, and 2k FEX for pizza boxes.  Rock solid.We were doing a bakeoff between the Cisco HyperFlex and the VXRail.  I didn't do much with it, but just looking at it briefly, I thought the VXRail was a better product than HyperFlex, with scaling being a part of it.  That said, I think at least one, if not two versions of VXRail have been delivered since the bake-off, and now they're heavy-handed in Dell hardware again.I was looking at Nutanix a bit for the environment I'm now in so that we can look at hosting, but it'd likely be either Nutanix or VXRail.  I didn't get to far into it as a I got busy with other things and really didn't get to dig into Nutanix very much.  The upside that I did see with Nutanix is that you can use basically your own hypervisor or theirs or I think even a combination, but with VXRail you only get ESXi.  Which is no big deal to me...I'm a VMware guy anyway.That said, unless you're hosting, or have a large enterprise setup, I don't see much need for HCI on the average SMB."
98.json,t3_7a7eb1,t1_dpflroi,2017-11-07 01:33:16,"You might find it helpful to take a look at the reviews for converged and hyper-converged solutions on IT Central Station. The reviews get pretty in depth and often discuss why the user chose them and how their company uses them, so you might find some information there that would help you learn more. You can also look up reviews for Nutanix, Dell EMC and some of the other systems mentioned here. I hope this helps."
98.json,t3_7a7eb1,t1_dpfmvx1,2017-11-07 01:51:55,"I would definitely look at ready nodes from HPE and Starwind. Not so familiar with HPE support, but Starwind guys were very helpful and helped to configure the systems from the scratch. Of course, the choice depends on how many servers do you want, what applications you're going to run."
98.json,t3_7a7eb1,t1_dph4e40,2017-11-07 23:03:45,"Yeah, it depends on the actual planned configuration of yours, and the production, that will be running on it. If you want to achieve an active-active replication between the servers, thus in case of a single node failure, your workload will be available from the second one. I'd suggest you looking into StarWind VSAN.There's not only the software, but several hardware options, such as Hyper-Converged Appliance. With this one you will have pre-configured and pre-tested configuration, with an umbrella support, so you will not need to configure anything by yourself.And even backup options, such as Cloud VTL to AWS, if you want to save the backups in the cloud."
98.json,t3_7a7eb1,t1_dpj2iv5,2017-11-09 00:39:35,Those are pretty useful information. Thanks
99.json,t3_brdg2l,REST APIs for VxRail?,2019-05-22 01:44:37,Does anyone have any documentation or examples for the REST APIs for VxRail?
99.json,t3_brdkp6,t1_eodonm0,2019-05-22 06:25:01,The VxRail API guide is on the Dell EMC support site (registration required) here: https://support.emc.com/docu91468_VxRail_Appliance_4.5.x_and_4.7.x_API_Guide.pdf
99.json,t3_brdkp6,t1_eofgiw4,2019-05-22 22:03:59,THANK YOU!
